[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH5835M Statistical Computing",
    "section": "",
    "text": "About MATH5835",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#organisation-of-math5835",
    "href": "index.html#organisation-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Organisation of MATH5835",
    "text": "Organisation of MATH5835\nThis module is MATH5835M Statistical Computing.\nThis module lasts for 11 weeks from 30 September to 13 December 2024. The exam will take place between 13 and 24 January 2025.\nThe module leader, the lecturer, and the main author of these notes is Dr Matthew Aldridge. (You can call me “Matt”, “Matthew”, or “Dr Aldridge”, pronounced “old-ridge”.) My email address is m.aldridge@leeds.ac.uk, although I much prefer questions in person at office hours (see below) rather than by email.\n\nLectures\nThe main way you will learn new material for this module is by attending lectures. There are three lectures per week:\n\nMondays at 1400 in Roger Stevens LT 13\nThursdays at 1200 in Roger Stevens LT 03\nFridays at 1000 in Rogers Stevens LT 09\n\nI recommend taking your own notes during the lecture. I will put brief summary notes from the lectures on this website, but they will not reflect all the details I say out loud and write on the whiteboard. Lectures will go through material quite quickly and the material may be quite difficult, so it’s likely you’ll want to spend time reading through your notes after the lecture. Lectures should be recorded on the lecture capture system; I find it very difficult to read the whiteboard in these videos, but if you unavoidably miss a lecture, for example due to illness, you may find they are better than nothing.\nIn Weeks 3, 5, 7, 9 and 11, the Thursday lecture will operate as a “problems class” – see more on this below.\nAttendance at lectures in compulsory.\n\n\nProblem sheets and problem classes\nMathematics and statistics are “doing” subjects! To help you learn material for the module and to help you prepare for the exam, I will provide 5 unassessed problem sheets. These are for you to work through in your own time to help you learn; they are not formally assessed and will not be marked by me (or anyone else). You are welcome to discuss work on the problem sheets with colleagues and friends, although my recommendation would be to write-up your “last, best” attempt neatly by yourself.\nYou should work through each problem sheet in preparation for the problems class in the Thursday lecture of Week 3, 5, 7, 9 and 11. In the problems class, you should be ready to discuss your answers to questions you managed to solve, explain your progress on questions you partially solved, and ask for help on questions you got stuck on. You can also ask for extra help or feedback at office hours (see below).\n\n\nCoursework\nThere will be one piece of assessed coursework, which will make up 20% of your module mark.\nThe coursework will be in the form of a worksheet. The worksheet will have some questions, mostly computational but also mathematical, and you will have to write a report containing your answers and computations.\nThe assessed coursework will be introduced in the computer practical sessions in Week 9.\nThe deadline for the coursework will be the penultimate day of the Autumn term, Thursday 12 December  at 1400. Feedback and marks will be returned on Monday 13 January, the first day of the Spring term.\n\n\nOffice hours\nI will run a weekly office hours drop-in session for feedback and consultation. You can come along if you want to talk to me about anything on the course, including if you’d like some feedback on your attempts at problem sheet questions. (For extremely short queries, you can approach me before or after lectures, but my response will often be: “Come to my office hours, and we can discuss it there!”)\nOffice hours will happen on Mondays from 1500 to 1600 – so directly after the Monday lecture – in my office, which is EC Stoner 9.10n on the 9th floor of the EC Stoner building. (One way my office is via the doors directly opposite the main entrance to the School of Mathematics. You can also get there from Staircase 1 on the Level 10 “red route” through EC Stoner, next to the Maths Satellite.) If you cannot make this time, contact me for an alternative arrangement.\n\n\nExam\nThere will be one exam, which will make up 80% of your module mark.\nThe exam will be in the January 2025 exam period (13–24 January); the date and time will be announced in December. The exam will be in person and on campus.\nThe exam will last 2 hours and 30 minutes. The exam will consist of 4 questions, all compulsory. You will be allowed to use a basic non-programmable calculator in the exam.",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#content-of-math5835",
    "href": "index.html#content-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Content of MATH5835",
    "text": "Content of MATH5835\n\nNecessary background\nIt is recommended that students should have completed at least two undergraduate level courses in probability and statistics, or something equivalent, although confidence and proficiency in basic material is more important than very deep knowledge. For Leeds undergraduates, the official prerequisite is MATH2715 Statistical Methods, although confidence and proficiency in the material of MATH1710 & MATH1712 Probability and Statistics 1 & 2 is probably more important.\nSome knowledge I will assume:\n\nProbability: Basic rules of probability; random variables, both continuous and discrete; “famous” distributions (especially the normal distribution and the continuous uniform distribution); expectation, variance, covariance, correlation; law of large numbers and central limit theorem.\nStatistics: Estimation of parameters; bias and error; sample mean and sample variance\n\nThis module will also include an material on Markov chains. I won’t assume any pre-existing knowledge of this, and I will introduce all new material we need, but students who have studied Markov chains before (for example in the Leeds module MATH2750 Introduction to Markov Processes) may find a couple of lectures here are merely a reminder of things they already know.\nThe lectures will include examples using the R program language. The coursework and problem sheets will require use of R. The exam, while just a “pencil and paper” exam, will require understanding and writing short portions of R code. We will assume basic R capability – that you can enter R commands, store R objects using the &lt;- assignment, and perform basic arithmetic with numbers and vectors. Other concepts will be introduced as necessary. If you want to use R on your own device, I recommend downloading (if you have not already) the R programming language and the program RStudio. (These lecture notes were written in R using RStudio.)\n\n\nSyllabus\nWe plan to cover the following topics in the module:\n\nMonte Carlo estimation: definition and examples; bias and error; variance reduction techniques: control variates, antithetic variables, importance sampling. [9 lectures]\nRandom number generation: pseudo-random number generation using linear congruential generators; inverse transform method; rejection sampling [7 lectures]\nMarkov chain Monte Carlo (MCMC): [7 lectures]\n\nIntroduction to Markov chains in discrete and continuous space\nMetropolis–Hastings algorithm: definition; examples; MCMC in practice; MCMC for Bayesian statistics\n\nBootstrap: Empirical distribution; definition of the bootstrap; bootstrap error; bootstrap confidence intervals [4 lectures]\nFrequently-asked questions [1 lecture]\n\nTogether with the 5 problems classes, this makes 33 lectures.\n\n\nBook\nThe following book is strongly recommended for the module:\n\nJ Voss, An Introduction to Statistical Computing: A simulation-based approach, Wiley Series in Computational Statistics, Wiley, 2014\n\nThe library has electronic access to this book (and two paper copies).\nDr Voss is a lecturer in the School of Mathematics and the University of Leeds, and has taught MATH5835 many times. An Introduction to Statistical Computing grew out of his lecture notes for this module, so the book is ideally suited for this module. My lectures will follow this book closely – specifically:\n\nMonte Carlo estimation: Sections 3.1–3.3\nRandom number generation: Sections 1.1–1.4\nMarkov chain Monte Carlo: Section 2.3 and Sections 4.1–4.3\nBootstrap: Section 5.2\n\nFor a second look at material, for preparatory reading, for optional extended reading, or for extra exercises, this book comes with my highest recommendation!",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html",
    "href": "lectures/L01-mc-intro.html",
    "title": "1  Introduction to Monte Carlo",
    "section": "",
    "text": "1.1 What is statistical computing?\n“Statistical computing” – or “computational statistics” – refers to the branch of statistics that involves not attacking statistical problems merely with a pencil and paper, but rather by combining human ingenuity with the immense calculating powers of computers.\nOne of the big ideas here is simulation. Simulation is the idea that we can understand the properties of a random model not by cleverly working out the properties using theory – this is usually impossible for anything but the simplest “toy models” – but rather by running the model many times on a computer. From these many simulations, we can observe and measure things like the typical (or “expected”) behaviour, the spread (or “variance”) of the behaviour, and other things. This concept of simulation is at the heart of the module MATH5835M Statistical Computing.\nIn particular, we will look at Monte Carlo estimation. Monte Carlo is about estimating a parameter, expectation or probability related to a random variable by taking many samples of that random variable, then computing a relevant sample mean from those samples. We will study Monte Carlo in its standard “basic” form (Lectures 1–9), but also in the modern Markov chain Monte Carlo form (Lectures 17–23), which has become such a crucial part of Bayesian statistical analysis.\nTo run a simulation, one needs random numbers with the correct distributions. Random number generation (Lectures 10–16) will be an important part of this module. We will look first at how to generate randomness of any sort, and then how to get that randomness into the shape of the distributions we want.\nWhen dealing with a very big data set, traditionally we want to “reduce the dimension” by representing it with a simple parametric model. For example, tens of thousands of datapoints might get reduced just to estimates of the parameters \\(\\mu\\) and \\(\\sigma^2\\) of a normal distribution. But with computational statistics, we don’t need to make such a simplification – we can do inference using the full details of the whole dataset itself, without making extra assumptions. An computational scheme that takes advantage of this idea is the bootstrap (Lectures 24–27).\nMATH5835M Statistical Computing is a mathematics module that will concentrate on the mathematical ideas that underpin statistical computing. It is not a programming module that will go deeply into the practical issues of the most efficient possible coding of the algorithms we study. But we will want to investigate the behaviour of the methods we learn about and to explore their properties, so will be computer programming to help us do that. (We will be using the statistical programming language R, although one could just as easily have used Python or other similar languages.) As my PhD supervisor once told me: “You don’t really understand a mathematical algorithm until you’ve coded it up yourself.”",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html#what-is-monte-carlo-estimation",
    "href": "lectures/L01-mc-intro.html#what-is-monte-carlo-estimation",
    "title": "1  Introduction to Monte Carlo",
    "section": "1.2 What is Monte Carlo estimation?",
    "text": "1.2 What is Monte Carlo estimation?\nLet \\(X\\) be a random variable. We recall the expectation \\(\\Ex X\\) of \\(X\\): if \\(X\\) is discrete with probability mass function (PMF) \\(p\\), then this is \\[ \\Ex X = \\sum_x x\\,p(x) ;\\] while if \\(X\\) is continuous with probability density function (PDF) \\(f\\), then this is \\[ \\Ex X = \\int_{-\\infty}^{+\\infty} x\\,f(x)\\,\\mathrm{d}x . \\] More generally, the expectation of a function \\(\\phi\\) of \\(X\\) is \\[ \\Exg \\phi(X) = \\begin{cases} {\\displaystyle \\sum_x \\phi(x)\\,p(x)} & \\text{for $X$ discrete}\\\\ {\\displaystyle \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x}  & \\text{for $X$ continuous.} \\end{cases}\\] (This matches with the “plain” expectation when \\(\\phi(x) = x\\).)\nBut how do we actually calculate an expectation like one of these? If \\(X\\) is discrete and can only take a small, finite number of values, we can simply add up the sum \\(\\sum_x \\phi(x)\\,p(x)\\). Otherwise, we just have to hope that \\(\\phi\\) and \\(p\\) or \\(f\\) are sufficiently “nice” that we can manage to work out the sum/integral using a pencil and paper. But while this is often the case in the sort of “toy example” one comes across in maths or statistics lectures, this is very rare in “real life”.\nMonte Carlo estimation is the idea that we can get an approximate answer for \\(\\Ex X\\) or \\(\\Exg \\phi(X)\\) if we have access to lots of samples from \\(X\\). For example, if we have access to \\(X_1, X_2 \\dots, X_n\\) , independent and identically distributed (IID) samples with the same distribution as \\(X\\), then we already know that the mean \\[ \\overline X = \\frac{1}{n}(X_1 + X_2 + \\cdots + X_n) = \\frac{1}{n} \\sum_{i=1}^n X_i \\] is usually close to the expectation \\(\\Ex X\\), at least if \\(n\\) is big. Similarly, it should be the case that \\[ \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] should be close to \\(\\Exg \\phi(X)\\).\nIn this module we will write that \\(X_1, X_2, \\dots, X_n\\) is a “random sample from \\(X\\)” to mean that \\(X_1, X_2, \\dots, X_n\\) are IID with the same distribution as \\(X\\).\n\nDefinition 1.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and write \\(\\theta = \\Exg\\phi(X)\\). Then the Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\).\n\nWhile general ideas for estimating using simulation go back a long time, the modern theory of Monte Carlo estimation was developed by the physicists Stanislaw Ulam and John von Neumann. Ulam (who was Polish) and von Neumann (who was Hungarian) moved to the US in the early 1940s to work on the Manhattan project to build the atomic bomb (as made famous by the film Oppenheimer). Later in the 1940s, they worked together in the Los Alamos National Laboratory continuing their research on nuclear weapons, where they used simulations on early computers to help them numerically solve difficult mathematical and physical problems.\nThe name “Monte Carlo” was chosen because the use of randomness to solve such problems reminded them of gamblers in the casinos of Monte Carlo, Monaco. Ulam and von Neumann also worked closely with another colleague Nicholas Metropolis, whose work we will study later in this module.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html#examples",
    "href": "lectures/L01-mc-intro.html#examples",
    "title": "1  Introduction to Monte Carlo",
    "section": "1.3 Examples",
    "text": "1.3 Examples\nLet’s see some simple examples of Monte Carlo estimation using R.\n\nExample 1.1 Let’s suppose we’ve forgotten the expectation of the exponential distribution \\(X \\sim \\operatorname{Exp}(2)\\) with rate 2. In this simple case, we could work out the answer using the PDF \\(f(x) = 2\\mathrm{e}^{-2x}\\) as\n\\[ \\mathbb E X = \\int_0^\\infty x\\,2\\mathrm{e}^{-2x}\\,\\mathrm{d}x \\](and, without too much difficulty, get the answer \\(\\frac12\\)). But instead, let’s do this the Monte Carlo way.\nIn R, we can use the rexp() function to get IID samples from the exponential distribution: the full syntax is rexp(n, rate), which gives n samples from an exponential distribution with rate rate. So our code here should be\n\nn &lt;- 100\nsamples &lt;- rexp(n, 2)\nMCest &lt;- (1 / n) * sum(samples)\nMCest\n\n[1] 0.3978279\n\n\nSo our Monte Carlo estimate is 0.39783, to 5 decimal places.\nTo get a (hopefully) more accurate estimation, we can use more samples. We could also simplify the third line of this code by using the mean() function.\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.5001547\n\n\n(In the second line, 1e6 is R code for the scientific notation \\(1 \\times 10^6\\), or a million. I just picked this as “a big number, but where my code still only took a few seconds to run.”)\nOur new Monte Carlo estimate is 0.50015, which is much closer to the true value of \\(\\frac12\\).\n\nBy the way: all R code “chunks” displayed in the notes should work perfectly if you copy-and-paste them into RStudio. (Indeed, these lecture notes should not appear unless the code runs correctly without errors.) I strongly encourage playing about with the code as a good way to learn this material and explore further!\n\nExample 1.2 Let’s try another example. Let \\(X \\sim \\operatorname{N}(1, 2^2)\\) be a normal distribution with mean 0 and standard deviation 2. Suppose we want to find out \\(\\mathbb E(\\sin X)\\) (for some reason). While it might be possible to somehow calculate the integral \\[ \\mathbb E(\\sin X) = \\int_{-\\infty}^{+\\infty} (\\sin x) \\, \\frac{1}{\\sqrt{2\\pi\\times 2^2}} \\exp\\left(\\frac{(x - 1)^2}{2\\times 2^2}\\right) \\, \\mathrm{d} x , \\] that looks extremely difficult to me.\nInstead, a Monte Carlo estimation of \\(\\Exg(\\sin X)\\) is very straightforward. (Although we must remember that when using the rnorm() function to generate normally distributed variates, the third argument is the standard deviation, here \\(2\\), not the variance, here \\(2^2 = 4\\).)\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n, 1, 2)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.1140989\n\n\nOur Monte Carlo estimate is 0.11410.\n\nNext time: We look at more examples of things we can estimate using the Monte Carlo method.\n\nSummary:\n\nStatistical computing is about solving statistical problems by combining human ingenuity with computing power.\nThe Monte Carlo estimate of \\(\\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, \\dots, X_n\\) are IID random samples from \\(X\\).\nMonte Carlo estimation typically gets more accurate as the number of samples \\(n\\) gets bigger.\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html",
    "href": "lectures/L02-mc-uses.html",
    "title": "2  Uses of Monte Carlo",
    "section": "",
    "text": "2.1 Monte Carlo for probabilities\nQuick recap: Last time we defined the Monte Carlo estimator for an expectation of a function of a random variable \\(\\theta = \\Exg \\phi(X)\\) to be \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are independent random samples from \\(X\\).\nBut what if we want to find a probability, rather than an expectation? What if we want \\(\\mathbb P(X = x)\\) for some \\(x\\), or \\(\\mathbb P(X \\geq a)\\) for some \\(a\\), or, more generally, \\(\\mathbb P(X \\in A)\\) for some set \\(A\\)?\nThe key thing that will help us here is the indicator function. The indicator function simply tells us whether an outcome \\(x\\) is in a set \\(A\\) or not.\nThe set \\(A\\) could just be a single element \\(A = \\{y\\}\\). In that case \\(\\Ind_A(x)\\) is 1 if \\(x = y\\) and 0 if \\(x \\neq y\\). Or \\(A\\) could be a semi-infinite interval, like \\(A = [a, \\infty)\\). In that case \\(\\Ind_A(x)\\) is 1 if \\(x \\geq a\\) and 0 if \\(x &lt; a\\).\nWhy is this helpful? Well \\(\\Ind_A\\) is a function, so let’s think about what the expectation \\(\\Exg \\Ind_A(X)\\) would be for some random variable \\(X\\). Since \\(\\Ind_A\\) can only take two values, 0 and 1, we have \\[ \\begin{align*}\n\\Exg \\Ind_A(X) &= \\sum_{y \\in\\{0,1\\}} y\\,\\mathbb P\\big( \\Ind_A(X) = y \\big) \\\\\\\n  &= 0 \\times \\mathbb P\\big( \\Ind_A(X) = 0 \\big) + 1 \\times \\mathbb P\\big( \\Ind_A(X) = 1 \\big) \\\\\n  &= 0 \\times \\mathbb P(X \\notin A) + 1 \\times \\mathbb P(X \\in A) \\\\\n  &= \\mathbb P(X \\in A) .\n\\end{align*} \\] In line three, we used that \\(\\Ind_A(X) = 0\\) if and only if \\(X \\notin A\\), and that \\(\\Ind_A(X) = 1\\) if and only if \\(X \\in A\\).\nSo the expectation of an indicator function a set is the probability that \\(X\\) is in that set. This idea connects “expectations of functions” back to probabilities: if we want to find \\(\\mathbb P(X \\in A)\\) we can find the expectation of \\(\\Ind_A(X)\\).\nWith this idea in hand, how do we estimate \\(\\theta = \\mathbb P(X \\in A)\\) using the Monte Carlo method? We write \\(\\theta = \\Exg\\Ind_A(X)\\). Then our Monte Carlo estimator is \\[  \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\Ind_A(X_i) . \\]\nWe remember that \\(\\Ind_A(X_i)\\) is 1 if \\(X_i \\in A\\) and 0 otherwise. So if we add up \\(n\\) of these, we count an extra +1 each time we have an \\(X_i \\in A\\). So \\(\\sum_{i=1}^n \\Ind_A(X_i)\\) counts the total number of the \\(X_i\\) that are in \\(A\\). So the Monte Carlo estimator can be written as \\[  \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{\\# \\text{ of } X_i \\text{ that are in $A$}}{n} . \\]\nAlthough we’ve had to do a bit of work to get here, this a totally logical outcome! The right-hand side here is the proportion of the samples for which \\(X_i \\in A\\). And if we want to estimate the probability something happens, looking at the proportion of times it happens in a random sample is very much the “intuitive” estimate to take. And that intuitive estimate is indeed the Monte Carlo estimate!\nWe should explain the third line in the code we used for the Monte Carlo estimation mean(samples &gt; 2). In R, some statements can be answered “true” or “false”: these are often statements involving equality == (that’s a double equals sign) or inequalities like &lt;, &lt;=, &gt;=, &gt;, for example. So 5 &gt; 2 is TRUE but 3 == 7 is FALSE. These can be applied “component by component” to vectors. So, for example, testing which numbers from 1 to 10 are greater than or equal to 7, we get\n1:10 &gt;= 7\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\nsix FALSEs (for 1 to 6) followed by four TRUEs (for 7 to 10).\nWe can also use & (“and”) and | (“or”) in true/false statements like these.\nBut R also knows to treat TRUE like the number 1 and FALSE like the number 0. So if we add up some TRUEs and FALSEs, R simply counts how many TRUEs there are\nsum(1:10 &gt;= 7)\n\n[1] 4\nSo in our Monte Carlo estimation code, samples &gt; 2 was a vector of TRUEs and FALSEs, depending on whether each sample was greater than 2 or not, then mean(samples &gt; 2) took the proportion of the samples that were greater than 2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html#monte-carlo-for-probabilities",
    "href": "lectures/L02-mc-uses.html#monte-carlo-for-probabilities",
    "title": "2  Uses of Monte Carlo",
    "section": "",
    "text": "Definition 2.1 Let \\(A\\) be a set. Then the indicator function \\(\\Ind_A\\) is defined by \\[ \\Ind_A(x) = \\begin{cases} 1 & \\text{if $x \\in A$} \\\\ 0 & \\text{if $x \\notin A$.} \\end{cases} \\]\n\n\n\n\n\n\n\n\nExample 2.1 Let \\(Z \\sim \\operatorname{N}(0,1)\\) be a standard normal distribution. Estimate \\(\\mathbb P(Z &gt; 2)\\).\nThis is a question that it is impossible to answer exactly using a pencil and paper: there’s no closed form for \\[ \\mathbb P(Z &gt; 2) = \\int_2^\\infty \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-z^2/2}\\,\\mathrm{d}z , \\] so we’ll have to use an estimation method.\nThe Monte Carlo estimate means taking a random sample \\(Z_1, Z_2, \\dots, Z_n\\) of standard normals, and calculating what proportion of them are greater than 2. In R, we can do this as follows.\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.0229\n\n\nIn the second line, we could have written rnorm(n, 0, 1). But, if you don’t give the parameters mean and sd to the function rnorm(), R just assumes you want the standard normal with mean = 0 and sd = 1.\nWe can check our answer: R’s inbuilt pnorm() function estimates probabilities for the normal distribution (using a method that, in this specific case, is much quicker and more accurate than Monte Carlo estimation). The true answer is very close to\n\npnorm(2, lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nso our estimate was pretty good.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html#monte-carlo-for-integrals",
    "href": "lectures/L02-mc-uses.html#monte-carlo-for-integrals",
    "title": "2  Uses of Monte Carlo",
    "section": "2.2 Monte Carlo for integrals",
    "text": "2.2 Monte Carlo for integrals\nThere’s another thing – a non-statistics thing – that Monte Carlo estimation is useful for. We can use Monte Carlo estimation to approximate integrals that are too hard to do by hand.\nThis might seem surprising. Estimating the expectation of (a function of) a random variable seems a naturally statistical thing to do. But an integral is just a straight maths problem – there’s not any randomness at all. But actually, integrals and expectations are very similar things.\nLet’s think of an integral: say, \\[ \\int_a^b h(x) \\,\\mathrm{d}x ,\\] the integral of some function \\(h\\) (the “integrand”) between the limits \\(a\\) and \\(b\\). Now let’s compare that to the integral \\(\\Exg \\phi(X)\\) of a continuous random variable that we can estimate using Monte Carlo estimation, \\[ \\Exg \\phi(X) = \\int_{-\\infty}^\\infty \\phi(x)\\,f(x)\\, \\mathrm{d} x. \\] Matching things up, we can see that we if we were to a function \\(\\phi\\) and a PDF \\(f\\) such that \\[ \\phi(x)\\,f(x) = \\begin{cases} 0 & x &lt; a \\\\ h(x) & a \\leq x \\leq b \\\\ 0 & x &gt; b , \\end{cases}  \\tag{2.1}\\] then we would have \\[ \\Exg \\phi(X) = \\int_{-\\infty}^\\infty \\phi(x)\\,f(x)\\, \\mathrm{d} x = \\int_a^b h(x) \\,\\mathrm{d}x, \\] so the value of the expectation would be precisely the value of the integral we’re after. Then we could use Monte Carlo to estimate that expectation/integral.\nThere are lots of choices of \\(\\phi\\) and \\(f\\) that would satisfy this the condition in Equation 2.1. But a “common-sense” choice that often works is to pick \\(f\\) to be the PDF of \\(X\\), a continuous uniform distribution on the interval \\([a,b]\\). (This certainly works when \\(a\\) and \\(b\\) are finite, anyway.) Recall that the continuous uniform distribution means that \\(X\\) has PDF \\[ f(x) = \\begin{cases} 0 & x &lt; a \\\\ \\displaystyle{\\frac{1}{b-a}} & a \\leq x \\leq b \\\\ 0 & x &gt; b . \\end{cases} \\] Comparing this equation with Equation 2.1, we then have to choose \\[\\phi(x) = \\frac{h(x)}{f(x)} = (b-a)h(x).\\]\nPutting this all together, we have \\[ \\Exg \\phi(X) = \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\int_a^b (b-a)h(x)\\,\\frac{1}{b-a}\\,\\mathrm{d}x = \\int_a^b h(x) \\,\\mathrm{d}x ,\\] as required. This can then be estimated using the Monte Carlo method.\n\nDefinition 2.2 Consider an integral \\(\\theta = \\int_a^b h(x)\\,\\mathrm{d}x\\). Let \\(f\\) be the probability density function of a random variable \\(X\\) and let \\(\\phi\\) be function such that Equation 2.1 holds. Then the Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) of the integral \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\).\n\n\nExample 2.2 Suppose we want to approximate the integral \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x . \\]\nSince this is an integral on the finite interval \\([0,2]\\), it would seem to make sense to pick \\(X\\) to be uniform on \\([0,2]\\). This means we should take \\[\\phi(x) = \\frac{h(x)}{f(x)} = (2-0)h(x) = 2\\,x^{1.6}(2-x)^{0.7}.\\] We can then approximate this integral in R using the Monte Carlo estimator \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x = \\operatorname{\\mathbb{E}} \\phi(X) \\approx \\frac{1}{n} \\sum_{i=1}^n 2\\,X_i^{1.6} (2-X_i)^{0.7} . \\]\n\nn &lt;- 1e6\nintegrand &lt;- function(x) x^1.6 * (2 - x)^0.7\na &lt;- 0\nb &lt;- 2\nsamples &lt;- runif(n, a, b)\nmean((b - a) * integrand(samples))\n\n[1] 1.445453\n\n\nYou have perhaps noticed that, here and elsewhere, I tend to split my R code up into lots of small bits, perhaps slightly unnecessarily. After all, those 6 lines of code could simply have been written as just 2 lines\n\nsamples &lt;- runif(1e6, 0, 2)\nmean(2 * samples^1.6 * (2 - samples)^0.7)\n\nThere’s nothing wrong with that. However, I find that code is easier to read if divided into small pieces. It also makes it easier to tinker with, if I want to use it to solve some similar but slightly different problem.\n\n\nExample 2.3 Suppose we want to approximate the integral \\[ \\int_{-\\infty}^{+\\infty}\n\\mathrm{e}^{-0.1|x|} \\cos x \\, \\mathrm{d}x . \\] This one is an integral on the whole real line, so we can’t take a uniform distribution. Maybe we should take \\(f(x)\\) to be the PDF of a normal distribution, and then put \\[ \\phi(x) = \\frac{h(x)}{f(x)} = \\frac{\\mathrm{e}^{-0.1|x|} \\cos x}{f(x)} . \\]\nBut which normal distribution should we take? Well, we’re allowed to take any one – we will still get an accurate estimate in the limit as \\(n \\to \\infty\\). But we’d like an estimator that gives accurate results at moderate-sized \\(n\\), and picking a “good” distribution for \\(X\\) will help that.\nWe’ll probably get the best results if we pick a distribution that is likely to mostly take values where \\(h(x)\\) is big – or, rather, where the absolute value \\(|h(x)|\\) is big, to be precise. That is because we don’t want to “waste” too many samples where \\(h(x)\\) is very small, because they don’t contribute much to the integral. But we don’t want to “miss” – or only sample very rarely – places where \\(h(x)\\) is big, which contribute a lot to the integral.\nLet’s have a look at the graph of \\(h(x) = \\mathrm{e}^{-0.1|x|} \\cos x\\).\n\n\nCode for drawing this graph\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\n\ncurve(\n  integrand, n = 1001, from = -55, to = 55,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(-50,50)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nThis suggests to me that a mean of 0 and a standard deviation of 20 might work quite well, since this will tend to take values in \\([-40,40]\\) or so.\nWe will use R’s function dnorm() for the probability density function of the normal distribution (which saves us from having to remember what that is).\n\nn &lt;- 1e6\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\npdf       &lt;- function(x) dnorm(x, 0, 20)\nphi       &lt;- function(x) integrand(x) / pdf(x)\n\nsamples &lt;- rnorm(n, 0, 20)\nmean(phi(samples))\n\n[1] 0.2151045\n\n\n\nNext time: We will analyse the accuracy of these Monte Carlo estimates.\n\nSummary:\n\nThe indicator \\(\\Ind_A(x)\\) function of a set \\(A\\) is 1 if \\(x \\in A\\) or 0 if \\(x \\notin A\\).\nWe can estimate a probability \\(\\mathbb P(X \\in A)\\) by using the Monte Carlo estimate for \\(\\Exg\\Ind_A(X)\\).\nWe can estimate an integral \\(\\int h(x) \\, \\mathrm{d}x\\) by using a Monte Carlo estimate with \\(\\phi(x)\\,f(x) = h(x)\\).\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html",
    "href": "lectures/L03-mc-error-1.html",
    "title": "3  Monte Carlo error I: theory",
    "section": "",
    "text": "3.1 Estimation error\nToday we are going to analysing the accuracy of Monte Carlo estimation. But before talking about Monte Carlo estimation specifically, let’s first remind ourselves of some concepts about error in statistical estimation more generally. We will use the following definitions.\nUsually, the main goal of estimation is to get the mean-square error of an estimate as small as possible. This is because the MSE measures by what distance we are missing on average. It can be more convenient to discuss the root-mean-square error, as that has the same units as the parameter being measured. (If \\(\\theta\\) and \\(\\widehat{\\theta}\\) are in metres, say, then the MSE is in metres-squared, whereas the RMSE error is in metres again.)\nIt’s nice to have an unbiased estimator – that is, one with bias 0. This is because bias measures any systematic error in a particular direction. However, unbiasedness by itself is not enough for an estimate to be good – we need low variance too. (Remember the old joke about the statistician who misses his first shot ten yards to the left, misses his second shot ten yards to the right, then claims to have “hit the target on average.”)\n(Remember also that “bias” is simply the word statisticians use for \\(\\mathbb E(\\widehat\\theta - \\theta)\\); we don’t mean “bias” in the derogatory way it is sometimes used in political arguments, for example.)\nYou probably also remember the relationship between the mean-square error, the bias, and the variance:\n(There’s a proof in Voss, An Introduction to Statistical Computing, Proposition 3.14, if you’ve forgotten.)\nSince the bias contributes to the mean-square error, that’s another reason to like estimator with low – or preferably zero – bias. (That said, there are some situations where there’s a “bias–variance tradeoff”, where allowing some bias reduces the variance and so can reduce the MSE. It turns out that Monte Carlo is not one of these cases, however.)",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#estimation-error",
    "href": "lectures/L03-mc-error-1.html#estimation-error",
    "title": "3  Monte Carlo error I: theory",
    "section": "",
    "text": "Definition 3.1 Let \\(\\widehat\\theta\\) be an estimator of a parameter \\(\\theta\\). Then we have the following definitions of the estimate \\(\\widehat\\theta\\):\n\nThe bias is \\(\\operatorname{bias}\\big(\\widehat\\theta\\big) = \\mathbb E\\big(\\widehat\\theta - \\theta\\big)  = \\mathbb E\\widehat\\theta - \\theta\\).\nThe mean-square error is \\(\\operatorname{MSE}\\big(\\widehat\\theta\\big) = \\mathbb E \\big(\\widehat\\theta - \\theta\\big)^2\\).\nThe root-mean-square error is the square-root of the mean-square error, \\[\\operatorname{RMSE}\\big(\\widehat\\theta\\big) = \\sqrt{\\operatorname{MSE}(\\widehat\\theta)} = \\sqrt{\\mathbb E (\\widehat\\theta - \\theta)^2} . \\]\n\n\n\n\n\n\n\nTheorem 3.1   \\(\\operatorname{MSE}\\big(\\widehat\\theta\\big) = \\operatorname{bias}\\big(\\widehat\\theta\\big)^2 + \\operatorname{Var}\\big(\\widehat\\theta\\big)\\).",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#bias-and-error-of-the-monte-carlo-estimator",
    "href": "lectures/L03-mc-error-1.html#bias-and-error-of-the-monte-carlo-estimator",
    "title": "3  Monte Carlo error I: theory",
    "section": "3.2 Bias and error of the Monte Carlo estimator",
    "text": "3.2 Bias and error of the Monte Carlo estimator\nIn this lecture, we’re going to be looking more carefully at the size of the errors made by the Monte Carlo estimator \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) . \\]\nOur main result is the following.\n\nTheorem 3.2 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and \\(\\theta = \\Exg\\phi(X)\\). Let \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] be the Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\({\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\[{\\displaystyle \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} = \\frac{1}{\\sqrt{n}} \\operatorname{sd}\\big(\\phi(X)\\big)}. \\]\n\n\nBefore we get to the proof, let’s recap some relevant probability.\nLet \\(Y_1, Y_2, \\dots\\) be IID random variables with common expectation \\(\\mathbb EY_1 = \\mu\\) and common variance \\(\\operatorname{Var}(Y_1) = \\sigma^2\\). Consider the mean of the first \\(n\\) random variables, \\[ \\overline{Y}_n = \\frac{1}{n} \\sum_{i=1}^n Y_i . \\] Then the expectation of \\(\\overline{Y}_n\\) is \\[ \\mathbb E \\overline{Y}_n = \\mathbb E\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\frac{1}{n}\n\\sum_{i=1}^n \\mathbb{E}Y_i = \\frac{1}{n}\\,n\\mu = \\mu . \\] The variance of \\(\\overline{Y}_n\\) is \\[ \\operatorname{Var}\\big(  \\overline{Y}_n \\big)= \\operatorname{Var} \\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\bigg(\\frac{1}{n}\\bigg)^2\n\\sum_{i=1}^n \\operatorname{Var}(Y_i) = \\frac{1}{n^2}\\,n\\sigma^2 = \\frac{\\sigma^2}{n} , \\] where, for this one, we used the independence of the random variables.\n\nProof. Apply the probability facts from above with \\(Y = \\phi(X)\\). This gives:\n\n\\(\\Ex \\widehat{\\theta}_n^{\\mathrm{MC}} = \\Ex \\overline Y_n = \\Ex Y = \\Exg \\phi(X)\\), so \\(\\operatorname{bias}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\Exg \\phi(X) - \\Exg \\phi(X) = 0\\).\n\\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\operatorname{Var}\\big(\\overline Y_n\\big) = \\frac{1}{n} \\operatorname{Var}(Y) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nUsing Theorem 3.1, \\[\\operatorname{MSE}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\operatorname{bias}(\\widehat{\\theta}_n^{\\mathrm{MC}})^2 + \\operatorname{Var}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = 0^2 + \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) . \\]\nTake the square root of part 3.\n\n\nThere’s a problem here, though. The reason we are doing Monte Carlo estimation in the first place is that we couldn’t calculate \\(\\Exg \\phi(X)\\). So it seems very unlikely we’ll be able to calculate the variance \\(\\operatorname{Var}(\\phi(X))\\) either.\nBut we can estimate the variance from our samples too: by taking the sample variance of our samples \\(\\phi(x_i)\\). That is, we can estimate the variance of the Monte Carlo estimator by \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\big(\\phi(X_i) - \\widehat{\\theta}_n^{\\mathrm{MC}} \\big)^2 . \\] Then we can similarly estimate the mean-square and root-mean-square errors by \\[ \\text{MSE} \\approx \\frac{1}{n}S^2 \\qquad \\text{and} \\qquad \\text{RMSE} \\approx \\sqrt{\\frac{1}{n} S^2} = \\frac{1}{\\sqrt{n}}S  \\] respectively.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#example",
    "href": "lectures/L03-mc-error-1.html#example",
    "title": "3  Monte Carlo error I: theory",
    "section": "3.3 Example",
    "text": "3.3 Example\n\nExample 3.1 Let’s go back to the very first example in the module, Example 1.1, where we were trying to find the expectation of an \\(\\operatorname{Exp}(2)\\) random variable. We used this R code:\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.5001026\n\n\n(Because Monte Carlo estimation is random, this won’t be the exact same estimate we had before, of course.)\nSo if we want to investigate the error, we can use the sample variance of these samples. We will use the sample variance function var() to calculate the sample variance.\n\nvar_est &lt;- var(samples)\nMSEest  &lt;- var_est / n\nRMSEest &lt;- sqrt(MSEest)\nc(var_est, MSEest, RMSEest)\n\n[1] 2.508338e-01 2.508338e-07 5.008331e-04\n\n\nThe first number is var_est \\(= 0.251\\), the sample variance of our \\(\\phi(x_i)\\)s: \\[ s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\big(\\phi(x_i) - \\widehat{\\theta}_n^{\\mathrm{MC}}\\big)^2 . \\] This should be a good estimate of the true variance \\(\\operatorname{Var}(\\phi(X))\\). In calculating this, we used R’s var() function, which calculate the sample variance of some data.\nThe second number is MSEest \\(= 2.51\\times 10^{-7}\\), our estimate of the mean-square error. Since \\(\\operatorname{MSE}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\frac{1}{n} \\operatorname{Var}(\\phi(X))\\), then \\(\\frac{1}{n} s^2\\) should be a good estimate of the MSE.\nThe third number is RMSEest \\(= 5.01\\times 10^{-4}\\) our estimate of the root-mean square error, which is simply the square-root of our estimate of the mean-square error.\n\nNext time: We’ll continue analysing Monte Carlo error, with more examples.\n\nSummary:\n\nThe Monte Carlo estimator is unbiased.\nThe Monte Carlo estimator has mean-square error \\(\\Var(\\phi(X))/n\\), so the root-mean-square error scales like \\(1/\\sqrt{n}\\).\nThe mean-square error can be estimated by \\(S^2 / n\\), where \\(S^2\\) is the sample variance of \\(\\phi(X)\\).\n\nOn Problem Sheet 1, you should now be able to answers Questions 1–6, except 2(c).\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.2.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html",
    "href": "lectures/L04-mc-error-2.html",
    "title": "4  Monte Carlo error II: practice",
    "section": "",
    "text": "4.1 Recap\nLet’s recap where we’ve got to. We know that the Monte Carlo estimator for \\(\\theta = \\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) .\\] Last time, we saw that the Monte Carlo estimator is unbiased, and that its mean-square and root-mean-square errors are \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) \\qquad \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} . \\] We saw that these themselves can be estimated as \\(S^2/n\\) and \\(S/\\sqrt{n}\\) respectively, where \\(S^2\\) is the sample variance of the \\(\\phi(X_i)\\)s.\nLet’s do one more example before moving on.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#recap",
    "href": "lectures/L04-mc-error-2.html#recap",
    "title": "4  Monte Carlo error II: practice",
    "section": "",
    "text": "Example 4.1 In Example 2.1, we were estimating \\(\\mathbb P(Z &gt; 2)\\), where \\(Z\\) is a standard normal.\nOur code was\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.02243\n\n\nSo our root-mean-square error can be approximated as\n\nRMSEest &lt;- sqrt(var(samples &gt; 2) / n)\nRMSEest\n\n[1] 0.0001493805",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#confidence-intervals",
    "href": "lectures/L04-mc-error-2.html#confidence-intervals",
    "title": "4  Monte Carlo error II: practice",
    "section": "4.2 Confidence intervals",
    "text": "4.2 Confidence intervals\nSo far, we have described our error tolerance in terms of the MSE or RMSE. But we could have talked about “confidence intervals” or “margins of error” instead. This might be easier to understand for non-mathematicians, for whom “root-mean-square error” doesn’t really mean anything.\nHere, we will want to appeal to the central limit theorem approximation. A bit more probability revision: Let \\(Y_1, Y_2, \\dots\\) be IID again, with expectation \\(\\mu\\) and variance \\(\\sigma^2\\). Write \\(\\overline Y_n\\) for the mean. We’ve already reminded ourselves that \\(\\mathbb E \\overline Y_n = \\mu\\) and \\(\\Var(\\overline{Y}_n) = \\sigma^2/n\\). But the central limit theorem says that the distribution of \\(\\overline Y_n\\) is approximately normally distributed with those parameters, so \\(\\overline Y_n \\approx \\operatorname{N}(\\mu, \\sigma^2/n)\\) when \\(n\\) is large. (This is an informal statement of the central limit theorem: you probably know some more formal ways to more precisely state the it, but this will do for us.)\nRecall that, in the normal distribution \\(\\operatorname{N}(\\mu, \\sigma^2)\\), we expect to be within \\(1.96\\) standard deviations of the mean with 95% probability. More generally, the interval \\([\\mu - q_{1-\\alpha/2}\\sigma, \\mu + q_{1-\\alpha/2}\\sigma]\\), where \\(q_{1-\\alpha/2}\\) is the \\((1- \\frac{\\alpha}{2})\\)-quantile of the normal distribution, contains the true value with probability approximately \\(1 - \\alpha\\).\nWe can form an approximate confidence interval for a Monte Carlo estimate using this idea. We have our Monte Carlo estimator \\(\\widehat{\\theta}_n^\\mathrm{MC}\\) as our estimator of the \\(\\mu\\) parameter, and our estimator of the root-mean-square error \\(S/\\sqrt{n}\\) as our estimator of the \\(\\sigma\\) parameter. So our confidence interval is estimated as \\[\\bigg[ \\widehat{\\theta}_n^\\mathrm{MC} - q_{1-\\alpha/2}\\,\\frac{S}{\\sqrt{n}}, \\ \\widehat{\\theta}_n^\\mathrm{MC} + q_{1-\\alpha/2}\\,\\frac{S}{\\sqrt{n}} \\bigg] . \\]\n\nExample 4.2 We continue the example of Example 2.1 and Example 4.1, where we were estimating \\(\\mathbb P(Z &gt; 2)\\) for \\(Z\\) a standard normal.\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest   &lt;- mean(samples &gt; 2)\nRMSEest &lt;- sqrt(var(samples &gt; 2) / n)\nMCest\n\n[1] 0.022621\n\n\nOur confidence interval is estimates as follows\n\nalpha &lt;- 0.05\nquant &lt;- qnorm(1 - alpha / 2)\nc(MCest - quant * RMSEest, MCest + quant * RMSEest)\n\n[1] 0.02232957 0.02291243",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#how-many-samples-do-i-need",
    "href": "lectures/L04-mc-error-2.html#how-many-samples-do-i-need",
    "title": "4  Monte Carlo error II: practice",
    "section": "4.3 How many samples do I need?",
    "text": "4.3 How many samples do I need?\nIn our examples we’ve picked the number of samples \\(n\\) for our estimator, then approximated the error based on that. But we could do things the other way around – fix an error tolerance that we’re willing to deal with, then work out what sample size we need to achieve it.\nWe know that the root-mean-square error is \\[ \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} \\] So if we want to get the RMSE down to \\(\\epsilon\\), say, then this shows that we need \\[ n = \\frac{1}{\\epsilon^2} \\Var\\big(\\phi(X)\\big) . \\]\nWe still have a problem here, though. We (usually) don’t know \\(\\Var(\\phi(X))\\). But we can’t even estimate \\(\\Var(\\phi(X))\\) until we’ve already taken the samples. But we can use this idea with a three-step process:\n\nRun an initial “pilot” Monte Carlo algorithm with a small number of samples \\(n\\). Use the results of the “pilot” to estimate the variance \\(S^2 \\approx \\Var(\\phi(X))\\). We want \\(n\\) small enough that this runs very quickly, but big enough that we get a reasonably OK estimate of the variance.\nPick a desired RMSE accuracy \\(\\epsilon\\). We now know that we require roughly \\(N = S^2 / \\epsilon^2\\) samples to get our desired accuracy.\nRun the “real” Monte Carlo algorithm with this big number of samples \\(N\\). We will put up with this being quite slow, because we know we’re definitely going to get the error tolerance we need.\n\n(We could potentially use further steps, where we now check the variance with the “real” big-\\(N\\) samples, and, if we learn we had underestimated in Step 1, take even more samples to correct for this.)\n\nExample 4.3 Let’s try this with Example 1.2 from before. We were trying to estimate \\(\\mathbb{E}(\\sin X)\\), where \\(X \\sim \\operatorname{N}(1, 2^2)\\).\nWe’ll start with just \\(n = 1000\\) samples, for our pilot study.\n\nn_pilot &lt;- 1000\nsamples &lt;- rnorm(n_pilot, 1, 2)\nvar_est &lt;- var(sin(samples))\nvar_est\n\n[1] 0.4850801\n\n\nThis was very quick! We won’t have got a super-accurate estimate of \\(\\mathbb E\\phi(X)\\), but we have a reasonable idea of roughly what \\(\\operatorname{Var}(\\phi(X))\\) is. This will allow us to pick out “real” sample size in order to get a root-mean-square error of \\(10^{-4}\\).\n\nepsilon &lt;- 1e-4\nn_real  &lt;- round(var_est / epsilon^2)\nn_real\n\n[1] 48508011\n\n\nThis tells us that we will need about 50 million samples! This is a lot, but now we know we’re going to get the accuracy we want, so it’s worth it. (In this particular case, 50 million samples will only take a few second on a modern computer. But generally, once we know our code works and we know how many samples we will need for the desired accuracy, this is the sort of thing that we could leave running overnight or whatever.)\n\nsamples &lt;- rnorm(n_real, 1, 2)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.1139476\n\nRMSEest &lt;- sqrt(var(sin(samples)) / n_real)\nRMSEest\n\n[1] 0.0001002164\n\n\nThis was very slow, of course. But we see that we have indeed got our Monte Carlo estimate to (near enough) the desired accuracy.\n\nGenerally, if we want a more accurate Monte Carlo estimator, we can just take more samples. But the equation \\[ n = \\frac{1}{\\epsilon^2} \\Var\\big(\\phi(X)\\big) \\] is actually quite bad news. To get an RMSE of \\(\\epsilon\\) we need order \\(1/\\epsilon^2\\) samples. That’s not good. Think of it like this: to double the accuracy we need to quadruple the number of samples. Even worse: to get “one more decimal place of accuracy” means dividing \\(\\epsilon\\) by ten; but that means multiplying the number of samples by one hundred!\nMore samples take more time, and cost more energy and money. Wouldn’t it be nice to have some better ways of increasing the accuracy of a Monte Carlo estimate besides just taking more and more samples?\nNext time: We begin our study of clever “variance reduction” methods for Monte Carlo estimation.\n\nSummary:\n\nWe can approximate confidence intervals for a Monte Carlo estimate by using a normal approximation.\nTo get the root-mean-square error below \\(\\epsilon\\) we need \\(n = \\Var(\\phi(X))/\\epsilon^2\\) samples.\nWe can use a two-step process, where a small “pilot” Monte Carlo estimation allows us to work out how many samples we will need for the big “real” estimation.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 3.2.2–3.2.4.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html",
    "href": "lectures/L05-cv.html",
    "title": "5  Control variate",
    "section": "",
    "text": "5.1 Variance reduction\nLet’s recap where we’ve got to. The Monte Carlo estimator of \\(\\theta = \\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i), \\] where \\(X_1, X_2, \\dots, X_n\\) are IID random samples from \\(X\\). The mean-square error of this estimator is \\[{\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} . \\] If we want a more accurate estimate, we can just take more samples \\(n\\). But the problem is that the root-mean-square error scales like \\(1/\\sqrt{n}\\). To double the accuracy, we need four times as many samples; for one more decimal place of accuracy, we need one hundred times as many samples.\nAre there other ways we could reduce the error of Monte Carlo estimation, so we need fewer samples? That is, can we use some mathematical ingenuity to adapt the Monte Carlo estimate to one with a smaller error?\nWell, the mean-square error is the variance divided by \\(n\\). So if we can’t (or don’t want to) increase \\(n\\), perhaps we can decrease the variance instead? Strategies to do this are called variance reduction strategies. In this module, we will look at three variance reduction strategies:",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html#variance-reduction",
    "href": "lectures/L05-cv.html#variance-reduction",
    "title": "5  Control variate",
    "section": "",
    "text": "Control variate: We can “anchor” our estimate of \\(\\Exg \\phi(X)\\) to a similar but easier-to-calculate value \\(\\Exg \\psi(X)\\). (This lecture)\nAntithetic variables: Instead of using independent samples, we could use correlated samples. If the correlation is negative this can improve our estimate. (Lectures 6 and 7)\nImportance sampling: Instead of sampling from \\(X\\), sample from some other more suitable distribution instead, then readjust the answer we get. (Lectures 8 and 9)",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html#control-variate-estimation",
    "href": "lectures/L05-cv.html#control-variate-estimation",
    "title": "5  Control variate",
    "section": "5.2 Control variate estimation",
    "text": "5.2 Control variate estimation\nIn last Friday’s lecture, I polled the class on this question: Estimate the average time it takes to fly from London to Washington D.C.\n\nThe actual answer is: 8 hours and 29 minutes.\nThe mean guess for the class was: 7 hours and 37 minutes (52 minutes too little)\nThe root-mean-square error for the guesses was: 133 minutes\n\nAfter you’d guessed, I gave the following hint: Hint: The average time it takes to fly from London to New York is 8 hours and 10 minutes. After the hint:\n\nThe mean guess for the class was: 8 hours and 45 minutes (16 minutes too much)\nThe root-mean-square error for the guesses was: 49 minutes\n\nSo after the hint, the error of the class was reduced by 63%.\nWhy did the hint help? We were trying to estimate \\(\\theta^{\\mathrm{DC}}\\), the distance to D.C. But that’s a big number, and the first estimates had a big error (over an hour, on average). After the hint, I expect most people thought something like this: “The answer \\(\\theta^{\\mathrm{DC}}\\) is going to be similar to the \\(\\theta^{\\mathrm{NY}} =\\) 8:10 to New York, but Washington D.C. is a bit further, so I should increase the number a bit, but not too much.”\nTo be more mathematical, we could write \\[\\theta^{\\mathrm{DC}} = \\theta^{\\mathrm{DC}} + \\big(\\theta^{\\mathrm{NY}} - \\theta^{\\mathrm{NY}}\\big)= \\underbrace{\\big(\\theta^{\\mathrm{DC}} - \\theta^{\\mathrm{NY}}\\big)}_{\\text{small}} + \\underbrace{\\theta^{\\mathrm{NY}}\\vphantom{\\big)}}_{\\text{known}} . \\] In that equation, the second term, \\(\\theta^{\\mathrm{NY}} =\\) 8:10 was completely known, so had error 0, while the first term \\(\\theta^{\\mathrm{DC}} - \\theta^{\\mathrm{NY}}\\) (actually 19 minutes) was a small number, so only had a small error.\nThis idea of improving an estimate by “anchoring” it to some known value is called controlled estimation. It is a very useful idea in statistics (and in life!).\nWe can apply this idea to Monte Carlo estimation too. Suppose we are trying to estimate \\(\\theta = \\Exg \\phi(X)\\). We could look for a function \\(\\psi\\) that is similar to \\(\\phi\\) (at least for the values of \\(x\\) that have high probability for the random variable \\(X\\)), but where we know for certain what \\(\\Exg \\psi(X)\\) is. Then we can write \\[ \\theta = \\Exg \\phi(X) = \\Exg \\big(\\phi(X) - \\psi(X) + \\psi(X)\\big) = \\underbrace{\\Exg\\big(\\phi(X) - \\psi(X)\\big)}_{\\text{estimate this with Monte Carlo}} + \\underbrace{\\Exg \\psi(X)\\vphantom{\\big)}}_{\\text{known}} . \\]\nHere, \\(\\psi(X)\\) is known as the control variate.\n\nDefinition 5.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(\\psi\\) be a function such that \\(\\eta = \\Exg\\psi(X)\\) is known. Suppose that \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\). Then the control variate Monte Carlo estimate \\(\\widehat\\theta_n^{\\mathrm{CV}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta . \\]\n\n\nExample 5.1 Let’s try to estimate \\(\\Ex \\cos(X)\\), where \\(X \\sim \\operatorname{N}(0,1)\\) is a standard normal distribution.\nWe could do this the “usual” Monte Carlo way.\n\nn &lt;- 1e6\nphi &lt;- function(x) cos(x)\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(phi(samples))\nMCest\n\n[1] 0.6066079\n\n\nBut we could see if we can do better with a control variate. But what should we pick for the control function \\(\\psi\\)? We want something that’s similar to \\(\\phi(x) = \\cos(x)\\), but where we can actually calculate the expectation.\nHere’s a suggestion. If we remember our Taylor series, we know that, for \\(x\\) near \\(0\\), \\[ \\cos x \\approx 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots . \\] So how about taking the first two nonzero terms in the Taylor series \\[ \\psi(x) = 1 - \\frac{x^2}{2} . \\] That is quite close to \\(\\cos x\\), at least for the values of \\(x\\) near 0 that \\(X \\sim \\operatorname{N}(0,1)\\) is most likely to take.\n\n\nCode for drawing this graph\ncurve(\n  cos(x), from = -4.5, to = 4.5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"\", xlim = c(-4,4), ylim = c(-1.2,1.2)\n)\ncurve(1 - x^2 / 2, add = TRUE, col = \"red\", lwd = 2)\nlegend(\n  \"topright\", c(\"cos x\", expression(1 - x^2 / 2)),\n  lwd = c(3, 2), col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nNot only that, but we know that for \\(Y \\sim \\operatorname{N}(\\mu, \\sigma^2)\\) we have \\(\\Ex Y^2 = \\mu^2 + \\sigma^2\\). So \\[ \\Exg \\psi(X) = \\Exg \\left(1 - \\frac{X^2}{2} \\right) = 1 - \\frac{\\Ex X^2}{2} = 1 - \\frac{0^2 + 1}{2} = \\frac12 . \\]\nSo our control variate estimate is:\n\npsi &lt;- function(x) 1 - x^2 / 2\nCVest &lt;- mean(phi(samples) - psi(samples)) + 1/2\nCVest\n\n[1] 0.6064711",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html#error-of-control-variate-estimate",
    "href": "lectures/L05-cv.html#error-of-control-variate-estimate",
    "title": "5  Control variate",
    "section": "5.3 Error of control variate estimate",
    "text": "5.3 Error of control variate estimate\nWhat is the error in a control variate estimate?\n\nTheorem 5.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and \\(\\theta = \\Exg\\phi(X)\\). Let \\(\\psi\\) be a function such that \\(\\eta \\Exg\\psi(X)\\) is known. Let \\[ \\widehat{\\theta}_n^{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta\\] be the control variate Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is \\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}\\).\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is \\({\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}\\).\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is \\({\\displaystyle \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = \\frac{1}{\\sqrt{n}} \\sqrt{\\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}}\\).\n\n\n\nProof. This is very similar to Theorem 3.2, so we’ll just sketch the important differences.\nIn part 1, we have \\[\\begin{align*}\n\\Exg \\widehat{\\theta}_n^{\\mathrm{CV}}\n  &= \\Exg \\left(\\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big)\\right) + \\eta \\\\\n  &= \\frac{1}{n}\\Exg \\left(\\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big)\\right) + \\eta \\\\\n  &= \\frac{n}{n}\\Exg\\big(\\phi(X) - \\psi(X)\\big) + \\eta \\\\\n  &= \\Exg\\phi(X) - \\Exg\\psi(X) + \\eta \\\\\n  &= \\Exg\\phi(X) ,\n\\end{align*}\\] since \\(\\eta = \\Exg\\psi(X)\\). So the estimator is unbiased.\nFor part 2, remembering that \\(\\eta = \\Exg \\psi(X)\\) is a constant, so doesn’t affect the variance, we have \\[\\begin{align*}\n\\Var \\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big)\n&= \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta \\right) \\\\\n&= \\Big( \\frac{1}{n}\\Big)^2 \\Var \\left(\\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) \\right) \\\\\n&= \\frac{n}{n^2} \\Var \\big(\\phi(X) - \\psi(X)\\big) \\\\\n&= \\frac{1}{n} \\Var \\big(\\phi(X) - \\psi(X)\\big) .\n\\end{align*}\\]\nParts 3 and 4 follow in the usual way.\n\nThis tells us that a control variate Monte Carlo estimate is good when the variance of \\(\\phi(X) - \\psi(X)\\). This variance is likely to be small if \\(\\phi(X) - \\psi(X)\\) is usually small – although, to be more precise, it’s more important for \\(\\phi(X) - \\psi(X)\\) to be consistent, rather than small per se.\nAs before, we can’t usually calculate the variance \\(\\Var(\\phi(X) - \\psi(X))\\) exactly, but we can estimate it from the samples. Again, we use the sample variance \\[S^2 = \\frac{1}{n-1}\\sum_{i=1}^n \\Big(\\big(\\phi(X_i) - \\psi(X_i)\\big) - \\big(\\widehat\\theta_n^{\\mathrm{CV}} + \\eta\\big)\\Big)^2 , \\] and estimate the MSE and RMSE by \\(S^2 / n\\) and \\(S / \\sqrt{n}\\) respectively.\n\nExample 5.2 We return to Example 5.1, where we were estimating \\(\\Ex \\cos(X)\\) for \\(X \\sim \\operatorname{N}(0,1)\\).\nThe naive Monte Carlo estimate had mean-square and root-mean-square error\n\nn &lt;- 1e6\nphi &lt;- function(x) cos(x)\nsamples &lt;- rnorm(n)\nMC_MSE &lt;- var(phi(samples)) / n\nc(MC_MSE, sqrt(MC_MSE))\n\n[1] 1.984850e-07 4.455166e-04\n\n\nThe variance and root-mean-square error of our control variate estimate, on the other hand, are\n\npsi &lt;- function(x) 1 - x^2 / 2\nCV_MSE &lt;- var(phi(samples) - psi(samples)) / n\nc(CV_MSE, sqrt(CV_MSE))\n\n[1] 9.299204e-08 3.049460e-04\n\n\nThis was a success! The mean-square error roughly halved, from \\(2\\times 10^{-7}\\) to \\(9.3\\times 10^{-8}\\). This meant the root-mean-square went down by about a third, from \\(4.5\\times 10^{-4}\\) to \\(3\\times 10^{-4}\\).\nHalving the mean-square error would normally have required doubling the number of samples \\(n\\), so we have effectively doubled the sample size by using the control variate.\n\nNext time: We look at our second variance reduction technique: antithetic variables.\n\nSummary:\n\nVariance reduction techniques attempt to improve on Monte Carlo estimation making the variance smaller.\nIf we know \\(\\eta = \\Exg \\psi(X)\\), then the control variate Monte Carlo estimate is \\[ \\widehat{\\theta}_n^{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta.\\]\nThe mean-square error of the control variate Monte Carlo estimate is \\[{\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}.\\]\n\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.3.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html",
    "href": "lectures/L06-antithetic-1.html",
    "title": "6  Antithetic variables I",
    "section": "",
    "text": "6.1 Estimation with correlation\nThis lecture and the next, we will be looking at our second variance reduction method for Monte Carlo estimation: the use of antithetic variables.” The word “antithetic” refers to using negative correlation to reduce the variance an estimator.\nLet’s start with the simple example of estimating an expectation from \\(n = 2\\) samples. Suppose \\(Y\\) has expectation \\(\\mu = \\Ex Y\\) and variance \\(\\Var(Y) = \\sigma^2\\). Suppose \\(Y_1\\) and \\(Y_2\\) are independent samples from \\(Y\\). Then the Monte Carlo estimator is \\[ \\overline Y = \\tfrac12(Y_1 + Y_2) . \\] This estimator is unbiased, since \\[ \\Ex \\overline Y = \\Ex \\big(\\tfrac12(Y_1 + Y_2)\\big) = \\tfrac12 ( \\Ex Y_1 + \\Ex Y_2 ) = \\tfrac12 (\\mu + \\mu) = \\mu . \\] Thus the mean-square error equals the variance, which is \\[ \\Var \\big( \\overline Y\\big) = \\Var \\big(\\tfrac12(Y_1 + Y_2)\\big) =\\tfrac14 \\big( \\Var(Y_1) + \\Var(Y_2) \\big)= \\tfrac14 (\\sigma^2 + \\sigma^2) = \\tfrac12 \\sigma^2 . \\]\nBut what if \\(Y_1\\) and \\(Y_2\\) still have the same distribution as \\(Y\\) but now are not independent? The expectation is still the same, so the estimator is still unbiased. But the variance (and hence mean-square error) is now \\[ \\Var \\big( \\overline Y\\big) = \\Var \\big(\\tfrac12(Y_1 + Y_2)\\big) =\\tfrac14 \\big( \\Var(Y_1) + \\Var(Y_2) + 2 \\Cov(Y_1, Y_2) \\big) . \\] Write \\(\\rho\\) for the correlation \\[ \\rho = \\Corr(Y_1, Y_2) = \\frac{\\Cov(Y_1, Y_2)}{\\sqrt{\\Var(Y_1) \\Var(Y_2)}} = \\frac{\\Cov(Y_1, Y_2)}{\\sqrt{\\sigma^2 \\times \\sigma^2}} = \\frac{\\Cov(Y_1, Y_2)}{\\sigma^2} . \\] (Remember that \\(-1 \\leq \\rho \\leq +1\\).) Then the variance is \\[ \\Var \\big( \\overline Y\\big) = \\tfrac14 ( \\sigma^2 + \\sigma^2 + 2 \\rho \\sigma^2 ) = \\frac{1+\\rho}{2} \\,\\sigma^2 . \\]\nWe can compare this with the variance \\(\\frac12 \\sigma^2\\) from the independent-sample case:",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html#estimation-with-correlation",
    "href": "lectures/L06-antithetic-1.html#estimation-with-correlation",
    "title": "6  Antithetic variables I",
    "section": "",
    "text": "If \\(Y_1\\) and \\(Y_2\\) are positively correlated, in that \\(\\rho &gt; 0\\), then the variance, and hence the mean-square error, has got bigger. This means the estimator is worse. This is because, with positive correlation, errors compound each other – if one sample is bigger than average, then the other one is likely to be bigger than average too; while if one sample is smaller than average, then the other one is likely to be smaller than average too.\nIf \\(Y_1\\) and \\(Y_2\\) are negatively correlated, in that \\(\\rho &lt; 0\\), then the variance, and hence the mean-square error, has got smaller. This means the estimator is better. This is because, with negative correlation, errors compensate for each other – if one sample is bigger than average, then the other one is likely to be smaller than average, which will help “cancel out” the error.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html#monte-carlo-with-antithetic-variables",
    "href": "lectures/L06-antithetic-1.html#monte-carlo-with-antithetic-variables",
    "title": "6  Antithetic variables I",
    "section": "6.2 Monte Carlo with antithetic variables",
    "text": "6.2 Monte Carlo with antithetic variables\nWe have seen that negative correlation helps improve estimation from \\(n=2\\) samples. How can we make this work in our favour for Monte Carlo simulation with many more samples?\nWe will look at the idea of antithetic pairs. So instead of taking \\(n\\) samples \\[ X_1, X_2, \\dots, X_n \\] that are all independent of each other, we will take \\(n/2\\) pairs of samples \\[ (X_1, X'_1), (X_2, X'_2), \\dots, (X_{n/2}, X'_{n/2}) . \\] (Here, \\(n/2\\) pairs means \\(n\\) samples over all.) Within each pair, \\(X_i\\) and \\(X_i'\\) will not be independent, but between different pairs \\(i \\neq j\\), \\((X_i, X_i')\\) and \\((X_j, X'_j)\\) will be independent.\n\nDefinition 6.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(X'\\) have the same distribution as \\(X\\) (but not necessarily be independent of it). Suppose that \\((X_1, X_1')\\), \\((X_2, X_2')\\), \\(\\dots\\), \\((X_{n/2}, X'_{n/2})\\) are pairs of random samples from \\((X, X')\\). Then the antithetic variables Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{AV}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X'_i) \\big) .\\]\n\nThe expression above for \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) makes it clear that that this is a mean of the sum from each pair. Alternatively, we can rewrite the estimator as \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{2} \\left( \\frac{1}{n/2} \\sum_{i=1}^{n/2} \\phi(X_i) + \\frac{1}{n/2} \\sum_{i=1}^{n/2} \\phi(X_i') \\right) , \\] which highlights that it is the mean of the estimator from the \\(X_i\\)s and the the estimator from the \\(X'_i\\)s.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html#examples",
    "href": "lectures/L06-antithetic-1.html#examples",
    "title": "6  Antithetic variables I",
    "section": "6.3 Examples",
    "text": "6.3 Examples\n\nExample 6.1 Recall Example 2.1 (continued in Example 4.1 and Example 4.2). Here, we were estimating \\(\\mathbb P(Z &gt; 2)\\) for \\(Z\\) a standard normal.\nThe basic Monte Carlo estimate was\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest   &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.022911\n\n\nCan we improve this estimate with an antithetic variable? Well, if \\(Z\\) is a standard normal, then \\(Z' = -Z\\) is also standard normal and is not independent of \\(Z\\). So maybe that could work as an antithetic variable. Let’s try\n\nn &lt;- 1e6\nsamples1 &lt;- rnorm(n / 2)\nsamples2 &lt;- -samples1\nAVest &lt;- (1 / n) * sum((samples1 &gt; 2) + (samples2 &gt; 2))\nAVest\n\n[1] 0.022929\n\n\n\n\nExample 6.2 Let’s consider estimating \\(\\mathbb E \\sin U\\), where \\(U\\) is continuous uniform on \\([0,1]\\).\nThe basic Monte Carlo estimate is\n\nn &lt;- 1e6\nsamples &lt;- runif(n)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.4599953\n\n\nWe used runif(n, min, max) to generate \\(n\\) samples on the interval \\([\\mathtt{min}, \\mathtt{max}]\\). However, if you omit the min and max arguments, then R assumes the default values min = 0, max = 1, which is what we want here.\nIf \\(U\\) is uniform on \\([0,1]\\), then \\(1 - U\\) is also uniform on \\([0,1]\\). We could try using that as an antithetic variable.\n\nn &lt;- 1e6\nsamples1 &lt;- runif(n / 2)\nsamples2 &lt;- 1 - samples1\nAVest &lt;- (1 / n) * sum(sin(samples1) + sin(samples2))\nAVest\n\n[1] 0.4597048\n\n\n\nWe have taken \\(n/2\\) pairs of samples, because that means we have \\(n/2 \\times 2 = n\\) samples over all, which seems like a fair comparison. This is certainly the case if generating the sample and generating its antithetic pair cost roughly the same in terms of time (or energy, or money). However, if generating the first variate of each pair is slow, but then generating the second antithetic variate is much quicker, it might be a fairer comparison to take a full \\(n\\) pairs.\nMore generally, you might put a cost \\(c_1\\) on each first variate and \\(c_2\\) on each antithetic pair. Then one can compare a cost of \\(c_1n\\) for standard Monte Carlo with \\(n\\) samples to a cost of \\((c_1 + c_2)m\\) for antithetic variables Monte Carlo with \\(m\\) pairs.\nAre these antithetic variables estimates an improvement on the basic Monte Carlo estimate? We’ll find out next time.\nNext time: We continue our study of the antithetic variables method with more examples and analysis of the error.\n\nSummary:\n\nEstimation is helped by combining individual estimates that are negatively correlated.\nFor antithetic variables Monte Carlo estimation, we take pairs of non-independent variables \\((X, X')\\), to get the estimator \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X'_i) \\big) . \\]\n\nOn Problem Sheet 1, you should now be able to answer all questions. You should work through this problem sheet in advance of the problems class on Thursday 17 October.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "problems/P1.html",
    "href": "problems/P1.html",
    "title": "Problem Sheet 1",
    "section": "",
    "text": "Full solutions are now available.\n\n\nThis is Problem Sheet 1, which covers material from Lectures 1 to 6. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Thursday 17 October. If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Mondays at 1500.\nThis problem sheet is to help you practice material from the module. It is not for assessment and will not be marked. However, you can get feedback on your work by: (a) being well prepared for the problems class, by making attempts at all questions in advance; (b) speaking up in the problems class, asking for clarification where things aren’t clear and offering ideas on how to solve the questions; (c) discussing your work with colleagues and friends; (d) coming along to my office hours on Mondays at 1500; (e) studying the full solutions when they are released and comparing with your own attempts.\nMany of these questions will require use of the R programming language (for example, by using the program RStudio).\nFull solutions should be released on Friday 18 October.\n\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\\]\n\n\n1.      Let \\(X\\) be uniform on \\([-1,2]\\).\n\n(a)   By hand, calculate the exact value of \\(\\Ex X^4\\).\n\nSolution.\n\\[\\int_{-1}^2 x^4\\,\\frac{1}{2-(-1)}\\,\\mathrm{d}x = \\tfrac13 \\Big[\\tfrac15x^5\\Big]_{-1}^2 = \\tfrac13\\Big(\\tfrac{32}{5}-\\big(-\\tfrac15\\big)\\Big) = \\tfrac{33}{15} = \\tfrac{11}{5} = 2.2\\]\n\n\n\n(b)   Using R, calculate a Monte Carlo estimate for \\(\\Ex X^4\\).\n\nSolution. I used the R code\n\nn &lt;- 1e6\nsamples &lt;- runif(n, -1, 2)\nmean(samples^4)\n\n[1] 2.201916\n\n\n\n\n\n\n2.      Let \\(X\\) and \\(Y\\) both be standard normal distributions. Compute a Monte Carlo estimate of \\(\\Exg \\max\\{X,Y\\}\\). (You may wish to investigate R’s pmax() function.)\n\nSolution. By looking at ?pmax (or maybe searching on Google) I discovered that pmax() gives the “parallel maxima” of two (or more vectors). That is the first element is the maximum of all the first elements of the vectors; the second element is the maximum of all the second elements of the vectors; and so on.\nSo I used the R code\n\nn &lt;- 1e6\nxsamples &lt;- rnorm(n)\nysamples &lt;- rnorm(n)\nmean(pmax(xsamples, ysamples))\n\n[1] 0.5632374\n\n\n\n\n\n3.      You are trapped alone on an island. All you have with you is a tin can (radius \\(r\\)) and a cardboard box (side lengths \\(2r \\times 2r\\)) that it fits snugly inside. You put the can inside the box [left picture].\nWhen it starts raining, each raindrop that falls in the cardboard box might fall into the tin can [middle picture], or might fall into the corners of the box outside the can [right picture].\n\n\n(a)   Using R, simulate rainfall into the box. You may take units such that \\(r = 1\\). Estimate the probability \\(\\theta\\) that a uniformly-random raindrop that falls in the cardboard box falls into the tin can also.\n\nSolution. I set things up so that the box is \\([-1, 1]^2\\), centered at the origin. This means that the inside of the can is the set of points is those \\((x,y)\\) such that \\(x^2 + y^2 \\leq 1\\).\n\nn &lt;- 1e6\nrain_x &lt;- runif(n, -1, 1)\nrain_y &lt;- runif(n, -1, 1)\nin_box &lt;- function(x, y) x^2 + y^2 &lt;= 1\nmean(in_box(rain_x, rain_y))\n\n[1] 0.785122\n\n\n\n\n\n(b)   Calculate exactly the probability \\(\\theta\\).\n\nSolution. The area of the box is \\(2r \\times 2r = 4r^2\\). The area of the can is \\(\\pi r^2\\). So the probability a raindrop landing in the box lands in the can is \\[ \\frac{\\text{area of can}}{\\text{area of box}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\approx 0.785. \\]\n\n\n\n(c)   You realise that, despite being trapped all alone on the island, you now have a way of approximating the value of \\(\\pi\\). If you want to calculate \\(\\pi\\) to 6 decimal places, roughly how many raindrops do you need to fall into the box?\n\nSolution. The phrase “to 6 decimal places” isn’t a precise mathematical one. I’m going to interpret this as getting the root-mean-square error below \\(10^{-6}\\). If you interpret it slightly differently that’s fine – for example, getting the width of a 95% confidence interval below \\(10^{-6}\\) could be another, slightly stricter, criterion.\nOne could work this out by hand. Since the variance of a Bernoulli random variable is \\(p(1-p)\\), the mean-square error of our estimator is \\[ \\frac{\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})}{n} \\approx \\frac{0.169}{n} . \\] So we need \\[n = \\frac{0.169}{(10^{-6})^2} \\approx 169 \\text{ billion} . \\]\nThat said, if we are trapped on our desert island, maybe we don’t know what \\(\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})\\) is. In that case we could do this using the can and the box. Our estimate of the variance is\n\nvar_est &lt;- var(in_box(rain_x, rain_y))\nvar_est / (1e-6)^2\n\n[1] 168705613823\n\n\nWe will probably spend a long time waiting for that much rain!\n\n\n\n\n4.      Let \\(h(x) = 1/(x + 0.1)\\). We wish to estimate \\(\\int_0^5 h(x) \\, \\mathrm{d}x\\) using a Monte Carlo method.\n\n(a)   Estimate the integral using \\(X\\) uniform on \\([0,5]\\).\n\nSolution.\n\nn &lt;- 1e6\nintegrand &lt;- function(x) 1 / (x + 0.1)\nsamples1 &lt;- runif(n, 0, 5)\nmean(5 * integrand(samples1))\n\n[1] 3.925967\n\n\n(In fact, the true answer is \\(\\log(5.1) - \\log(0.1) = 3.932\\), so it looks like this is working correctly.)\n\n\n\n(b)   Can you come up with a choice of \\(X\\) that improves on the estimate from (a)?\n\nSolution. Let’s look at a graph of the integrand \\(h\\).\n\n\nCode for drawing this graph\ncurve(\n  integrand, n = 1001, from = 0, to = 5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(0,5)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nWe see that we get a much bigger contribution to the integral from values near 0. So a random variable that picks values nearer to 0 more often might have a chance of giving a more accurate result.\n\nI decided to try an exponential distribution with rate 1, which should sample the smaller values of \\(x\\) more often.\n\npdf &lt;- function(x) dexp(x, 1)\nphi &lt;- function(x) (integrand(x) / pdf(x)) * (x &lt;= 5)\n\nsamples2 &lt;- rexp(n, 1)\nmean(phi(samples2))\n\n[1] 3.936713\n\n\n(I had to include x &lt;= 5 in the expression for \\(\\phi\\), because my exponential distribution will sometimes take samples above 5, but they should count as 0 in an estimate for the integral between 0 and 5.)\nTo see whether or not this was an improvement, I estimated the mean-square error.\n\nvar(5 * integrand(samples1)) / n\n\n[1] 3.349673e-05\n\nvar(phi(samples2)) / n\n\n[1] 6.100181e-06\n\n\nI found that I had reduced the mean-square error by roughly a factor of 5. \n\n\n\n\n5.      When calculating a Monte Carlo estimate \\(\\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\\), one might wish to first generate the \\(n\\) samples \\((x_1, x_2, \\dots, x_n)\\) and store them, and only then, after all samples are generated, finally calculate the estimate. However, when \\(n\\) is extremely large, storing all \\(n\\) samples uses up a lot of space in a computer’s memory. Describe (in words, in R code, or in a mixture of the two) how the Monte Carlo estimate could be produced using much less memory.\n\nSolution. The idea is to keep a “running total” of the \\(\\phi(x_i)\\)s. Then we only have to store that running total, not all the samples. Once this has been done \\(n\\) times, then divide by \\(n\\) to get the estimate.\nIn R code, this might be something like\n\nn &lt;- 1e6\n\ntotal &lt;- 0\nfor (i in 1:n) {\n  sample &lt;- # sampling code for 1 sample\n  total &lt;- total + phi(sample)\n}\n\nMCest &lt;- total / n\n\n\n\n\n6.      Show that the indicator functions \\(\\mathbb I_A(X)\\) and \\(\\mathbb I_B(X)\\) have correlation 0 if and only if the events \\(\\{X \\in A\\}\\) and \\(\\{X \\in B\\}\\) are independent.\n\nSolution. Recall that two random variables \\(U\\), \\(V\\) have correlation 0 if and only if their covariance \\(\\Cov(U,V) = \\Ex UV - (\\Ex U)(\\Ex V)\\) is 0 too.\nWe know that \\(\\Exg\\Ind_A(X) = \\mathbb P(X \\in A)\\) and \\(\\Exg \\Ind_B(Y) = \\mathbb P(X \\in B)\\). What about \\(\\Exg \\Ind_A(X) \\Ind_B(X)\\)? Well, \\(\\Ind_A(x) \\Ind_B(x)\\) is 1 if and only if both indicator functions equal 1, which is if and only if both \\(x \\in A\\) and \\(x \\in B\\). So \\(\\Exg \\Ind_A(X) \\Ind_B(X) = \\mathbb P(X \\in A \\text{ and } X \\in B)\\).\nSo the covariance is \\[ \\Cov \\big(\\Ind_A(X), \\Ind_B(X) \\big) = \\mathbb P(X \\in A \\text{ and } X \\in B) - \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B) . \\] If this is 0, then \\(\\mathbb P(X \\in A \\text{ and } X \\in B) = \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B)\\), which is precisely the definition of those two events being independent.\n\n\n\n7.      Let \\(X\\) be an exponential distribution with rate 1.\n\n(a)   Estimate \\(\\mathbb EX^{2.1}\\) using the standard Monte Carlo method.\n\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 1)\nmean(samples^2.1)\n\n[1] 2.192614\n\n\n\n\n\n(b)   Estimate \\(\\mathbb EX^{2.1}\\) using \\(X^2\\) as a control variate. (You may recall that if \\(Y\\) is exponential with rate \\(\\lambda\\) then \\(\\mathbb EY^2 = 2/\\lambda^2\\).)\n\nSolution. We have \\(\\Ex X^2 = 2\\). So, re-using the same sample as before (you don’t have to do this – you could take new samples), our R code is as follows.\n\nmean(samples^2.1 - samples^2) + 2\n\n[1] 2.196733\n\n\n\n\n\n(c)   Which method is better?\n\nSolution. The better answer is the one with the smaller mean-square error.\nFor the basic method,\n\nvar(samples^2.1) / n\n\n[1] 2.753474e-05\n\n\nFor the control variate method,\n\nvar(samples^2.1 - samples^2) / n\n\n[1] 6.706706e-07\n\n\nSo the control variates method is much, much better.\n\n\n\n\n8.      Let \\(Z\\) be a standard normal distribution. A statistician has been investigating Monte Carlo estimates of \\(\\mathbb EZ^k\\) for different positive integers values of \\(k\\). Her colleague suggests using \\(Z' = -Z\\) as an antithetic variable. Without running any R code, explain whether or not this is a good idea (a) when \\(k\\) is even; (b) when \\(k\\) is odd.\n\nSolution.\n(a) When \\(k\\) is even, we have \\(Z^k = (-Z)^k\\). So the antithetic variables method just repeats each sample twice. This is obviously no benefit at all, and just wastes time. Indeed, we have perfect positive correlation \\(\\rho = +1\\), which is the “worst-case scenario”.\n(b) When \\(k\\) is odd, we have \\(Z^k = -(-Z)^k\\). In this case we know that \\(\\Ex Z^k = 0\\), because the results for positive \\(z\\) exactly balance out those for negative \\(z\\), so no Monte Carlo sampling is necessary. If our statistician has somehow forgotten that, though, she will get a pleasant surprise! After just two samples, she will get the estimate \\[\\frac{1}{2} \\big(Z_1^k + (-Z_1)^k \\big) = \\frac{1}{2} (Z_1^k - Z_1^k) = 0 ,\\] Thereby getting the result exactly right. Indeed, we have perfect negative correlation \\(\\rho = -1\\), which is the “best-case scenario”.",
    "crumbs": [
      "Monte Carlo estimation",
      "Problem Sheet 1"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html",
    "href": "lectures/L07-antithetic-2.html",
    "title": "7  Antithetic variables II",
    "section": "",
    "text": "7.1 Error with antithetic variables\nRecall from last time the antithetic variables Monte Carlo estimator. We take sample pairs \\[ (X_1, X'_1), (X_2, X'_2), \\dots, (X_{n/2}, X_{n/2}') , \\] where samples are independent between different pairs but not independent within the same pair. The estimator of \\(\\theta = \\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X'_i) \\big) .\\] We hope this is better than the standard Monte Carlo estimator if \\(\\phi(X)\\) and \\(\\phi(X')\\) are negatively correlated.\nIn points 2, 3 and 4, generally the first expression, involving the variance \\(\\operatorname{Var}(\\phi(X) + \\phi(X'))\\), is the most convenient for computation. We can estimate this easily from data using the sample variance in the usual way (as we will in the examples below).\nThe second expression, involving the correlation \\(\\rho\\), is usually clearer for understanding. Comparing these to the same results for the standard Monte Carlo estimator (Theorem 3.2), we see that the antithetic variables method is an improvement (that is, has a smaller mean-square error) when \\(\\rho &lt; 0\\), but is worse when \\(\\rho &gt; 0\\). This proves that negative correlation improves our estimator.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html#error-with-antithetic-variables",
    "href": "lectures/L07-antithetic-2.html#error-with-antithetic-variables",
    "title": "7  Antithetic variables II",
    "section": "",
    "text": "Theorem 7.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and \\(\\theta = \\Exg\\phi(X)\\). Let \\(X'\\) have the same distribution as \\(X\\), and write \\(\\rho = \\operatorname{Corr}(\\phi(X_i),\\phi(X'_i))\\). Let \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X_i')\\big) \\] be the antithetic variables Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is \\[ \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big) = \\frac{1+\\rho}{n}\\Var\\big(\\phi(X)\\big). \\]\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big) = \\frac{1+\\rho}{n}\\Var\\big(\\phi(X)\\big). \\]\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is \\[ \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{\\sqrt{2n}} \\sqrt{\\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big)} = \\frac{\\sqrt{1+\\rho}}{\\sqrt{n}}\\sqrt{\\Var\\big(\\phi(X)\\big)}. \\]\n\n\n\n\n\nProof. For unbiasedness, we have \\[ \\Ex \\widehat{\\theta}_n^{\\mathrm{AV}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X_i')\\big)\\right) = \\frac{1}{n} \\,\\frac{n}{2} \\big(\\Exg\\phi(X) + \\Exg \\phi(X')) = \\frac{1}{2}(\\theta+ \\theta) = \\theta ,\\] since \\(X'\\) has the same distribution as \\(X\\).\nFor the other three points, each of the first expressions follows straightforwardly in essentially the same way. (You can fill in the details yourself, if you need to.) For the second expressions, we have \\[\\begin{align*}\n\\Var \\big(\\phi(X) + \\phi(X')\\big)\n&= \\Var\\big(\\phi(X)\\big) + \\Var\\big(\\phi(X')\\big) + 2\\operatorname{Cov}\\big(\\phi(X),\\phi(X')\\big) \\\\\n&= \\Var\\big(\\phi(X)\\big) + \\Var\\big(\\phi(X')\\big) + 2\\rho\\sqrt{\\Var\\big(\\phi(X)\\big) \\Var\\big(\\phi(X')\\big)} \\\\\n&= \\Var\\big(\\phi(X)\\big) + \\Var\\big(\\phi(X)\\big) + 2\\rho\\sqrt{\\Var\\big(\\phi(X)\\big) \\Var\\big(\\phi(X)\\big)} \\\\\n&= 2(1+\\rho)\\Var\\big(\\phi(X)\\big) .\n\\end{align*}\\] The results then follow.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html#examples",
    "href": "lectures/L07-antithetic-2.html#examples",
    "title": "7  Antithetic variables II",
    "section": "7.2 Examples",
    "text": "7.2 Examples\nLet’s return to the two examples we tried last time.\n\nExample 7.1 In Example 6.1, we were estimating \\(\\mathbb P(Z &gt; 2)\\) for \\(Z\\) a standard normal.\nThe basic Monte Carlo estimate and its root-mean-square error are\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest   &lt;- mean(samples &gt; 2)\nMC_RMSE &lt;- sqrt(var(samples &gt; 2) / n)\nc(MCest, MC_RMSE)\n\n[1] 0.0227870000 0.0001492239\n\n\nWe then used \\(Z' = -Z\\) as an antithetic variable. its root-mean-square error are\n\nn &lt;- 1e6\nsamples1 &lt;- rnorm(n / 2)\nsamples2 &lt;- -samples1\nAVest &lt;- (1 / n) * sum((samples1 &gt; 2) + (samples2 &gt; 2))\nAV_RMSE &lt;- sqrt(var((samples1 &gt; 2) + (samples2 &gt; 2)) / (2 * n))\nc(AVest, AV_RMSE)\n\n[1] 0.0228490000 0.0001476648\n\n\nThis looked like it made very little difference – perhaps a small improvement. This can be confirmed by looking at the sample correlation with R’s cor() function.\n\ncor(samples1 &gt; 2, samples2 &gt; 2)\n\n[1] -0.02338327\n\n\nWe see there was a very small but negative correlation: the variance, and hence the mean-square error, was reduced by about 2%.\n\n\nExample 7.2 In Example 6.2, we were estimating \\(\\mathbb E \\sin U\\), where \\(U\\) is continuous uniform on \\([0,1]\\).\nThe basic Monte Carlo estimate and its root-mean square error is\n\nn &lt;- 1e6\nsamples &lt;- runif(n)\nMCest &lt;- mean(sin(samples))\nMC_RMSE &lt;- sqrt(var(sin(samples)) / n)\nc(MCest, MC_RMSE)\n\n[1] 0.4601180588 0.0002478952\n\n\nWe then used \\(U' = 1 - U\\) as an antithetic variable\n\nn &lt;- 1e6\nsamples1 &lt;- runif(n / 2)\nsamples2 &lt;- 1 - samples1\nAVest &lt;- (1 / n) * sum(sin(samples1) + sin(samples2))\nAV_RMSE &lt;- sqrt(var(sin(samples1) + sin(samples2)) / (2 * n))\nc(AVest, AV_RMSE)\n\n[1] 0.4596760966 0.0000248325\n\n\nThis time, we see a big improvement: the root-mean-square error has gone down by a whole order of magnitude, from \\(2\\times 10^{-4}\\) to \\(2\\times 10^{-5}\\). It would normally take 100 times as many samples to reduce the RMSE by a factor of 10, but we’ve got the extra 99 million samples for free by using antithetic variables!\nThe benefit here can be confirmed by looking at the sample correlation.\n\ncor(sin(samples1), sin(samples2))\n\n[1] -0.9899601\n\n\nThat’s a very large negative correlation, which shows why the antithetic variables made such a huge improvement.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html#finding-antithetic-variables",
    "href": "lectures/L07-antithetic-2.html#finding-antithetic-variables",
    "title": "7  Antithetic variables II",
    "section": "7.3 Finding antithetic variables",
    "text": "7.3 Finding antithetic variables\nAntithetic variables can provide a huge advantage compared to standard Monte Carlo, as we saw in the second example above. The downside is that it can often be difficult to find an appropriate antithetic variable.\nTo even be able to try the antithetic variables method, we need to find a random variable \\(X'\\) with the same distribution as \\(X\\) that isn’t merely an independent copy. Both the examples we have seen of this use a symmetric distribution; that is, a distribution \\(X\\) such that \\(X' = a - X\\) has the same distribution as \\(X\\), for some \\(a\\).\n\nWe saw that if \\(X \\sim \\operatorname{N}(0, 1)\\) is a standard normal distribution, then \\(X' = -X \\sim \\operatorname{N}(0, 1)\\) too. More generally, if \\(X\\sim \\operatorname{N}(\\mu, \\sigma^2)\\), then \\(X' = 2\\mu - X \\sim \\operatorname{N}(\\mu, \\sigma^2)\\) can be tried as an antithetic variable.\nWe saw that if \\(U \\sim \\operatorname{U}[0, 1]\\) is a continuous uniform distribution on \\([0,1]\\), then \\(U' = 1-U \\sim \\operatorname{U}[0, 1]\\) too. More generally, if \\(X\\sim \\operatorname{U}[a, b]\\), then \\(X' = (a + b) - X \\sim \\operatorname{U}[a, b]\\) can be tried as an antithetic variable.\n\nLater, when we study the inverse transform method (in Lecture 13) we will see another, more general, way to generate antithetic variables.\nBut to be a good antithetic variable, we need \\(\\phi(X)\\) and \\(\\phi(X')\\) to be negatively correlated too – preferably strongly so. Often, this is a matter of trial-and-error – it’s difficult to set out hard principles. But there are some results that try to formalise the idea that “nice functions of negatively correlated random variables are themselves negatively correlated”, which can be useful. We give one example of such a result here.\n\nTheorem 7.2 Let \\(U \\sim \\operatorname{U}[0, 1]\\) and \\(U' = 1 - U\\). Let \\(\\phi\\) be a monotonically increasing function. Then \\(\\phi(U)\\) and \\(\\phi'(U)\\) are negatively correlated, in that \\(\\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) \\leq 0\\).\n\nIf I were to just write out the proof of this theorem, the proof would be fairly short and easy to understand. But it would be difficult to understand why I had taken the steps I had. So I will try to explain why each of the steps in the proof is a natural one – although doing so may make the proof seem longer and more complicated than it really is.\n\nProof. The covariance in question is \\[ \\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) = \\Exg \\phi(U)\\phi(1-U) - \\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-U)\\big) . \\]\nWe would like to take a single expectation sign all the way outside. But we can’t do this yet, because \\(\\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-U)\\big)\\) and \\(\\Exg\\phi(U)\\phi(1-U)\\) are different things. However, we can do this if we introduce a new random variable \\(V\\) that is also \\(\\operatorname{U}[0,1]\\) but is independent of \\(U\\). Then we so have \\[\\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-U)\\big) = \\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-V)\\big) = \\Exg \\phi(U)\\phi(1-V).\\] So now we can write \\[ \\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) = \\Exg \\big(\\phi(U)\\phi(1-U) - \\phi(U)\\phi(1-V)\\big) . \\]\nWe’ve got a slightly odd asymmetry between \\(U\\) and \\(V\\) here, though. Why is the first term \\(\\phi(U)\\phi(1-U)\\) and not \\(\\phi(V)\\phi(1-V)\\)? Why is the second term \\(\\phi(U)\\phi(1-V)\\) and not \\(\\phi(V)\\phi(1-U)\\)? Well, maybe to even things up, we can write them as half of one and half of the other. That is, \\[\\begin{align*}\n\\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) &= \\tfrac12 \\Exg \\big(\\phi(U)\\phi(1-U) +\\phi(V)\\phi(1-V) \\\\\n&\\qquad\\qquad\\quad {}- \\phi(U)\\phi(1-V) - \\phi(V)\\phi(1-U)\\big) . \\end{align*}\\]\nThis expression will factorise nicely. It’s \\[ \\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) = \\tfrac12 \\Exg \\big(\\phi(U)-\\phi(V)\\big)\\big(\\phi(1-U) - \\phi(1-V)\\big) . \\]\nWe now claim that this expectation is negative. In fact, we have a stronger result: \\[\\big(\\phi(U) - \\phi(V)\\big)\\big(\\phi(1-U) - \\phi(1-V)\\big) \\tag{7.1}\\] is always negative, so its expectation certainly is. To see this, think separately of the two cases \\(U \\leq V\\) and \\(V \\leq U\\).\n\nIf \\(U \\leq V\\), then \\(\\phi(U) \\leq \\phi(V)\\) too, since \\(\\phi\\) is increasing. But, also this means that \\(1-U \\geq 1-V\\), so \\(\\phi(1 - U) \\geq \\phi(1-V)\\). This means that, in Equation 7.1, the first term is negative and the second term is positive, so the product is negative.\nIf \\(V \\leq U\\), then \\(\\phi(V) \\leq \\phi(U)\\) too, since \\(\\phi\\) is increasing. But, also this means that \\(1-V \\geq 1-U\\), so \\(\\phi(1 - V) \\geq \\phi(1-U)\\). This means that, in Equation 7.1, the first term is positive and the second term is negative, so the product is negative.\n\nThis completes the proof.\n\nNext time: We come to the third, and most important, variance reduction scheme: importance sampling.\n\nSummary:\n\nThe antithetic variables estimator is unbiased and has mean-square error \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big) = \\frac{1+\\rho}{n}\\Var\\big(\\phi(X)\\big). \\]\nIf \\(U \\sim \\operatorname{U}[0, 1]\\) and \\(\\phi\\) is monotonically increasing, then \\(\\phi(U)\\) and \\(\\phi(1-U)\\) are negatively correlated.\n\nOn Thursday’s lecture, we will be discussing your answers to Problem Sheet 1.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html",
    "href": "lectures/L08-is-1.html",
    "title": "8  Importance sampling I",
    "section": "",
    "text": "8.1 Sampling from other distributions\nSo far, we have looked at estimating \\(\\Exg \\phi(X)\\) using samples \\(X_1, X_2, \\dots, X_n\\) that are from the same distribution as \\(X\\). Importance sampling is based on the idea of taking samples \\(Y_1, Y_2, \\dots, Y_n\\) from some different distribution \\(Y\\), but then making an appropriate adjustment, so that we’re still estimating \\(\\Exg \\phi(X)\\).\nWhy might we want to do this? There are two main reasons:\nConsider, for example, trying to estimate \\(\\Exg\\phi(X)\\) where \\(X\\) is uniform on \\([0, 20]\\) and \\(\\phi\\) is the function shown below.\nCode for drawing this graph\nphi &lt;- function(x) sin(5 * x) / (5 * x)\ncurve(\n  phi, n = 10001, from = 0, to = 20,\n  lwd = 3, col = \"blue\",\n  xlab = \"x\", ylab = expression(phi(x)), ylim = c(-0.2, 1)\n)\nabline(h = 0)\nWe can see that what happens for small \\(x\\) – say, for \\(x\\) between 0 and 2, or so – will have an important effect on the value of \\(\\Exg \\phi(X)\\), because that where \\(\\phi\\) has the biggest (absolute) values. But what happens for large \\(x\\) – say for \\(x \\geq 10\\) or so – will be much less important for estimating \\(\\Exg\\phi(X)\\). So it seems wasteful to have all values in \\([0, 20]\\) to be sampled equally, and it would seem to make sense to take more samples from small values of \\(x\\).\nThis is all very well in practice, but how exactly should we down-weight those over-sampled areas?\nThink about estimating \\(\\Exg \\phi(X)\\). Let’s assume that \\(X\\) is continuous with probability density function \\(f\\). (Throughout this lecture and the next, we will assume all our random variables are continuous. The arguments for discrete random variables are very similar – just swap probability density functions with probability mass functions and integrals with sums. You can fill in the details yourself, if you like.) Then we are trying to estimate \\[ \\Exg \\phi(X) = \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\int_{-\\infty}^{+\\infty} \\phi(y)\\,f(y)\\,\\mathrm{d}y . \\] (In the second equality, we merely changed the “dummy variable” from \\(x\\) to \\(y\\), as we are at liberty to do.)\nNow suppose we sample from some other continuous distribution \\(Y\\), with PDF \\(g\\). If we estimate \\(\\Exg \\psi(Y)\\), say, for some function \\(\\psi\\), then we are estimating \\[\\Exg \\psi(Y) = \\int_{-\\infty}^{+\\infty} \\psi(y)\\,g(y) \\, \\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\psi(x)\\,g(x) \\, \\mathrm{d}x . \\]\nBut we want to be estimating \\(\\Exg\\phi(X)\\), not \\(\\Exg\\psi(Y)\\). So we will need to pick \\(\\psi\\) such that \\[ \\Exg \\phi(X) = \\int_{-\\infty}^{+\\infty} \\phi(y)\\,f(y)\\,\\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\psi(y)\\,g(y) \\, \\mathrm{d}y = \\Exg \\psi(Y) . \\] So we need to pick \\(\\psi\\) such that \\(\\phi(y)\\,f(y) = \\psi(y)\\,g(y)\\). That means that we should take \\[\\psi(y) = \\frac{\\phi(y) f(y)}{g(y)} = \\frac{f(y)}{g(y)}\\,\\phi(y). \\]\nSo we could build a Monte Carlo estimate for \\(\\Exg \\phi(X)\\) instead as a Monte Carlo estimate for \\[ \\Exg \\psi(Y) = \\Exg \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\]\nThere is one other thing: we need to be careful of division by \\(0\\) errors. So we should make sure that \\(g\\) is only 0 when \\(f\\) is 0. In other words, if it’s possible for \\(X\\) to take some value, then it must be possible for \\(Y\\) to take that value too.\nWe are finally ready to define our estimator.\nWe can think of this as taking a weighted mean of the \\(\\phi(Y_i)\\)s, where the weights are \\(f(Y_i)/g(Y_i)\\). So if a value \\(y\\) is more likely under \\(Y\\) than under \\(X\\), then \\(g(y)\\) is big compared to \\(f(y)\\), so \\(f(y)/g(y)\\) is small, and \\(y\\) gets a low weight. If a value \\(y\\) is less likely under \\(Y\\) than under \\(X\\), then \\(g(y)\\) is small compared to \\(f(y)\\), so \\(f(y) / g(y)\\) is big, and it gets a high weight. Thus we see that the weighting compensates for values that are likely to be over- or under-sampled.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html#sampling-from-other-distributions",
    "href": "lectures/L08-is-1.html#sampling-from-other-distributions",
    "title": "8  Importance sampling I",
    "section": "",
    "text": "First, we might not be able to sample from \\(X\\), so we might be forced into sampling from some other distribution \\(Y\\) instead. So far, \\(X\\) has always been a nice pleasant distribution, like a normal, exponential or continuous uniform distribution, for which we can use R’s built-in sampling function. But what if \\(X\\) were instead a very unusual or awkward distribution? In that case, we might not be able to sample directly from \\(X\\), so would be forced into sampling from a different distribution.\nSecond, we might prefer to sample from a distribution other than \\(Y\\). This might be the case if \\(\\phi(x)\\) varies a lot over different values of \\(x\\). There might be some areas of \\(x\\) where it’s very important to get an accurate estimation, because they contribute a lot to \\(\\Exg\\phi(X)\\), so we’d like to “oversample” (take lots of samples) there; meanwhile, other areas of \\(x\\) where it is not very important to get an accurate estimation, because they contribute very little to \\(\\Exg\\phi(X)\\), so we don’t mind “undersampling” (taking relatively few samples) there. Then we could sample instead from a distribution \\(Y\\) that concentrates on the most important areas for \\(\\phi\\); although we’ll need to make sure to adjust our estimator by “down-weighting” the places that we have oversampled.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 8.1 Let \\(X\\) be a continuous random variable with probability density function \\(f\\), let \\(\\phi\\) be a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(Y\\) be a continuous random variable with probability desnity function \\(g\\), where \\(g(y) &gt; 0\\) for all \\(y\\) where \\(f(y) &gt; 0\\). Then the importance sampling Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(Y_i)}{g(Y_i)}\\, \\phi(Y_i)   ,\\] where \\(Y_1, Y_2, \\dots, Y_n\\) are independent random samples from \\(Y\\).",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html#example",
    "href": "lectures/L08-is-1.html#example",
    "title": "8  Importance sampling I",
    "section": "8.2 Example",
    "text": "8.2 Example\n\nExample 8.1 Let \\(X \\sim \\operatorname{N}(0,1)\\) be a standard normal. Suppose we want to estimate \\(\\mathbb P(X &gt; 4)\\). We could do this the standard Monte Carlo way by sampling from \\(X\\) itself. \\[ \\widehat{\\theta}_n^{\\mathrm{MS}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb I_{[4,\\infty)}(X_i) . \\]\nHowever, this will not be a good estimator. To see the problem, lets run this with \\(n = 100\\,000 = 10^5\\) samples, but do it 10 times, and see what all the estimates are.\n\nn &lt;- 1e5\nMCest &lt;- rep(0, 10)\nfor (i in 1:10) MCest[i] &lt;- mean(rnorm(n) &gt; 4)\nMCest\n\n [1] 1e-05 0e+00 1e-05 1e-05 0e+00 2e-05 4e-05 3e-05 1e-05 0e+00\n\n\nWe see a big range of values. I get different results each time I run it, but anything between \\(1 \\times 10^{-5}\\) and \\(8 \\times 10^{-5}\\), and even \\(0\\), comes out fairly regularly as the estimate. The problem is that \\(X &gt; 4\\) is a very rare event – it only comes out a handful of times (perhaps 0 to 8) out of the 100,000 samples. This means our estimate is (on average) quite inaccurate.\nIt would be better not to sample from \\(X\\), but rather to sample from a distribution that is greater than 4 a better proportion of the time. We could try anything for this distribution \\(Y\\), but to keep things simple, I’m going to stick with a normal distribution with standard deviation 1. I’ll want to increase the mean, though, so that we sample values bigger than 4 more often. Let’s try importance sampling with \\(Y \\sim \\operatorname{N}(4,1)\\).\nThe PDFs of \\(X \\sim \\operatorname{N}(0,1)\\) and \\(Y \\sim \\operatorname{N}(4,1)\\) are \\[f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 x^2\\big) \\qquad g(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-4)^2\\big) , \\] so the relevant weighting of a sample \\(y\\) is \\[ \\frac{f(y)}{g(y)} = \\frac{\\exp\\big(-\\tfrac12 y^2\\big)}{\\exp\\big(-\\tfrac12 (y-4)^2\\big)} = \\exp \\big(\\tfrac12\\big(-y^2 + (y-4)^2\\big)\\big) = \\exp(-4y+8) .  \\] So our importance sampling estimate will be \\[ \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\mathrm{e}^{-4Y_i +8} \\, \\mathbb I_{[4,\\infty)}(Y_i) . \\]\nLet’s try this in R. Although we could use the function \\(\\mathrm{e}^{-4y+8}\\) for the weights, I’ll do this by using the ratios of the PDFs directly in R (just in case I made a mistake…).\n\nn &lt;- 1e5\npdf_x &lt;- function(y) dnorm(y, 0, 1)\npdf_y &lt;- function(y) dnorm(y, 4, 1)\nsamples_y &lt;- rnorm(n, 4, 1)\nISest &lt;- mean((pdf_x(samples_y) / pdf_y(samples_y)) * (samples_y &gt; 4))\nISest\n\n[1] 3.143342e-05",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html#errors-in-importance-sampling",
    "href": "lectures/L08-is-1.html#errors-in-importance-sampling",
    "title": "8  Importance sampling I",
    "section": "8.3 Errors in importance sampling",
    "text": "8.3 Errors in importance sampling\nThe following theorem should not by now be a surprise.\n\nTheorem 8.1 Let \\(X\\) be a continuous random variable with probability density function \\(f\\), let \\(\\phi\\) be a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(Y\\) another continuous random variable with probability density function with probability density function \\(g\\), such that \\(g(y) = 0\\) only when \\(f(y) = 0\\). Let \\[ \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(Y_i)  \\] be the importance sampling Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is \\[ \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right). \\]\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\]\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is \\[ \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{\\sqrt{n}} \\,\\sqrt{\\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right)}. \\]\n\n\n\nProof. Part 1 follows essentially the same argument as our discussion at the beginning of this lecture. We have \\[ \\Ex \\left( \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(Y_i) \\right) = \\frac{1}{n}\\, n\\, \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) . \\] But \\[ \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\int_{-\\infty}^{+\\infty} \\frac{f(y)}{g(y)}\\,\\phi(y)\\,g(y)\\,\\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\phi(y) \\, f(y) \\, \\mathrm{d}y = \\Exg \\phi(X) . \\] This last step is because \\(f\\) is the PDF of \\(X\\); it doesn’t matter whether the dummy variable in the integration is \\(x\\) or \\(y\\). Hence the estimator is unbiased.\nParts 2 to 4 follow in the usual way.\n\nAs we are now used to, we can estimate the variance using the sample variance.\n\nExample 8.2 We continue Example 8.1, where we are estimating \\(\\mathbb P(X &gt; 4)\\) for \\(X \\sim \\operatorname{N}(0,1)\\).\nFor the standard Monte Carlo method, we estimate the root-mean-square error as\n\nn &lt;- 1e5\nMC_MSE &lt;- var(rnorm(n) &gt; 4) / n\nsqrt(MC_MSE)\n\n[1] 2.236023e-05\n\n\nAs before, this still varies a lot, but it seems to usually be about \\(2 \\times 10^{-5}\\).\nFor the importance sampling method, we estimate the mean-square error as\n\nn &lt;- 1e5\npdf_x &lt;- function(x) dnorm(x, 0, 1)\npdf_y &lt;- function(y) dnorm(y, 4, 1)\nsamples_y &lt;- rnorm(n, 4, 1)\nIS_MSE &lt;- var((pdf_x(samples_y) / pdf_y(samples_y)) * (samples_y &gt; 4)) / n\nsqrt(IS_MSE)\n\n[1] 2.131441e-07\n\n\nThis is about \\(2 \\times 10^{-7}\\). This is about 100 times smaller than for the standard method: equivalent to taking about 10,000 times as many samples! That’s a huge improvement, which demonstrates the power of importance sampling.\n\nNext time: We continue our study of importance sampling – and complete our study of Monte Carlo estimation, for now – by considering how to pick a good distribution \\(Y\\).\n\nSummary:\n\nImportance sampling estimates \\(\\Exg \\phi(X)\\) by sampling from a different distribution \\(Y\\).\nThe importance sampling estimator is \\({\\displaystyle \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(Y_i)}\\).\nThe importance sampling estimator is unbiased with mean-square error \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\]\n\nSolutions are now available for Problem Sheet 1.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html",
    "href": "lectures/L09-is-2.html",
    "title": "9  Importance sampling II",
    "section": "",
    "text": "9.1 Picking a good distribution\nWe’ve now seen that importance sampling can be a very powerful tool, when used well. But how should pick a good distribution \\(Y\\) to sample from?\nLet’s examine the mean-square error more carefully: \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\] So our goal is to try and pick \\(Y\\) such that \\(\\frac{f(Y)}{g(Y)}\\phi(Y)\\) has low variance. (We also, of course, want to be able to sample from \\(Y\\).)\nThe best possible choice, then, would be to pick \\(Y\\) such that \\(\\frac{f(Y)}{g(Y)}\\phi(Y)\\) is constant – and therefore has zero variance! If \\(\\phi\\) is non-negative, then it seems like we should pick \\(Y\\) such that its probability density function is \\(g(y) \\propto f(y)\\phi(y)\\). (Here, \\(\\propto\\) is the “proportional to” symbol.) That is, to have \\[ g(y) = \\frac{1}{Z} f(y)\\phi(y) , \\] for some constant \\(Z\\). Then \\(\\frac{f(Y)}{g(Y)}\\phi(Y) = Z\\) is a constant, has zero variance, and we have a perfect estimator!\nWhat is this constant \\(Z\\)? Well, \\(g\\) is a PDF, so it has to integrate. So we will need to have \\[ 1 = \\int_{-\\infty}^{+\\infty} g(y)\\, \\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\frac{1}{Z} f(y)\\phi(y) \\, \\mathrm{d}y = \\frac{1}{Z} \\int_{-\\infty}^{+\\infty} f(x)\\phi(x) \\, \\mathrm{d}x = \\frac{1}{Z} \\Exg\\phi(X) . \\] (We did the “switching the dummy variable from \\(y\\) to \\(x\\)” thing again.) So \\(Z = \\Exg \\phi(X)\\). But that’s no good: \\(\\theta = \\Exg \\phi(X)\\) was the thing we were trying to estimate in the first place. If we knew that, we wouldn’t have to do Monte Carlo estimation to start with!\nSo, as much as we would like to, we can’t use this “perfect” ideal distribution \\(Y\\). More generally, if \\(\\phi\\) is not always non-negative, it can be shown that \\(g(y) \\propto f(y)\\,|\\phi(y)| = |f(x)\\,\\phi(x)|\\) would be the best possible distribution, but this has the same problems.\nHowever, we can still be guided by this idea – we would like \\(g(y)\\) to be as close to proportional to \\(f(y) \\phi(y)\\) (or \\(|f(y) \\phi(y)|\\)) as we can manage, so that \\(\\frac{f(y)}{g(y)}\\phi(y)\\) is close to being constant, so hopefully has low variance. This tells us that \\(Y\\) should be likely – that is, \\(g(y)\\) should be big – where both \\(f\\) and \\(|\\phi|\\) are both big – that is, where \\(X\\) is likely and also \\(\\phi\\) is big in absolute value. While \\(Y\\) should be unlikely where both \\(X\\) is unlikely and \\(\\phi\\) is small in absolute value.\nAside from the exact theory, in the absence of any better idea, choosing \\(Y\\) to be “in the same distribution family as \\(X\\) but with different parameters” is often a reasonable thing to try. For example:",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html#picking-a-good-distribution",
    "href": "lectures/L09-is-2.html#picking-a-good-distribution",
    "title": "9  Importance sampling II",
    "section": "",
    "text": "Example 9.1 Let’s look again at Example 8.1 (continued in Example 8.2), where we wanted to estimate \\(\\mathbb P(X &gt; 4) = \\Exg\\Ind_{(4,\\infty)}(X)\\) for \\(X \\sim \\operatorname{N}(0,1)\\). We found are estimator was enormously improved when we used instead \\(Y \\sim \\operatorname{N}(4,1)\\).\nIn the figure below, the blue line is \\[f(y)\\,\\phi(y) = f(y)\\,\\Ind_{(4,\\infty)}(y) = \\begin{cases} \\displaystyle\\frac{1}{\\sqrt{2\\pi}} \\,\\mathrm{e}^{-y^2/2} & y &gt; 4 \\\\ 0 & y \\leq 4 \\end{cases} \\] (scaled up, otherwise it would be so close to the axis line you wouldn’t see it).\nThe black line is the PDF \\(f(y)\\) of the original distribution \\(X \\sim \\operatorname{N}(0,1)\\), while the red line is the PDF \\(g(y)\\) of our importance distribution \\(Y \\sim \\operatorname{N}(4,1)\\).\n\n\nCode for drawing this graph\ncurve(\n  dnorm(x, 0, 1), n = 1001, from = -2.5, to = 7.5,\n  col = \"black\", lwd = 2,\n  xlim = c(-2, 7), xlab = \"y\", ylim = c(0, 0.7), ylab = \"\"\n)\ncurve(\n  dnorm(x, 4, 1), n = 1001, from = -2.5, to = 7.5,\n  add = TRUE, col = \"red\", lwd = 3,\n)\ncurve(\n  dnorm(x, 0, 1) * (x &gt; 4) * 5000, n = 1001, from = -2.5, to = 7.5,\n  add = TRUE, col = \"blue\", lwd = 3\n)\nlegend(\n  \"topright\",\n  c(expression(paste(\"f(y)\", varphi, \"(y) [scaled]\")), \"N(0, 1)\", \"N(4, 1)\"),\n  lwd = c(3, 2, 3), col = c(\"blue\", \"black\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nWe have noted that a good distribution will have a PDF that is big when \\(f(x)\\phi(x)\\) (the blue line) is big. Clearly the red line is much better at this then the black line, which is why the importance sampling method was so much better here.\nThere’s scope to do better here, though. Perhaps an asymmetric distribution with a much more quickly-decaying left-tail might be good – for example, a shifted exponential \\(4 + \\operatorname{Exp}(\\lambda)\\) might be worth investigating. Or a thinner, spikier distribution, such as a normal with smaller standard deviation. In both cases, though, we have to be careful – because it’s the ratio \\(f(y)/g(y)\\), we still have to be a bit careful about what happens when both \\(f(y)\\) and \\(g(y)\\) are small absolutely, in case one is proportionally much bigger than the other.\n\n\n\nIf \\(X \\sim \\operatorname{N}(\\mu, \\sigma^2)\\), then try \\(Y \\sim \\operatorname{N}(\\nu, \\sigma^2)\\) for some other value \\(\\nu\\).\nIf \\(X \\sim \\operatorname{Exp}(\\lambda)\\), then try \\(Y \\sim \\operatorname{Exp}(\\mu)\\) for some other value \\(\\mu\\).",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html#monte-carlo-summary",
    "href": "lectures/L09-is-2.html#monte-carlo-summary",
    "title": "9  Importance sampling II",
    "section": "9.3 Monte Carlo summary",
    "text": "9.3 Monte Carlo summary\nThis is our last lecture on Monte Carlo estimation – at least for now, and at least in its standard form. So let’s end this section of the module by summarising the estimators we have learned about. We have been learning how to estimate \\(\\theta = \\Exg \\phi(X)\\)\n\nThe standard Monte Carlo estimator simple takes a sample mean of \\(\\phi(X_i)\\), where \\(X_i\\) are independent random samples from \\(X\\).\nThe control variate Monte Carlo estimator “anchors” the estimator to some known value \\(\\eta = \\Exg \\psi(X)\\), for a function \\(\\psi\\) that is “similar to \\(\\phi\\), but easier to calculate the expectation exactly”.\nThe antithetic variables Monte Carlo estimator uses pairs of samples \\((X_i, X'_i)\\) that both have the same distribution as \\(X\\), but where \\(\\phi(X)\\) and \\(\\phi(X')\\) have negative correlation \\(\\rho &lt; 0\\).\nThe importance sampling Monte Carlo estimator samples not from \\(X\\), with PDF \\(f\\), but from a different distribution \\(Y\\), with PDF \\(Y\\). The distribution \\(Y\\) is chosen to oversample from the most important values, but then gives lower weight to those samples.\n\n\n\n\n\n\n\n\n\n\nEstimator\nMSE\n\n\n\n\nStandard Monte Carlo\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i)}\\)\n\\({\\displaystyle\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\)\n\n\nControl variate\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big)} + \\eta\\)\n\\({\\displaystyle\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X_i) - \\psi(X_i)\\big)}\\)\n\n\nAntithetic variables\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X_i')\\big)}\\)\n\\({\\displaystyle\\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X_i) + \\phi(X_i')\\big)}\\) \\({\\displaystyle {}= \\frac{1+\\rho}{n} \\,\\operatorname{Var}\\big(\\phi(X)\\big)}\\)\n\n\nImportance sampling\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(X_i)}\\)\n\\({\\displaystyle\\frac{1}{n} \\operatorname{Var}\\left(\\frac{f(Y_i)}{g(Y_i)}\\,\\phi(X_i)\\right)}\\)\n\n\n\n \nNext time: We begin the second section of the module, on random number generation.\n\nSummary:\n\nA good importance sampling distribution \\(Y\\) is one whose PDF \\(g(y)\\) is roughly proportional to \\(|f(y)\\,\\phi(y)|\\). Equivalently, \\(\\frac{f(y)}{g(y)}|\\phi(y)|\\) is approximately constant.\n\nProblem Sheet 2 should be available soon.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  },
  {
    "objectID": "problems/solutions.html",
    "href": "problems/solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Problem Sheet 1\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\\]\n\n\n1.      Let \\(X\\) be uniform on \\([-1,2]\\).\n\n(a)   By hand, calculate the exact value of \\(\\Ex X^4\\).\n\nSolution.\n\\[\\int_{-1}^2 x^4\\,\\frac{1}{2-(-1)}\\,\\mathrm{d}x = \\tfrac13 \\Big[\\tfrac15x^5\\Big]_{-1}^2 = \\tfrac13\\Big(\\tfrac{32}{5}-\\big(-\\tfrac15\\big)\\Big) = \\tfrac{33}{15} = \\tfrac{11}{5} = 2.2\\]\n\n\n\n(b)   Using R, calculate a Monte Carlo estimate for \\(\\Ex X^4\\).\n\nSolution. I used the R code\n\nn &lt;- 1e6\nsamples &lt;- runif(n, -1, 2)\nmean(samples^4)\n\n[1] 2.202239\n\n\n\n\n\n\n2.      Let \\(X\\) and \\(Y\\) both be standard normal distributions. Compute a Monte Carlo estimate of \\(\\Exg \\max\\{X,Y\\}\\). (You may wish to investigate R’s pmax() function.)\n\nSolution. By looking at ?pmax (or maybe searching on Google) I discovered that pmax() gives the “parallel maxima” of two (or more vectors). That is the first element is the maximum of all the first elements of the vectors; the second element is the maximum of all the second elements of the vectors; and so on.\nSo I used the R code\n\nn &lt;- 1e6\nxsamples &lt;- rnorm(n)\nysamples &lt;- rnorm(n)\nmean(pmax(xsamples, ysamples))\n\n[1] 0.5648906\n\n\n\n\n\n3.      You are trapped alone on an island. All you have with you is a tin can (radius \\(r\\)) and a cardboard box (side lengths \\(2r \\times 2r\\)) that it fits snugly inside. You put the can inside the box [left picture].\nWhen it starts raining, each raindrop that falls in the cardboard box might fall into the tin can [middle picture], or might fall into the corners of the box outside the can [right picture].\n\n\n(a)   Using R, simulate rainfall into the box. You may take units such that \\(r = 1\\). Estimate the probability \\(\\theta\\) that a uniformly-random raindrop that falls in the cardboard box falls into the tin can also.\n\nSolution. I set things up so that the box is \\([-1, 1]^2\\), centered at the origin. This means that the inside of the can is the set of points is those \\((x,y)\\) such that \\(x^2 + y^2 \\leq 1\\).\n\nn &lt;- 1e6\nrain_x &lt;- runif(n, -1, 1)\nrain_y &lt;- runif(n, -1, 1)\nin_box &lt;- function(x, y) x^2 + y^2 &lt;= 1\nmean(in_box(rain_x, rain_y))\n\n[1] 0.784906\n\n\n\n\n\n(b)   Calculate exactly the probability \\(\\theta\\).\n\nSolution. The area of the box is \\(2r \\times 2r = 4r^2\\). The area of the can is \\(\\pi r^2\\). So the probability a raindrop landing in the box lands in the can is \\[ \\frac{\\text{area of can}}{\\text{area of box}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\approx 0.785. \\]\n\n\n\n(c)   You realise that, despite being trapped all alone on the island, you now have a way of approximating the value of \\(\\pi\\). If you want to calculate \\(\\pi\\) to 6 decimal places, roughly how many raindrops do you need to fall into the box?\n\nSolution. The phrase “to 6 decimal places” isn’t a precise mathematical one. I’m going to interpret this as getting the root-mean-square error below \\(10^{-6}\\). If you interpret it slightly differently that’s fine – for example, getting the width of a 95% confidence interval below \\(10^{-6}\\) could be another, slightly stricter, criterion.\nOne could work this out by hand. Since the variance of a Bernoulli random variable is \\(p(1-p)\\), the mean-square error of our estimator is \\[ \\frac{\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})}{n} \\approx \\frac{0.169}{n} . \\] So we need \\[n = \\frac{0.169}{(10^{-6})^2} \\approx 169 \\text{ billion} . \\]\nThat said, if we are trapped on our desert island, maybe we don’t know what \\(\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})\\) is. In that case we could do this using the can and the box. Our estimate of the variance is\n\nvar_est &lt;- var(in_box(rain_x, rain_y))\nvar_est / (1e-6)^2\n\n[1] 168828739992\n\n\nWe will probably spend a long time waiting for that much rain!\n\n\n\n\n4.      Let \\(h(x) = 1/(x + 0.1)\\). We wish to estimate \\(\\int_0^5 h(x) \\, \\mathrm{d}x\\) using a Monte Carlo method.\n\n(a)   Estimate the integral using \\(X\\) uniform on \\([0,5]\\).\n\nSolution.\n\nn &lt;- 1e6\nintegrand &lt;- function(x) 1 / (x + 0.1)\nsamples1 &lt;- runif(n, 0, 5)\nmean(5 * integrand(samples1))\n\n[1] 3.93898\n\n\n(In fact, the true answer is \\(\\log(5.1) - \\log(0.1) = 3.932\\), so it looks like this is working correctly.)\n\n\n\n(b)   Can you come up with a choice of \\(X\\) that improves on the estimate from (a)?\n\nSolution. Let’s look at a graph of the integrand \\(h\\).\n\n\nCode for drawing this graph\ncurve(\n  integrand, n = 1001, from = 0, to = 5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(0,5)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nWe see that we get a much bigger contribution to the integral from values near 0. So a random variable that picks values nearer to 0 more often might have a chance of giving a more accurate result.\n\nI decided to try an exponential distribution with rate 1, which should sample the smaller values of \\(x\\) more often.\n\npdf &lt;- function(x) dexp(x, 1)\nphi &lt;- function(x) (integrand(x) / pdf(x)) * (x &lt;= 5)\n\nsamples2 &lt;- rexp(n, 1)\nmean(phi(samples2))\n\n[1] 3.93244\n\n\n(I had to include x &lt;= 5 in the expression for \\(\\phi\\), because my exponential distribution will sometimes take samples above 5, but they should count as 0 in an estimate for the integral between 0 and 5.)\nTo see whether or not this was an improvement, I estimated the mean-square error.\n\nvar(5 * integrand(samples1)) / n\n\n[1] 3.387878e-05\n\nvar(phi(samples2)) / n\n\n[1] 6.072919e-06\n\n\nI found that I had reduced the mean-square error by roughly a factor of 5. \n\n\n\n\n5.      When calculating a Monte Carlo estimate \\(\\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\\), one might wish to first generate the \\(n\\) samples \\((x_1, x_2, \\dots, x_n)\\) and store them, and only then, after all samples are generated, finally calculate the estimate. However, when \\(n\\) is extremely large, storing all \\(n\\) samples uses up a lot of space in a computer’s memory. Describe (in words, in R code, or in a mixture of the two) how the Monte Carlo estimate could be produced using much less memory.\n\nSolution. The idea is to keep a “running total” of the \\(\\phi(x_i)\\)s. Then we only have to store that running total, not all the samples. Once this has been done \\(n\\) times, then divide by \\(n\\) to get the estimate.\nIn R code, this might be something like\n\nn &lt;- 1e6\n\ntotal &lt;- 0\nfor (i in 1:n) {\n  sample &lt;- # sampling code for 1 sample\n  total &lt;- total + phi(sample)\n}\n\nMCest &lt;- total / n\n\n\n\n\n6.      Show that the indicator functions \\(\\mathbb I_A(X)\\) and \\(\\mathbb I_B(X)\\) have correlation 0 if and only if the events \\(\\{X \\in A\\}\\) and \\(\\{X \\in B\\}\\) are independent.\n\nSolution. Recall that two random variables \\(U\\), \\(V\\) have correlation 0 if and only if their covariance \\(\\Cov(U,V) = \\Ex UV - (\\Ex U)(\\Ex V)\\) is 0 too.\nWe know that \\(\\Exg\\Ind_A(X) = \\mathbb P(X \\in A)\\) and \\(\\Exg \\Ind_B(Y) = \\mathbb P(X \\in B)\\). What about \\(\\Exg \\Ind_A(X) \\Ind_B(X)\\)? Well, \\(\\Ind_A(x) \\Ind_B(x)\\) is 1 if and only if both indicator functions equal 1, which is if and only if both \\(x \\in A\\) and \\(x \\in B\\). So \\(\\Exg \\Ind_A(X) \\Ind_B(X) = \\mathbb P(X \\in A \\text{ and } X \\in B)\\).\nSo the covariance is \\[ \\Cov \\big(\\Ind_A(X), \\Ind_B(X) \\big) = \\mathbb P(X \\in A \\text{ and } X \\in B) - \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B) . \\] If this is 0, then \\(\\mathbb P(X \\in A \\text{ and } X \\in B) = \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B)\\), which is precisely the definition of those two events being independent.\n\n\n\n7.      Let \\(X\\) be an exponential distribution with rate 1.\n\n(a)   Estimate \\(\\mathbb EX^{2.1}\\) using the standard Monte Carlo method.\n\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 1)\nmean(samples^2.1)\n\n[1] 2.19999\n\n\n\n\n\n(b)   Estimate \\(\\mathbb EX^{2.1}\\) using \\(X^2\\) as a control variate. (You may recall that if \\(Y\\) is exponential with rate \\(\\lambda\\) then \\(\\mathbb EY^2 = 2/\\lambda^2\\).)\n\nSolution. We have \\(\\Ex X^2 = 2\\). So, re-using the same sample as before (you don’t have to do this – you could take new samples), our R code is as follows.\n\nmean(samples^2.1 - samples^2) + 2\n\n[1] 2.197787\n\n\n\n\n\n(c)   Which method is better?\n\nSolution. The better answer is the one with the smaller mean-square error.\nFor the basic method,\n\nvar(samples^2.1) / n\n\n[1] 2.775811e-05\n\n\nFor the control variate method,\n\nvar(samples^2.1 - samples^2) / n\n\n[1] 6.743602e-07\n\n\nSo the control variates method is much, much better.\n\n\n\n\n8.      Let \\(Z\\) be a standard normal distribution. A statistician has been investigating Monte Carlo estimates of \\(\\mathbb EZ^k\\) for different positive integers values of \\(k\\). Her colleague suggests using \\(Z' = -Z\\) as an antithetic variable. Without running any R code, explain whether or not this is a good idea (a) when \\(k\\) is even; (b) when \\(k\\) is odd.\n\nSolution.\n(a) When \\(k\\) is even, we have \\(Z^k = (-Z)^k\\). So the antithetic variables method just repeats each sample twice. This is obviously no benefit at all, and just wastes time. Indeed, we have perfect positive correlation \\(\\rho = +1\\), which is the “worst-case scenario”.\n(b) When \\(k\\) is odd, we have \\(Z^k = -(-Z)^k\\). In this case we know that \\(\\Ex Z^k = 0\\), because the results for positive \\(z\\) exactly balance out those for negative \\(z\\), so no Monte Carlo sampling is necessary. If our statistician has somehow forgotten that, though, she will get a pleasant surprise! After just two samples, she will get the estimate \\[\\frac{1}{2} \\big(Z_1^k + (-Z_1)^k \\big) = \\frac{1}{2} (Z_1^k - Z_1^k) = 0 ,\\] Thereby getting the result exactly right. Indeed, we have perfect negative correlation \\(\\rho = -1\\), which is the “best-case scenario”.",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html#bonus-example",
    "href": "lectures/L09-is-2.html#bonus-example",
    "title": "9  Importance sampling II",
    "section": "9.2 Bonus example",
    "text": "9.2 Bonus example\n\nExample 9.2 Let \\(X \\sim \\operatorname{U}[0, 10]\\) be an uniform distribution, so \\(f(x) = \\frac{1}{10}\\) for \\(0 \\leq x \\leq 10\\), and let $\\phi(x) = \\mathrm{e}^{-|x-8|}$. Estimate $(X).\nThe standard Monte Carlo estimator and its RMSE are as follows\n\nphi &lt;- function(x) exp(-abs(x - 8))\nn &lt;- 1e6\nsamples &lt;- runif(n, 0, 10)\nMCest &lt;- mean(phi(samples))\nMC_MSE &lt;- var(phi(samples)) / n\nc(MCest, sqrt(MC_MSE))\n\n[1] 0.1863843908 0.0002536575\n\n\nMaybe we can improve on this using importance sampling. Let’s have a look at a graph of \\(f(y)\\,\\phi(y)\\) (blue line).\n\n\nCode for drawing this graph\ncurve(\n  dunif(x, 0, 10) * exp(-abs(x - 8)), n = 1001, from = 0, to = 10,\n  col = \"blue\", lwd = 3,\n  xlim = c(0, 10), xlab = \"y\", ylab = \"\"\n)\ncurve(\n  dnorm(x, 8, 2)*0.36, n = 1001, from = -1, to = 11,\n  add = TRUE, col = \"red\", lwd = 2,\n)\nlegend(\n  \"topleft\",\n  c(expression(paste(\"f(y)\", varphi, \"(y)\")), \"N(8, 4) [scaled]\"),\n  lwd = c(3, 2), col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nAfter some experimentation, I decided that \\(Y \\sim \\operatorname(8, 2^2)\\) (red line; not to same scale) seemed to work quite well, in \\(g(y)\\) being roughly proportional to \\(f(y)\\,\\phi(y)\\). I was tempted to take the standard deviation less than 2, the get the red curve a bit tighter. But I discovered that \\(f(y)/g(y)\\) got very large for \\(y\\) near 0, due to the left tale decaying to quickly. The following graph shows \\(\\frac{f(x)}{g(x)} \\phi(x)\\), and shows that the spike at 0 is now under control compared to the “main spike” at \\(y = 8\\).\n\n\nCode for drawing this graph\ncurve(\n  dunif(x, 0, 10) * exp(-abs(x - 8)) / dnorm(x, 8, 2), n = 1001, from = -2, to = 12,\n  col = \"red\", lwd = 3,\n  xlim = c(-1, 11), xlab = \"y\", ylab = \"\"\n)\n\n\n\n\n\n\n\n\n\nSo our importance sampling estimate is as follows.\n\nphi &lt;- function(x) exp(-abs(x - 8))\npdf_x &lt;- function(x) dunif(x, 0, 10)\npdf_y &lt;- function(y) dnorm(y, 8, 2)\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n, 8, 2)\nISest &lt;- mean((pdf_x(samples) / pdf_y(samples)) * phi(samples))\nIS_MSE &lt;- var((pdf_x(samples) / pdf_y(samples)) * phi(samples)) / n\nc(ISest, sqrt(IS_MSE))\n\n[1] 0.1864286978 0.0001351437\n\n\nWe see that the RMSE has roughly halved, which is the equivalent of taking four times as many samples.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  }
]