[
  {
    "objectID": "lectures/L14-rejection.html",
    "href": "lectures/L14-rejection.html",
    "title": "14  Rejection sampling",
    "section": "",
    "text": "14.1 Rejection\nIn the last two lectures, we have taken standard uniform \\(U \\sim \\operatorname{U}[0,1]\\) random variables, and have applied a function to them to transform into some other distribution \\(X\\). One \\(U\\) gets turned into one \\(X\\). (Or, for the Box–Muller transform, two \\(U\\)s become two \\(X\\)s.)\nBut so far, we have taken each sample we are given. But another way to get a different distribution is to throw out samples we don’t like and wait until we get a sample we do like. This is called rejection sampling.\nSuppose we want not a \\(\\operatorname{U}[0,1]\\) random variable but instead a \\(\\operatorname{U}[0,\\tfrac12]\\) random variable. One way we’ve already seen to do this is by the inverse transform method: simply multiply \\(U\\) by \\(\\tfrac12\\). But we could also do this by rejection. We start with a proposed sample \\(U \\sim \\operatorname{U}[0,1]\\). If \\(U \\leq \\tfrac12\\), we “accept” the sample, and keep it. But if \\(U &gt; \\tfrac12\\), we “reject” the samples – we throw it away and ask for a new one. We keep proposing samples until we accept one that’s less than \\(\\tfrac12\\). It should be easy to convince yourself that we get a \\(\\operatorname{U}[0,\\tfrac12]\\) random variable this way. (But we’ll prove it later, if not.)\nThe advantage of rejection sampling is that it can help us get samples from some distributions that we couldn’t access with the inverse transform method. The disadvantage is that it can be costly or slow, because we may have to reject lots of samples before finding enough that we can accept. The more often we reject samples, the slower the procedure will be.\nRejection sampling is particularly useful for sampling from a conditional distribution, such as the conditional distribution of \\(Y\\) given that \\(Y \\in A\\): we simply accept a sample \\(y\\) if \\(y \\in A\\) and reject it if not.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "lectures/L14-rejection.html#rejection",
    "href": "lectures/L14-rejection.html#rejection",
    "title": "14  Rejection sampling",
    "section": "",
    "text": "Example 14.1 Let \\(Y \\sim \\operatorname{N}(0, 1)\\). Suppose we wish to use Monte Carlo estimation to estimate \\(\\mathbb E(Y \\mid Y \\geq 1)\\).\nTo do this, we will need samples from the conditional distribution \\(Y \\mid Y \\geq 1\\). So we accept proposed standard normal samples that are at least 1, and reject proposed samples that are less than 1.\nThere are two ways we could run this in practice. First, we could decide to take \\(n\\) proposal samples from \\(Y\\) and just see how many get accepted.\n\nn_prop &lt;- 1e6\nprops   &lt;- rnorm(n_prop)\naccepts &lt;- props[props &gt;= 1]\nlength(accepts)\n\n[1] 158692\n\nMCest1 &lt;- mean(accepts)\nMCest1\n\n[1] 1.525705\n\n\nWe end up accepting around 160,000 samples out of the 1,000,000 proposals we had to start with.\nSecond, we could keep proposing as many samples as needed until we reach some desired number of acceptances.\n\nn_acc &lt;- 1e5\n\nsamples &lt;- rep(0, n_acc)\ncount &lt;- 0\nfor (i in 1:n_acc) {\n  newsample &lt;- 0\n  while (newsample &lt; 1) {\n    newsample &lt;- rnorm(1)\n    count &lt;- count + 1\n  }\n  samples[i] &lt;- newsample\n}\ncount\n\n[1] 628133\n\nMCest2 &lt;- mean(samples)\nMCest2\n\n[1] 1.526865\n\n\nThis required taking about 630,000 proposals to get 100,000 acceptances.\nHere we used a “while” loop to keep taking samples until we go one that was not less than 1. The lines involving count were just so I could see how many proposals ended up being needed – these aren’t an integral part of the code.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "lectures/L14-rejection.html#acceptance-probability",
    "href": "lectures/L14-rejection.html#acceptance-probability",
    "title": "14  Rejection sampling",
    "section": "14.2 Acceptance probability",
    "text": "14.2 Acceptance probability\nSo far, we have looked at always accepting or always rejecting a proposed sample, depending on its value. But we could “perhaps” accept some proposals too. Suppose we are already sampling from some distribution \\(Y\\) (perhaps generated via the inverse transform method, for example). If we see the proposed sample \\(Y = x\\), we could accept it with some acceptance probability \\(\\alpha(x) \\in [0,1]\\). We can control the accepted samples more delicately by adjusting this acceptance function \\(\\alpha\\) to values that aren’t just 0 or 1.\nWhat is the distribution of an accepted sample \\(X\\)?\nWell, using Bayes’ theorem, we have in the discrete case \\[\\mathbb P(X = x) = \\mathbb P(Y = x \\mid \\text{accept}) = \\frac{\\mathbb P(Y = x)\\,\\mathbb P(\\text{accept} \\mid Y = x)}{\\mathbb P(\\text{accept})} = \\frac{1}{Z} \\alpha(x)\\,\\mathbb P(Y = x) .\\] where \\(Z = \\mathbb P(\\text{accept})\\) is the normalising constant. In the continuous case, with \\(g\\) the PDF of the original \\(Y\\) and \\(f\\) the PDF of the accepted \\(X\\), we have \\[ f(x) = g(x \\mid \\text{accept}) = \\frac{g(x)\\,\\mathbb P(\\text{accept} \\mid X = x)}{\\mathbb P(\\text{accept})} = \\frac{1}{Z}\\,\\alpha(x)\\,g(x) , \\] where \\(Z = \\mathbb P(\\text{accept})\\) again.\n\nExample 14.2 Suppose we wish to sample from the distribution \\[ f(x) \\propto \\exp\\big(-\\tfrac12x^2\\big)\\,(\\sin^2 x) . \\tag{14.1}\\] How can we do this?\nWell, we can note that the PDF of the standard normal is \\[ g(x) = \\frac{1}{2\\pi}\\,\\exp\\big(-\\tfrac12x^2\\big) \\propto \\exp\\big(-\\tfrac12x^2\\big) \\] and that \\[ 0 \\leq \\sin^2x\\leq 1 .\\] (Here, \\(\\sin^2 x\\) means \\((\\sin x)^2\\), by the way.) This means that, if we take proposals \\(Y \\sim \\operatorname{N}(0,1)\\), and then accept an proposed sample with probability \\(\\alpha(x) = \\sin^2 x\\), that will give us the distribution Equation 14.1.\n\nn_prop &lt;- 1e6\nprops &lt;- rnorm(n_prop)\naccepts &lt;- props[runif(n_prop) &lt;= sin(props)^2]\nlength(accepts)\n\n[1] 431663\n\nhist(accepts, probability = TRUE, breaks = 50)\ncurve(\n  0.92 * exp(-x^2 / 2) * sin(x)^2, add = TRUE, n = 1001,\n  lwd = 2, col = \"blue\"\n)\n\n\n\n\n\n\n\n\nBy rejecting lots of proposals with values near 0, we turned the unimodal (“one hump”)proposal distribution \\(Y \\sim \\operatorname{N}(0,1)\\) into this interesting bimodal (“two hump”) distribution.\nLet’s explain line 3 more carefully. We want to accept a proposal \\(x\\) with probability \\(\\sin^2 x\\). We saw in Lecture 12 that we can simulate a Bernoulli\\((p)\\) distribution by taking the value 1 is \\(U \\leq p\\) and taking 0 if \\(U &gt; p\\). So in line 3, we are accepting each proposed sample \\(x_i\\) if a standard uniform variate \\(u_i\\) satisfies \\(u_i \\leq \\sin^2 x_i\\).\nIn this example, we found we accepted about 430,000 samples.\n\nIn this example, we managed to sample from the PDF in Equation 14.1, \\[ f(x) = \\frac{1}{Z}\\,\\exp\\big(-\\tfrac12x^2\\big)\\,(\\sin^2 x) ,\\] even though we never found out what the normalising constant \\(Z\\) was. This idea – that we can sample from a distribution even if we only know it up to a multiplicative constant – is a very important one that will come up a lot later in this module.\nWe won’t go into that idea deeply now, but we briefly mention that it is very important in Bayesian statistics. In Bayesian statistics, the posterior distribution is often known only up to proportionality. That’s because we have \\[ \\begin{align}\n\\text{posterior} &\\propto \\text{prior}\\times\\text{likelihood} \\\\\n\\pi(\\theta \\mid x) &\\propto\\, \\pi(x) \\times p(x \\mid \\theta)\n\\end{align} \\] It’s often very difficult to find the normalising constant in this expression – indeed, it can be impossible in practice. So being able to sample from such a posterior distribution without finding the constant is very important for Bayesian statisticians.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "lectures/L14-rejection.html#how-many-samples",
    "href": "lectures/L14-rejection.html#how-many-samples",
    "title": "14  Rejection sampling",
    "section": "14.3 How many samples?",
    "text": "14.3 How many samples?\nWe have mentioned that the downside of rejection sampling is that we may have to take lots of samples to get enough accepted ones. Or, conversely, we may not get enough accepted samples from a fixed number of “proposed” samples. Remember that the accuracy of Monte Carlo estimation, for example, depends on how many samples we get – the mean-square error scales like \\(1/n\\) and the root-mean-square error like \\(1/\\sqrt{n}\\). So it’s important to be able to get a lot of samples \\(n\\).\nLet’s examine these questions a bit closer. Write \\(a = \\mathbb P(\\text{accept})\\) for the probability a sample \\(Y\\) gets accepted (a priori, before we have seen the value \\(Y = x\\) of that sample). In the discrete case, this is \\[ a = \\mathbb P(\\text{accept}) = \\sum_x \\mathbb P(Y = x)\\,\\alpha(x) , \\] and in the discrete case this is \\[ a  = \\mathbb P(\\text{accept}) = \\int_{-\\infty}^{+\\infty} g(x)\\,\\alpha(x)\\,\\mathrm{d}x . \\] In both cases, this can be written more succinctly as \\(a = \\Exg\\alpha(Y)\\).\nLet’s first look at the “fixed number of proposed samples” case.\nIf we take \\(m\\) proposed samples, each is accepted independently with probability \\(a\\). So the total number of accepted samples \\(N\\) follows a binomial distribution \\(N \\sim \\operatorname{Bin}(m, a)\\). This is because the binomial distribution describes the number of successes from \\(m\\) trials. This has expectation \\(am\\) and standard deviation \\(\\sqrt{a(1-a)m}\\).\nFor large \\(m\\), the standard deviation will be very small compared to the expectation (unless \\(a\\) is very close to \\(0\\) or \\(1\\)), so the number of acceptances \\(N\\) will be very close to \\(am\\). If we need a more precise approximation, \\(N\\) can be approximated by a normal distribution \\(N \\approx \\operatorname{N}(am, a(1-a)m)\\). When \\(a\\) is very small, \\(N\\) can be better approximated by a Poisson distribution \\(N \\approx \\operatorname{Po}(am)\\).\nThe second way is the “keep taking proposed samples until we have accepted enough of them” way.\nTo get one acceptance requires a geometric \\(\\operatorname{Geom}(a)\\) number of samples. This is because the geometric distribution counts the number of trials needed until the first success. To get \\(n\\) accepted samples will require the sum of \\(n\\) independent \\(\\operatorname{Geom}(a)\\) distributions – this is sometimes called the negative binomial distribution \\(M \\sim \\operatorname{NegBin}(n, a)\\), which is the number of trials needed until the \\(n\\)th success. This has expectation \\(n/a\\) and standard deviation \\(\\sqrt{(1-a)n}/a\\).\nFor large \\(n\\), the standard deviation will be very small compared to the expectation (unless \\(a\\) is very close to \\(0\\) or \\(1\\)), so the number of proposal \\(M\\) will be very close to \\(n/a\\). If we need a more precise approximation, \\(M\\) can be approximated by a normal distribution \\(M \\approx \\operatorname{N}(n/a, (1-a)n/a^2)\\). When \\(a\\) is very small, \\(N\\) can be better approximated by a Gamma distribution \\(M \\approx \\Gamma(n, a)\\).\n\nExample 14.3 In Example 14.1, we accepted \\(Y \\sim \\operatorname{N}(0,1)\\) if \\(Y \\geq 1\\). In this case, we happen to know the acceptance probability exactly: it’s\\(\\mathbb P(Y \\geq 1) = 0.158\\).\nFrom 1,000,000 proposals, the number of accepted samples we might get is shown below.\n\n\nCode for drawing this graph\nn_prop &lt;- 1e6\nacc_prob &lt;- pnorm(1, lower.tail = FALSE)\nplot(\n  0:2e5, dbinom(0:2e5, n_prop, acc_prob),\n  type = \"l\", col = \"blue\", lwd = 2,\n  xlab = \"number of accepted samples\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nWe see that the number of acceptance will be very close to \\(an = 0.158\\times 1\\,000\\,000 = 158\\,000\\).\nThe number of proposals required for 100,000 acceptances is shown below.\n\n\nCode for drawing this graph\nn_acc &lt;- 1e5\nacc_prob &lt;- pnorm(1, lower.tail = FALSE)\nplot(\n  n_acc + 0:7e5, dnbinom(0:7e5, n_acc, acc_prob),\n  type = \"l\", col = \"blue\", lwd = 2,\n  xlab = \"number of required proposals\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nThe number of required proposals will be very close to \\(n/a = 100\\,000 / 0.158 = 630\\,000\\).\n\n\nExample 14.4 Suppose instead we were trying to sample from a standard normal conditional on \\(Y &gt; 4\\). Again, we take a standard normal proposal and accept if the proposal is greater than 4. This is a different matter, because the acceptance probability is very small: about \\(0.00003\\).\nHere, the number of acceptances from 100,000 proposals is\n\n\nCode for drawing this graph\nn_prop &lt;- 1e5\nacc_prob &lt;- pnorm(4, lower.tail = FALSE)\nplot(\n  0:15, dbinom(0:15, n_prop, acc_prob),\n  type = \"h\", col = \"blue\", lwd = 3,\n  xlab = \"number of accepted samples\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nThere is likely to be a very small number of acceptances – and around a 4% chance there are no acceptances at all. The distribution of the number of acceptances is roughly Poisson.\nThe number of proposals required for just 5 acceptances, on the other hand, is:\n\n\nCode for drawing this graph\nn_acc &lt;- 5\nacc_prob &lt;- pnorm(4, lower.tail = FALSE)\nplot(\n  n_acc + 0:6e5, dnbinom(0:6e5, n_acc, acc_prob),\n  type = \"l\", col = \"blue\", lwd = 2,\n  xlab = \"number of required proposals\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nThis has a very wide spread of how many proposals will be required – anything between about 50,000 and 300,000 is reasonably plausible.\nThis shows the danger of running rejection sampling when the probability of acceptance is very small. Not only is the procedure slow and wasteful, but it’s also very unpredictable.\n\n(Annoyingly, R defines the geometric distribution to be the number of failures before the first success – 1 less then our definition – and the negative binomial to be the number of failures before the \\(n\\)th success – \\(n\\) less than our definition. This is why n_acc + dnbinom() was plotted for the two second graphs.)\nNext time. We look closer at rejection sampling, and in particular how we can target rejection sampling at a given distribution using the “envelope” method.\n\nSummary:\n\nIn rejection sampling, we accept a proposed sample \\(Y = x\\) with probability \\(\\alpha(x)\\).\nIf the PDF of a proposed sample is \\(g\\), then the PDF of an accepted sample is proportional to \\(\\alpha(x) \\,g(x)\\).\nWhen the acceptance probability is low, rejection sampling can require a lot of proposed samples to get enough accepted samples.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 1.4.1 and 1.4.3.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "problems/solutions.html",
    "href": "problems/solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Problem Sheet 1\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.      Let \\(X\\) be uniform on \\([-1,2]\\).\n\n(a)   By hand, calculate the exact value of \\(\\Ex X^4\\).\n\nSolution.\n\\[\\int_{-1}^2 x^4\\,\\frac{1}{2-(-1)}\\,\\mathrm{d}x = \\tfrac13 \\Big[\\tfrac15x^5\\Big]_{-1}^2 = \\tfrac13\\Big(\\tfrac{32}{5}-\\big(-\\tfrac15\\big)\\Big) = \\tfrac{33}{15} = \\tfrac{11}{5} = 2.2\\]\n\n\n\n(b)   Using R, calculate a Monte Carlo estimate for \\(\\Ex X^4\\).\n\nSolution. I used the R code\n\nn &lt;- 1e6\nsamples &lt;- runif(n, -1, 2)\nmean(samples^4)\n\n[1] 2.200859\n\n\n\n\n\n\n2.      Let \\(X\\) and \\(Y\\) both be standard normal distributions. Compute a Monte Carlo estimate of \\(\\Exg \\max\\{X,Y\\}\\). (You may wish to investigate R’s pmax() function.)\n\nSolution. By looking at ?pmax (or maybe searching on Google) I discovered that pmax() gives the “parallel maxima” of two (or more vectors). That is the first element is the maximum of all the first elements of the vectors; the second element is the maximum of all the second elements of the vectors; and so on.\nSo I used the R code\n\nn &lt;- 1e6\nxsamples &lt;- rnorm(n)\nysamples &lt;- rnorm(n)\nmean(pmax(xsamples, ysamples))\n\n[1] 0.5637339\n\n\n\n\n\n3.      You are trapped alone on an island. All you have with you is a tin can (radius \\(r\\)) and a cardboard box (side lengths \\(2r \\times 2r\\)) that it fits snugly inside. You put the can inside the box [left picture].\nWhen it starts raining, each raindrop that falls in the cardboard box might fall into the tin can [middle picture], or might fall into the corners of the box outside the can [right picture].\n\n\n(a)   Using R, simulate rainfall into the box. You may take units such that \\(r = 1\\). Estimate the probability \\(\\theta\\) that a uniformly-random raindrop that falls in the cardboard box falls into the tin can also.\n\nSolution. I set things up so that the box is \\([-1, 1]^2\\), centered at the origin. This means that the inside of the can is the set of points is those \\((x,y)\\) such that \\(x^2 + y^2 \\leq 1\\).\n\nn &lt;- 1e6\nrain_x &lt;- runif(n, -1, 1)\nrain_y &lt;- runif(n, -1, 1)\nin_box &lt;- function(x, y) x^2 + y^2 &lt;= 1\nmean(in_box(rain_x, rain_y))\n\n[1] 0.785823\n\n\n\n\n\n(b)   Calculate exactly the probability \\(\\theta\\).\n\nSolution. The area of the box is \\(2r \\times 2r = 4r^2\\). The area of the can is \\(\\pi r^2\\). So the probability a raindrop landing in the box lands in the can is \\[ \\frac{\\text{area of can}}{\\text{area of box}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\approx 0.785. \\]\n\n\n\n(c)   You realise that, despite being trapped all alone on the island, you now have a way of approximating the value of \\(\\pi\\). If you want to calculate \\(\\pi\\) to 6 decimal places, roughly how many raindrops do you need to fall into the box?\n\nSolution. The phrase “to 6 decimal places” isn’t a precise mathematical one. I’m going to interpret this as getting the root-mean-square error below \\(10^{-6}\\). If you interpret it slightly differently that’s fine – for example, getting the width of a 95% confidence interval below \\(10^{-6}\\) could be another, slightly stricter, criterion.\nOne could work this out by hand. Since the variance of a Bernoulli random variable is \\(p(1-p)\\), the mean-square error of our estimator is \\[ \\frac{\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})}{n} \\approx \\frac{0.169}{n} . \\] So we need \\[n = \\frac{0.169}{(10^{-6})^2} \\approx 169 \\text{ billion} . \\]\nThat said, if we are trapped on our desert island, maybe we don’t know what \\(\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})\\) is. In that case we could do this using the can and the box. Our estimate of the variance is\n\nvar_est &lt;- var(in_box(rain_x, rain_y))\nvar_est / (1e-6)^2\n\n[1] 168305380974\n\n\nWe will probably spend a long time waiting for that much rain!\n\n\n\n\n4.      Let \\(h(x) = 1/(x + 0.1)\\). We wish to estimate \\(\\int_0^5 h(x) \\, \\mathrm{d}x\\) using a Monte Carlo method.\n\n(a)   Estimate the integral using \\(X\\) uniform on \\([0,5]\\).\n\nSolution.\n\nn &lt;- 1e6\nintegrand &lt;- function(x) 1 / (x + 0.1)\nsamples1 &lt;- runif(n, 0, 5)\nmean(5 * integrand(samples1))\n\n[1] 3.929335\n\n\n(In fact, the true answer is \\(\\log(5.1) - \\log(0.1) = 3.932\\), so it looks like this is working correctly.)\n\n\n\n(b)   Can you come up with a choice of \\(X\\) that improves on the estimate from (a)?\n\nSolution. Let’s look at a graph of the integrand \\(h\\).\n\n\nCode for drawing this graph\ncurve(\n  integrand, n = 1001, from = 0, to = 5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(0,5)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nWe see that we get a much bigger contribution to the integral from values near 0. So a random variable that picks values nearer to 0 more often might have a chance of giving a more accurate result.\n\nI decided to try an exponential distribution with rate 1, which should sample the smaller values of \\(x\\) more often.\n\npdf &lt;- function(x) dexp(x, 1)\nphi &lt;- function(x) (integrand(x) / pdf(x)) * (x &lt;= 5)\n\nsamples2 &lt;- rexp(n, 1)\nmean(phi(samples2))\n\n[1] 3.935744\n\n\n(I had to include x &lt;= 5 in the expression for \\(\\phi\\), because my exponential distribution will sometimes take samples above 5, but they should count as 0 in an estimate for the integral between 0 and 5.)\nTo see whether or not this was an improvement, I estimated the mean-square error.\n\nvar(5 * integrand(samples1)) / n\n\n[1] 3.361979e-05\n\nvar(phi(samples2)) / n\n\n[1] 6.073885e-06\n\n\nI found that I had reduced the mean-square error by roughly a factor of 5. \n\n\n\n\n5.      When calculating a Monte Carlo estimate \\(\\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\\), one might wish to first generate the \\(n\\) samples \\((x_1, x_2, \\dots, x_n)\\) and store them, and only then, after all samples are generated, finally calculate the estimate. However, when \\(n\\) is extremely large, storing all \\(n\\) samples uses up a lot of space in a computer’s memory. Describe (in words, in R code, or in a mixture of the two) how the Monte Carlo estimate could be produced using much less memory.\n\nSolution. The idea is to keep a “running total” of the \\(\\phi(x_i)\\)s. Then we only have to store that running total, not all the samples. Once this has been done \\(n\\) times, then divide by \\(n\\) to get the estimate.\nIn R code, this might be something like\n\nn &lt;- 1e6\n\ntotal &lt;- 0\nfor (i in 1:n) {\n  sample &lt;- # sampling code for 1 sample\n  total &lt;- total + phi(sample)\n}\n\nMCest &lt;- total / n\n\n\n\n\n6.      Show that the indicator functions \\(\\mathbb I_A(X)\\) and \\(\\mathbb I_B(X)\\) have correlation 0 if and only if the events \\(\\{X \\in A\\}\\) and \\(\\{X \\in B\\}\\) are independent.\n\nSolution. Recall that two random variables \\(U\\), \\(V\\) have correlation 0 if and only if their covariance \\(\\Cov(U,V) = \\Ex UV - (\\Ex U)(\\Ex V)\\) is 0 too.\nWe know that \\(\\Exg\\Ind_A(X) = \\mathbb P(X \\in A)\\) and \\(\\Exg \\Ind_B(Y) = \\mathbb P(X \\in B)\\). What about \\(\\Exg \\Ind_A(X) \\Ind_B(X)\\)? Well, \\(\\Ind_A(x) \\Ind_B(x)\\) is 1 if and only if both indicator functions equal 1, which is if and only if both \\(x \\in A\\) and \\(x \\in B\\). So \\(\\Exg \\Ind_A(X) \\Ind_B(X) = \\mathbb P(X \\in A \\text{ and } X \\in B)\\).\nSo the covariance is \\[ \\Cov \\big(\\Ind_A(X), \\Ind_B(X) \\big) = \\mathbb P(X \\in A \\text{ and } X \\in B) - \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B) . \\] If this is 0, then \\(\\mathbb P(X \\in A \\text{ and } X \\in B) = \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B)\\), which is precisely the definition of those two events being independent.\n\n\n\n7.      Let \\(X\\) be an exponential distribution with rate 1.\n\n(a)   Estimate \\(\\mathbb EX^{2.1}\\) using the standard Monte Carlo method.\n\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 1)\nmean(samples^2.1)\n\n[1] 2.204066\n\n\n\n\n\n(b)   Estimate \\(\\mathbb EX^{2.1}\\) using \\(X^2\\) as a control variate. (You may recall that if \\(Y\\) is exponential with rate \\(\\lambda\\) then \\(\\mathbb EY^2 = 2/\\lambda^2\\).)\n\nSolution. We have \\(\\Ex X^2 = 2\\). So, re-using the same sample as before (you don’t have to do this – you could take new samples), our R code is as follows.\n\nmean(samples^2.1 - samples^2) + 2\n\n[1] 2.198793\n\n\n\n\n\n(c)   Which method is better?\n\nSolution. The better answer is the one with the smaller mean-square error.\nFor the basic method,\n\nvar(samples^2.1) / n\n\n[1] 2.82498e-05\n\n\nFor the control variate method,\n\nvar(samples^2.1 - samples^2) / n\n\n[1] 6.958636e-07\n\n\nSo the control variates method is much, much better.\n\n\n\n\n8.      Let \\(Z\\) be a standard normal distribution. A statistician has been investigating Monte Carlo estimates of \\(\\mathbb EZ^k\\) for different positive integers values of \\(k\\). Her colleague suggests using \\(Z' = -Z\\) as an antithetic variable. Without running any R code, explain whether or not this is a good idea (a) when \\(k\\) is even; (b) when \\(k\\) is odd.\n\nSolution.\n(a) When \\(k\\) is even, we have \\(Z^k = (-Z)^k\\). So the antithetic variables method just repeats each sample twice. This is obviously no benefit at all, and just wastes time. Indeed, we have perfect positive correlation \\(\\rho = +1\\), which is the “worst-case scenario”.\n(b) When \\(k\\) is odd, we have \\(Z^k = -(-Z)^k\\). In this case we know that \\(\\Ex Z^k = 0\\), because the results for positive \\(z\\) exactly balance out those for negative \\(z\\), so no Monte Carlo sampling is necessary. If our statistician has somehow forgotten that, though, she will get a pleasant surprise! After just two samples, she will get the estimate \\[\\frac{1}{2} \\big(Z_1^k + (-Z_1)^k \\big) = \\frac{1}{2} (Z_1^k - Z_1^k) = 0 ,\\] Thereby getting the result exactly right. Indeed, we have perfect negative correlation \\(\\rho = -1\\), which is the “best-case scenario”.\n\n\n\n\nProblem Sheet 2\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.      [2018 exam, Question 4]\n\n(a)   Suppose it is desired to estimate the value of an integral \\[ \\theta = \\int_0^1 h(x)\\,\\mathrm{d}x \\] by Monte Carlo integration\n\n        i.   By splitting \\(h(x) = \\phi(x)f(x)\\), where \\(f\\) is a probability density function, describe how the Monte Carlo method of estimating \\(\\theta\\) works.\n\nSolution. As suggested, we have \\[ \\theta = \\int_0^1 h(x)\\,\\mathrm{d}x = \\int_0^1 \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\Exg \\phi(X) ,\\] where \\(X\\) is a random variable with PDF \\(f\\). To estimate this, we sample \\(X_1, \\dots, X_n\\) from \\(X\\), and use \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) . \\]\n\n\n\n        ii.  Let \\(\\widehat\\theta_n\\) be the Monte Carlo estimate based on \\(n\\) simulations. Find the expectation and variance of \\(\\widehat\\theta_n\\), as a function of \\(n\\).\n\nSolution. As in lectures, we have \\[ \\Exg \\theta_n^{\\mathrm{MC}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n}\\,n\\Exg\\phi(X) = \\Exg\\phi(X) \\] and \\[ \\Var\\big((\\theta_n^{\\mathrm{MC}}\\big) = \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n^2}\\,n\\Var\\big(\\phi(X)\\big) = \\frac{1}{n}\\Var\\big(\\phi(X)\\big) . \\]\n\n\n\n        iii.  What guidelines can be given for the choice of \\(f\\) in practice?\n\nSolution. First, \\(f\\) (or equivalently \\(X\\)) should be easy to sample from. Second, we want to minimise \\(\\Var(\\phi(X))\\), so should pick \\(f\\) approximately proportional to \\(h\\), so that \\(\\phi\\) is roughly constant, and therefore has low variance.\nIn the absence of better options, \\(X\\) being uniform on \\([0, 1]\\) (so \\(f(x) = 1\\) on this interval) is often not a bad choice.\n\n\n\n\n(b)  Consider evaluation the integral \\[ \\int_0^1 x^2\\,\\mathrm{d}x \\] by Monte Carlo integration using \\(f(x) = \\Ind_{[0,1]}(x)\\). Write down the Monte Carlo estimator \\(\\widehat\\theta_n\\).\n\nSolution. Since \\(f(x) = 1\\) on this interval, we must take \\(\\phi(x) = x^2\\). Thus the estimator is \\[\\widehat\\theta_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2 ,\\] where \\(X_i \\sim \\operatorname{U}[0,1]\\) are independent.\n\n          Explain how antithetic variables can be used in this situation, and justify briefly why their use here is guaranteed to improve efficiency.\n\nSolution. Antithetic variables attempt to reduce the variance in Monte Carlo estimation by using pairs of variables \\((X_i, X'_i)\\) that both have the same distribution as \\(X\\), but where \\(\\phi(X)\\) and \\(\\phi(X')\\) are negatively correlated.\nIn this situation, if \\(X_i \\sim \\operatorname{U}[0,1]\\), then \\(X'_i = 1 - X_i\\) has this same distribution. The corresponding antithetic variable estimator is \\[\\widehat\\theta_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(X_i^2 + (1 - X_i)^2\\big) .\\]\nAn anitithetic variables estimator always decreases the variance of a Monte Carlo estimator if the correlation (or, equivalently, the covariance) between \\(\\phi(X)\\) and \\(\\phi(X')\\) is negative. We saw in lectures that, if \\(X \\sim \\operatorname{U}[0,1]\\) and \\(\\phi\\) is monotonically increasing, then \\(\\phi(X)\\) and \\(\\phi(1 - X)\\) have negative correlation. Since \\(x^2\\) is increasing on \\([0,1]\\), that is the case here.\n\n          For \\(U \\sim \\operatorname{U}[0,1]\\), use the results \\[\\Ex U^2 = \\tfrac13 \\qquad \\Ex U^4 = \\tfrac15 \\qquad \\Ex U^2(1-U)^2 = \\tfrac{1}{30} \\] to find the correlation between \\(U^2\\) and \\((1-U)^2\\). Hence, or otherwise, confirm that using antithetic variables reduces the variance of the Monte Carlo estimator by a factor of 8.\n\nSolution. We need to find the correlation between \\(U^2\\) and \\((1 - U)^2\\), where \\(U \\sim \\operatorname{U}[0,1]\\). The covariance is \\[ \\operatorname{Cov}\\big(U^2, (1-U)^2\\big)\n= \\Ex U^2(1-U)^2 - \\big(\\Ex U^2\\big) \\big(\\Ex (1-U)^2\\big)\n= \\tfrac{1}{30} - \\Big(\\tfrac{1}{3}\\Big)^2 = -\\tfrac{7}{90} .\\] The variance are both \\[ \\Var(U^2) = \\Ex U^4 - \\big(\\Ex U^2\\big)^2 = \\tfrac15 - \\Big(\\tfrac{1}{3}\\Big)^2 = \\tfrac{4}{45} . \\] Hence, the correlation is \\[ \\rho = \\operatorname{Corr}\\big(U^2, (1-U)^2\\big) = \\frac{-\\frac{7}{90}}{\\frac{4}{45}} = -\\tfrac78 . \\]\nThe variance of the standard Monte Carlo estimator is \\(\\frac{1}{n}\\Var(\\phi(X))\\), while the variance of the antithetic variables estimator is \\(\\frac{1+\\rho}{n}\\Var(\\phi(X))\\). So the variance changes by a factor of \\(1 + \\rho\\), which here is \\(1 - \\frac{7}{8} = \\frac{1}{8}\\). So the variance reduces by a factor of 8, as claimed.\n\n          (You may use without proof any results about the variance of antithetic variable estimates, but you should clearly state any results you are using.)\n\n\n\n2.     Let \\(X \\sim \\operatorname{N}(0,1)\\). Consider importance sampling estimation for the probability \\(\\theta = \\mathbb P(3 \\leq X \\leq 4)\\) using samples \\(Y_i\\) from the following sample distributions: (i) \\(Y \\sim \\operatorname{N}(1,1)\\); (ii) \\(Y \\sim \\operatorname{N}(2,1)\\); (iii) \\(Y \\sim \\operatorname{N}(3.5,1)\\); (iv) \\(Y \\sim 3 + \\operatorname{Exp}(1)\\).\nEach of these four distributions gives rise to a different importance sampling method. Our aim is to compare the resulting estimates.\n\n(a)  For each of the four methods, estimate the variance \\(\\Var\\big(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\big)\\). Which of these four methods gives the best results?\n\nSolution. I used the following R code\n\nn &lt;- 1e5\nphi   &lt;- function(x) (x &gt;= 3) & (x &lt;= 4)\npdf_x &lt;- function(x) dnorm(x, 0, 1)\n\npdf_y1 &lt;- function(x) dnorm(x, 1, 1)\nsamples_y1 &lt;- rnorm(n, 1, 1)\nvar1 &lt;- var((pdf_x(samples_y1) / pdf_y1(samples_y1)) * phi(samples_y1))\n\npdf_y2 &lt;- function(x) dnorm(x, 2, 1)\nsamples_y2 &lt;- rnorm(n, 2, 1)\nvar2 &lt;- var((pdf_x(samples_y2) / pdf_y2(samples_y2)) * phi(samples_y2))\n\npdf_y3 &lt;- function(x) dnorm(x, 3.5, 1)\nsamples_y3 &lt;- rnorm(n, 3.5, 1)\nvar3 &lt;- var((pdf_x(samples_y3) / pdf_y3(samples_y3)) * phi(samples_y3))\n\npdf_y4 &lt;- function(x) dexp(x - 3, 1)\nsamples_y4 &lt;- 3 + rexp(n, 1)\nvar4 &lt;- var((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4))\n\nsignif(c(var1, var2, var3, var4), 3)\n\n[1] 8.20e-05 1.39e-05 6.69e-06 1.92e-06\n\n\n(For the fourth PDF, we used that the PDF of \\(3 + Z\\) is \\(f_Z(z-3)\\).)\nWe see that the fourth method \\(3 + \\operatorname{Exp}(1)\\) is the most accurate.\n\n\n\n(b)  Determine a good estimate for \\(\\mathbb P(3 \\leq X \\leq 4)\\), and discuss the accuracy of your estimate.\n\nSolution. We’ll use the fourth method, and boost the number of samples to one million.\n\nn &lt;- 1e6\nISest &lt;- mean((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4))\nIS_MSE &lt;- var((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4)) / n\nc(ISest, sqrt(IS_MSE))\n\n[1] 1.316356e-03 1.387238e-06\n\n\nSo our estimate is \\(\\widehat\\theta = 0.00132\\). Since the RMSE is three orders of magnitude less than the estimate, so the estimate is probably accurate to a couple of significant figures.\n\n\n\n(c)  For each of the four methods, approximate how many samples from \\(Y\\) are required to reduce the root-mean-square error of the estimate of \\(\\mathbb P(3 \\leq X \\leq 4)\\) to 1%?\n\nSolution. To get the error to 1% means an absolute error of roughly \\(\\epsilon = 0.01\\widehat\\theta\\). Then we know that the required number of samples is \\[ n = \\frac{\\Var\\big(\\frac{f(Y)}{g(Y)}\\phi(Y)\\big)}{\\epsilon^2} . \\]\n\neps &lt;- 0.01 * ISest\nround(c(var1, var2, var3, var4) / eps^2)\n\n[1] 473334  80024  38607  11106\n\n\n\n\n\n\n3.     [2017 exam, Question 3]\n\n(a)  Defining any notation you use, write down the basic Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) and the importance sampling estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) for an expectation of the form \\(\\theta = \\Exg \\phi(X)\\), where \\(X\\) is a random variable with probability density function \\(f\\).\nWhat is the advantage of importance sampling over the standard Monte Carlo method?\n\nSolution. The basic Monte Carlo estimator is \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n\\phi(X_i) , \\] where the \\(X_i\\) are independent random samples from \\(X\\).\nThe basic importance sampling estimator is \\[ \\widehat\\theta_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) , \\] where the \\(Y_i\\) are independent random samples from a random variable \\(Y\\) with probability density function \\(g\\). We must have \\(g(y) &gt; 0\\) whenever \\(f(y) &gt; 0\\).\nThe main advantage of the importance sampling estimator is that can reduce the variance of the estimator by oversampling the most important areas of \\(y\\), but then downweighting those samples. Another advantage is that importance sampling can be used when it is difficult to sample from \\(X\\).\n\n\n\n(b)  Prove that both the basic Monte Carlo and importance sampling estimates from part (a) are unbiased.\n\nSolution. For the standard Monte Carlo estimator, \\[ \\Exg \\theta_n^{\\mathrm{MC}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n}\\,n\\Exg\\phi(X) = \\Exg\\phi(X) = \\theta .\\]\nFor the importance sampling estimator, first note that \\[ \\mathbb E \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\int_{-\\infty}^{+\\infty} \\frac{f(y)}{g(y)}\\,\\phi(y)\\,g(y)\\,\\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\phi(y) \\,f(y) \\, \\mathrm{d}y = \\Exg\\phi(X)  , \\] since \\(f\\) is the PDF of \\(X\\). Hence \\[ \\Exg \\widehat\\theta_n^{\\mathrm{IS}} = \\Exg \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) \\right) = \\frac{1}{n}\\,n\\,\\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\Exg \\phi(X) = \\theta. \\]\nHence, both estimators are unbiased.\n\n\n\n(c)  Show that the variance of the importance sampling estimator is given by \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\frac{1}{n}\\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y - \\frac{1}{n}\\big(\\Exg \\phi(X)\\big)^2. \\]\n\nSolution. First, note that \\[\\begin{align} \\Var \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right)\n&= \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right)^2 - \\big(\\Exg \\phi(X)\\big)^2 \\\\\n&= \\int_{-\\infty}^{+\\infty} \\left(\\frac{f(y)}{g(y)}\\,\\phi(y)\\right)^2 g(y)\\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2\\\\\n&= \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2}{g(y)^2}\\,\\phi(y)^2 \\,g(y)\\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2\\\\\n&= \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2}{g(y)}\\,\\phi(y)^2 \\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2 \\end{align} \\] Second, we have \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) \\right) = \\frac{1}{n} \\Var \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) . \\] Putting these together proves the result.\n\n\n\n(d)  Let \\(X \\sim \\operatorname{N}(0,2)\\) and \\(a \\in \\mathbb R\\). We want to estimate \\(\\theta = \\Ex \\big(\\sqrt{2}\\exp(-(X-a)^2/4)\\big)\\), using importance sampling with samples \\(Y \\sim \\operatorname{N}(\\mu, 1)\\) for some \\(\\mu \\in \\mathbb R\\). Using the result from part (c), or otherwise, show that in the case the variance of the importance sampling estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) is given by \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\frac{\\exp(\\mu^2 - a\\mu) - \\theta^2}{n} . \\] [Note: This equation has changed since an earlier version of the question.]\n\nSolution. It’s clear, using part (c), that it will suffice to show that \\[ \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y = \\exp(\\mu^2 - a\\mu) . \\] Here, we have \\[ \\begin{align}\nf(y) &= \\frac{1}{\\sqrt{4\\pi}} \\exp\\big(-\\tfrac14 y^2 \\big) \\\\\ng(y) &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-\\mu)^2 \\big) \\\\\n\\phi(y) &= \\sqrt{2} \\exp\\big(-\\tfrac14 (y-a)^2 \\big) .\n\\end{align} \\] Therefore, by a long and painful algebra slog, we have \\[ \\begin{align}\n\\frac{f(y)^2 \\phi(y)^2}{g(y)} &= \\frac{\\frac{1}{4\\pi} \\exp\\big(-\\tfrac12 y^2 \\big)\\times 2\\exp\\big(-\\tfrac12 (y-a)^2 \\big)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-\\mu)^2 \\big)} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y^2 + (y-a)^2 - (y-\\mu)^2 \\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y^2 - 2(a - \\mu)y + a^2 + \\mu^2\\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big((y - (a - \\mu))^2 - (a- \\mu)^2 + a^2 - \\mu^2\\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big((y - (a - \\mu))^2 + 2a\\mu - 2\\mu^2 \\big)\\Big) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\exp(\\mu^2 + a\\mu),\n\\end{align} \\] where we ‘completed the square’ on the fourth line. Thus \\[ \\begin{align}\n\\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y\n&= \\int_{-\\infty}^{+\\infty}  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\exp(\\mu^2 + 2a) \\,\\mathrm{d}y\\\\\n&= \\exp(\\mu^2 + 2a) \\int_{-\\infty}^{+\\infty}  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\,\\mathrm{d}y \\\\\n&= \\exp(\\mu^2 + a\\mu) ,\n\\end{align} \\] since the big integral on the right is the integral of the PDF of a normal \\(\\operatorname{N}(a-\\mu, 1)\\) distribution, so equals 1.\nHence, we have proved the result.\n\n          For fixed \\(n\\) and \\(a\\), find the value of \\(\\mu\\) for which the importance sampling estimator has the smallest mean-square error. Comment on the result.\n\nSolution. Minimising this expression is equivalent to minimising \\(\\mu^2 + a\\mu\\). By differentiating with respect to \\(\\mu\\) (or otherwise), we see that this is at \\(a = \\tfrac12 \\mu\\).\n\n\n\n\n4.     (Answer the following question “by hand”, without using R. You may check your answer with R, if you wish.)\n\n(a)  Consider the LCG with modulus \\(m = 2^4 = 16\\), multiplier \\(a = 5\\), and increment \\(a = 8\\). What is the period of this LCG when started from the seed (i) \\(x_1 = 1\\); (ii) \\(x_1 = 2\\)?\n\nSolution. For (i), we have \\[ \\begin{align}\nx_1 &= 1 \\\\\nx_2 &= (5 \\times 1 + 8) \\bmod 16 = 13 \\bmod 16 = 13 \\\\\nx_3 &= (5 \\times 13 + 8) \\bmod 16 = 73 \\bmod 16 = 9 \\\\\nx_4 &= (5 \\times 9 + 8) \\bmod 16 = 53 \\bmod 16 = 5 \\\\\nx_5 &= (5 \\times 5 + 8) \\bmod 16 = 33 \\bmod 16 = 1 .\n\\end{align}\\] Here, \\(x_5\\) is a repeat of \\(x_1\\), so the period is \\(5-1=4\\).\nFor (ii), we have \\[ \\begin{align}\nx_1 &= 2 \\\\\nx_2 &= (5 \\times 2 + 8) \\bmod 16 = 18 \\bmod 16 = 2 .\n\\end{align}\\] This is an immediate repeat, so the period is 2.\n\n\n\n(b) Consider the LCG with modulus \\(m = 2^4 = 16\\), multiplier \\(a = 2\\), and increment \\(c = 4\\). Start from the seed \\(x_1 = 3\\). (i) When do we first see a repeat output? (ii) What is the period?\n\nSolution. We have \\[ \\begin{align}\nx_1 &= 3 \\\\\nx_2 &= (2 \\times 3 + 4) \\bmod 16 = 10 \\bmod 16 = 10 \\\\\nx_3 &= (2 \\times 10 + 4) \\bmod 16 = 24 \\bmod 16 = 8 \\\\\nx_4 &= (2 \\times 8 + 4) \\bmod 16 = 20 \\bmod 16 = 4 \\\\\nx_5 &= (2 \\times 4 + 4) \\bmod 16 = 12 \\bmod 16 = 12 \\\\\nx_6 &= (2 \\times 12 + 4) \\bmod 16 = 28 \\bmod 16 = 12 .\n\\end{align}\\]\n(i) The first repeat is \\(x_6\\).\n(ii) Since 12 is a fixed point of this LCG, the remainder of the sequence is 12 forever, with period 1.\n\n\n\n\n5.     Consider the following LCGs with modulus \\(m = 2^8 = 256\\):\n (i) \\(a = 31\\), \\(c = 47\\);\n (ii) \\(a = 21\\), \\(c = 47\\);\n (iii) \\(a = 129\\), \\(c = 47\\).\n\n(a)  Without using a computer, work out which of these LCGs have a full period of 256.\n\nSolution.\n (i) Here, \\(a\\) is \\(3 \\bmod 4\\), not \\(1 \\bmod 4\\), so this does not have full period of 256.\n (ii) Here, \\(c\\) is odd and \\(a\\) is \\(1 \\bmod 4\\), so this does have full period of 256.\n (iii) Here, \\(c\\) is odd and \\(a\\) is \\(1 \\bmod 4\\), so this does have full period of 256.\n\n\n\n(b)  Which of these LCGs would make good pseudorandom number generators?\n\nSolution. We will check the random appearance of the outputs using R.\n\nlcg &lt;- function(n, modulus, mult, incr, seed) {\n  samples &lt;- rep(0, n)\n  samples[1] &lt;- seed\n  for (i in 1:(n - 1)) {\n    samples[i + 1] &lt;- (mult * samples[i] + incr) %% modulus\n  }\n  return(samples)\n}\n\n (i) This does not have full period, so is unlikely to be good PRNG. Let’s check\n\nm &lt;- 2^8\nseed &lt;- 1\nplot(lcg(m, m, 31, 47, seed))\n\n\n\n\n\n\n\n\nThe picture confirms that this does not look random, and in fact has very short period of 16.\n (ii) This does have full period, so is a candidate for a good PRNG if the output looks random.\n\nplot(lcg(m, m, 21, 47, seed))\n\n\n\n\n\n\n\n\nThis mostly looks random, but there does appear to be a sort of thick diagonal line in the picture going from bottom left to rop right. I’d might be happy to use this for casual statistical work – the lack of randomness does not seem super-serious – but I would avoid this for cryptographic purposes, for example.\n (iii) This does have full period, so is a candidate for a good PRNG if the output looks random.\n\nplot(lcg(m, m, 129, 47, seed))\n\n\n\n\n\n\n\n\nEven though this has full period, there is a very clear non-random pattern. This is not appropriate for a PRNG.\n\n\n\n\n6.     Consider an LCG with modulus \\(m = 2^{10} = 1024\\), multiplier \\(a = 125\\), and increment \\(c = 5\\). Using R:\n\n(a) Generate 1000 outputs in \\(\\{0, 1, \\dots, 1023\\}\\) from this LCG, starting with the seed \\(x_1 = 1\\).\n[This question originally said 200 outputs, not 1000. It’s fine if you answered that version, but the conclusions to the problem are less interesting that way.]\n\nSolution. Using the same lcg() function from Question 5, we have\n\nn &lt;- 1000\nm &lt;- 2^{10}\nseed &lt;- 1\nsamples1 &lt;- lcg(n, m, 125, 5, seed)\n\n\n\n\n(b)  Convert these to 200 outputs to pseudorandom uniform samples in \\([0, 1]\\).\n\nTo do this, we simply divide the samples by \\(m\\).\n\nsamples1 &lt;- samples1 / m\n\n\n\n\n(c)  Using these samples, obtain a Monte Carlo estimate for \\(\\mathbb E\\cos(U)\\), where \\(U \\sim \\operatorname{U}[0,1]\\).\n\nSolution.\n\nmean(cos(samples1))\n\n[1] 0.8418218\n\n\nThis is very close to the correct answer \\(\\sin 1 = 0.8414\\)\n\n\n\n(d)  What is the root-mean-square error of your estimate?\n\nSolution.\n\nsqrt(var(cos(samples1)) / n)\n\n[1] 0.004392217\n\n\n\n\n\n(e)  Repeat parts (a) to (d) for the LCG with the same \\(m\\), but now with multiplier \\(a = 127\\) and increment \\(c = 4\\).\n\nSolution.\n\nsamples2 &lt;- lcg(n, m, 127, 4, seed)\nsamples2 &lt;- samples2 / m\nmean(cos(samples2))\n\n[1] 0.868257\n\nsqrt(var(cos(samples2)) / n)\n\n[1] 0.003899394\n\n\nThis does not seem to be quite accurate to the answer to \\(\\sin 1 = 0.841\\), and the reported RMSE to too small to account for the error.\nHowever, the problem is that this LCG is not actually a uniform (pseudo)random number generator – it has period 8.\n\nplot(samples2)\n\n\n\n\n\n\n\n\nThus the estimator is just keeping using the same 8 points over and over again. So this is actually estimating \\(\\mathbb E(\\cos Y)\\), where \\(Y\\) is uniform on the 8 points actually visited by the LCG. So while the correct answer is \\(\\mathbb EU = \\sin 1 = 0.841\\), this is in fact estimating \\[ \\begin{multline}\n\\frac{1}{8} \\bigg(\\cos \\frac{1}{2^{10}} + \\cos \\frac{131}{2^{10}} + \\cos \\frac{257}{2^{10}} + \\cos \\frac{899}{2^{10}} \\\\\n+ \\cos \\frac{513}{2^{10}} + \\cos \\frac{643}{2^{10}} + \\cos \\frac{769}{2^{10}} + \\cos \\frac{387}{2^{10}}\\bigg) = 0.868\n\\end{multline} \\] (where the numerators are the 8 values visited by the LCG), which is not the correct answer.",
    "crumbs": [
      "Solutions"
    ]
  }
]