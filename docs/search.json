[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH5835M Statistical Computing",
    "section": "",
    "text": "About MATH5835",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#organisation-of-math5835",
    "href": "index.html#organisation-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Organisation of MATH5835",
    "text": "Organisation of MATH5835\nThis module is MATH5835M Statistical Computing.\nThis module lasts for 11 weeks from 29 September to 12 December 2025. The exam will take place sometime between 12 and 23 January 2026.\nThe module leader, the lecturer, and the main author of these notes is Dr Matthew Aldridge. (You can call me “Matt”, “Matthew”, or “Dr Aldridge”, pronounced “old-ridge”.) My email address is m.aldridge@leeds.ac.uk, although I much prefer questions in person at office hours (see below) rather than by email.\n\nLectures\nThe main way you will learn new material for this module is by attending lectures. There are three lectures per week:\n\nMondays at 1400\nThursdays at 1200\nFridays at 1000\n\nall in in Roger Stevens LT 14.\nI recommend taking your own notes during the lecture. I will put brief summary notes from the lectures on this website, but they will not reflect all the details I say out loud and write on the whiteboard. Lectures will go through material quite quickly and the material may be quite difficult, so it’s likely you’ll want to spend time reading through your notes after the lecture. Lectures should be recorded on the lecture capture system; I find it very difficult to read the whiteboard in these videos, but if you unavoidably miss a lecture, for example due to illness, you may find they are better than nothing.\nIn Weeks 3, 5, 7, 9 and 11, the Thursday lecture will operate as a “problems class” – see more on this below.\nAttendance at lectures in compulsory. You should record your attendance using the UniLeeds app and the QR code on the wall in the 15 minutes before the lecture or the 15 minutes after the lecture (but not during the lecture).\n\n\nProblem sheets and problem classes\nMathematics and statistics are “doing” subjects! To help you learn material for the module and to help you prepare for the exam, I will provide 5 unassessed problem sheets. These are for you to work through in your own time to help you learn; they are not formally assessed. You are welcome to discuss work on the problem sheets with colleagues and friends, although my recommendation would be to write-up your “last, best” attempt neatly by yourself.\nThere will be an optional opportunity to submit one or two questions from the problem sheet to me in advance of the problems class for some brief informal feedback on your work. See the problem sheets for details.\nYou should work through each problem sheet in preparation for the problems class in the Thursday lecture of Week 3, 5, 7, 9 and 11. In the problems class, you should be ready to explain your answers to questions you managed to solve, discuss your progress on questions you partially solved, and ask for help on questions you got stuck on.\nYou can also ask for extra help or feedback at office hours (see below).\n\n\nCoursework\nThere will be one piece of assessed coursework, which will make up 20% of your module mark. You can read more about the coursework here.\nThe coursework will be in the form of a worksheet. The worksheet will have some questions, mostly computational but also mathematical, and you will have to write a report containing your answers and computations.\nThe assessed coursework will be introduced in the computer practical sessions in Week 9.\nThe deadline for the coursework will be the penultimate day of the Autumn term, Thursday 12 December  at 1400. Feedback and marks will be returned on Monday 13 January, the first day of the Spring term.\n\n\nOffice hours\nI will run a n optional “office hours” drop-in session each week for feedback and consultation. You can come along if you want to talk to me about anything on the module, including if you’d like more feedback on your attempts at problem sheet questions. (For extremely short queries, you can approach me before or after lectures, but my response will often be: “Come to my office hours, and we can discuss it there!”)\nOffice hours will happen on Thursdays from 1300 to 1400 – so directly after the Thursday lecture / problems class – in my office, which is EC Stoner 9.10n in “Maths Research Deck” area on the 9th floor of the EC Stoner building. (One way to the Maths Research Deck is via the doors directly opposite the main entrance to the School of Mathematics; you can also get there from Staircase 1 on the Level 10 “red route” through EC Stoner, next to the Maths Satellite.) If you cannot make this time, contact me for an alternative arrangement.\n\n\nExam\nThere will be one exam, which will make up 80% of your module mark.\nThe exam will be in the January 2026 exam period (12–23 January); the date and time will be announced in December. The exam will be in person and on campus.\nThe exam will last 2 hours and 30 minutes. The exam will consist of 4 questions, all compulsory. You will be allowed to use a permitted calculator in the exam.",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#content-of-math5835",
    "href": "index.html#content-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Content of MATH5835",
    "text": "Content of MATH5835\n\nNecessary background\nI recommend that students should have completed at least two undergraduate level courses in probability or statistics – although confidence and proficiency in basic material is more important than very deep knowledge of more complicated topics.\nFor Leeds undergraduates, MATH2715 Statistical Methods is an official prerequisite (please get in touch with me if you are/were a Leeds undergraduate and have not taken MATH2715), although confidence and proficiency in the more basic material of MATH1710 & MATH1712 Probability and Statistics 1 & 2 is probably more important.\nSome knowledge I will assume:\n\nProbability: Basic rules of probability; random variables, both continuous and discrete; “famous” distributions (especially the normal distribution and the continuous uniform distribution); expectation, variance, covariance, correlation; law of large numbers and central limit theorem.\nStatistics: Estimation of parameters; bias and error; sample mean and sample variance\n\nThis module will also include an material on Markov chains. I won’t assume any pre-existing knowledge of this, and I will introduce all new material we need, but students who have studied Markov chains before (for example in the Leeds module MATH2750 Introduction to Markov Processes) may find a couple of lectures here are merely a reminder of things they already know.\nThe lectures will include examples using the R program language. The coursework and problem sheets will require use of R. The exam, while just a “pencil and paper” exam, will require understanding and writing short portions of R code. We will assume basic R capability – that you can enter R commands, store R objects using the &lt;- assignment, and perform basic arithmetic with numbers and vectors. Other concepts will be introduced as necessary. If you want to use R on your own device, I recommend downloading (if you have not already) the R programming language and the program RStudio. (These lecture notes were written in R using RStudio.)\n\n\nSyllabus\nWe plan to cover the following topics in the module:\n\nMonte Carlo estimation: definition and examples; bias and error; variance reduction techniques: control variates, antithetic variables, importance sampling. [9 lectures]\nRandom number generation: pseudo-random number generation using linear congruential generators; inverse transform method; rejection sampling [7 lectures]\nMarkov chain Monte Carlo (MCMC): [7 lectures]\n\nIntroduction to Markov chains in discrete and continuous space\nMetropolis–Hastings algorithm: definition; examples; MCMC in practice; MCMC for Bayesian statistics\n\nResampling methods: Empirical distribution; plug-in estimation; bootstrap statistics; bootstrap estimation [4 lectures]\nFrequently-asked questions [1 lecture]\n\nTogether with the 5 problems classes, this makes 33 lectures.\n\n\nBook\nThe following book is strongly recommended for the module:\n\nJ Voss, An Introduction to Statistical Computing: A simulation-based approach, Wiley Series in Computational Statistics, Wiley, 2014\n\nThe library has electronic access to this book (and two paper copies).\nDr Voss is a lecturer in the School of Mathematics and the University of Leeds, and has taught MATH5835 many times. An Introduction to Statistical Computing grew out of his lecture notes for this module, so the book is ideally suited for this module. My lectures will follow this book closely – specifically:\n\nMonte Carlo estimation: Sections 3.1–3.3\nRandom number generation: Sections 1.1–1.4\nMarkov chain Monte Carlo: Section 2.3 and Sections 4.1–4.3\nBootstrap: Section 5.2\n\nFor a second look at material, for preparatory reading, for optional extended reading, or for extra exercises, this book comes with my highest recommendation!",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html",
    "href": "lectures/L01-mc-intro.html",
    "title": "1  Introduction to Monte Carlo",
    "section": "",
    "text": "\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n\nExample 1.1 Let’s suppose we’ve forgotten the expectation of the exponential distribution \\(X \\sim \\operatorname{Exp}(2)\\) with rate 2. In this simple case, we could work out the answer using the PDF \\(f(x) = 2\\mathrm{e}^{-2x}\\) as\n\\[ \\mathbb E X = \\int_0^\\infty x\\,2\\mathrm{e}^{-2x}\\,\\mathrm{d}x \\]and, without too much difficulty, get the answer \\(\\frac12\\). But instead, let’s do this the Monte Carlo way.\nIn R, we can use the rexp() function to get IID samples from the exponential distribution: the full syntax is rexp(n, rate), which gives n samples from an exponential distribution with rate rate. The following code takes the mean of \\(n = 100\\) samples from the exponential distribution.\n\nn &lt;- 100\nsamples &lt;- rexp(n, 2)\nMCest &lt;- (1 / n) * sum(samples)\nMCest\n\n[1] 0.5378136\n\n\nSo our Monte Carlo estimate is 0.5378, to 4 decimal places.\nThat’s fairly close to the correct answer of \\(\\frac12\\). But we should (hopefully) be able to get a more accurate estimation if we use more samples. We could also simplify the third line of our code by using the mean() function.\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.4994785\n\n\nIn the second line, 1e6 is R code for the scientific notation \\(1 \\times 10^6\\), or a million. I just picked this as “a big number, but where my code still only took a few seconds to run.”\nOur new Monte Carlo estimate is 0.4995, which is (probably) much closer to the true value of \\(\\frac12\\).\n\nBy the way: all R code “chunks” displayed in the notes should work perfectly if you copy-and-paste them into RStudio. (Indeed, when I compile these lecture notes in RStudio, all the R code gets run on my computer – so I’m certain in must work correctly!) If you hover over a code chunk, a little “clipboard” icon should appear in the top-right, and clicking on that will copy it so you can paste it into RStudio. I strongly encourage playing about with the code as a good way to learn this material and explore further!\n\nExample 1.2 Let’s try another example. Let \\(X \\sim \\operatorname{N}(1, 2^2)\\) be a normal distribution with mean 1 and standard deviation 2. Suppose we want to find out \\(\\mathbb E(\\sin X)\\) (for some reason). While it might be possible to somehow calculate the integral \\[ \\mathbb E(\\sin X) = \\int_{-\\infty}^{+\\infty} (\\sin x) \\, \\frac{1}{\\sqrt{2\\pi\\times 2^2}} \\exp\\left(-\\frac{(x - 1)^2}{2\\times 2^2}\\right) \\, \\mathrm{d} x , \\] that looks extremely difficult to me.\nInstead, a Monte Carlo estimation of \\(\\mathbb{E}(\\sin X)\\) is very straightforward: we just take the mean of the sine of a bunch of normally distributed random numbers.\n(We must remember, though, when using the rnorm() function to generate normally distributed random variates, that the third argument is the standard deviation, here \\(2\\), not the variance, here \\(2^2 = 4\\).)\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n, 1, 2)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.1129711\n\n\nOur Monte Carlo estimate is 0.11297.\n\n\n\nSummary:\n\nStatistical computing is about solving statistical problems by combining human ingenuity with computing power.\nThe Monte Carlo estimate of \\(\\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, \\dots, X_n\\) are IID random samples from \\(X\\).\nMonte Carlo estimation typically gets more accurate as the number of samples \\(n\\) gets bigger.\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html",
    "href": "lectures/L02-mc-uses.html",
    "title": "2  Uses of Monte Carlo",
    "section": "",
    "text": "\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n\nExample 2.1 Let \\(Z \\sim \\operatorname{N}(0,1)\\) be a standard normal distribution. Estimate \\(\\mathbb P(Z &gt; 2)\\).\nThis is a question that it is impossible to answer exactly using a pencil and paper: there’s no closed form for \\[ \\mathbb P(Z &gt; 2) = \\int_2^\\infty \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-z^2/2}\\,\\mathrm{d}z , \\] so we’ll have to use an estimation method.\nThe Monte Carlo estimate means taking a random sample \\(Z_1, Z_2, \\dots, Z_n\\) of standard normals, and calculating what proportion of them are greater than 2. In R, we can do this as follows.\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.022873\n\n\nIn the second line, we could have written rnorm(n, 0, 1). But, if you don’t give the parameters mean and sd to the function rnorm(), R just assumes you want the standard normal with mean = 0 and sd = 1.\nWe can check our answer: R’s inbuilt pnorm() function estimates probabilities for the normal distribution (using a method that, in this specific case, is much quicker and more accurate than Monte Carlo estimation). The true answer is very close to\n\npnorm(2, lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nso our estimate was pretty good.\n\n\n\nExample 2.2 Suppose we want to approximate the integral \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x . \\]\nSince this is an integral on the finite interval \\([0,2]\\), it would seem to make sense to pick \\(X\\) to be uniform on \\([0,2]\\). This means we should take \\[\\phi(x) = \\frac{h(x)}{f(x)} = (2-0)h(x) = 2\\,x^{1.6}(2-x)^{0.7}.\\] We can then approximate this integral in R using the Monte Carlo estimator \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x = \\operatorname{\\mathbb{E}} \\phi(X) \\approx \\frac{1}{n} \\sum_{i=1}^n 2\\,X_i^{1.6} (2-X_i)^{0.7} . \\]\n\nn &lt;- 1e6\nintegrand &lt;- function(x) x^1.6 * (2 - x)^0.7\na &lt;- 0\nb &lt;- 2\nsamples &lt;- runif(n, a, b)\nmean((b - a) * integrand(samples))\n\n[1] 1.443793\n\n\nYou have perhaps noticed that, here and elsewhere, I tend to split my R code up into lots of small bits, perhaps slightly unnecessarily. After all, those 6 lines of code could simply have been written as just 2 lines\n\nsamples &lt;- runif(1e6, 0, 2)\nmean(2 * samples^1.6 * (2 - samples)^0.7)\n\nThere’s nothing wrong with that. However, I find that code is easier to read if divided into small pieces. It also makes it easier to tinker with, if I want to use it to solve some similar but slightly different problem.\n\n\nExample 2.3 Suppose we want to approximate the integral \\[ \\int_{-\\infty}^{+\\infty}\n\\mathrm{e}^{-0.1|x|} \\cos x \\, \\mathrm{d}x . \\] This one is an integral on the whole real line, so we can’t take a uniform distribution. Maybe we should take \\(f(x)\\) to be the PDF of a normal distribution, and then put \\[ \\phi(x) = \\frac{h(x)}{f(x)} = \\frac{\\mathrm{e}^{-0.1|x|} \\cos x}{f(x)} . \\]\nBut which normal distribution should we take? Well, we’re allowed to take any one – we will still get an accurate estimate in the limit as \\(n \\to \\infty\\). But we’d like an estimator that gives accurate results at moderate-sized \\(n\\), and picking a “good” distribution for \\(X\\) will help that.\nWe’ll probably get the best results if we pick a distribution that is likely to mostly take values where \\(h(x)\\) is big – or, rather, where the absolute value \\(|h(x)|\\) is big, to be precise. That is because we don’t want to “waste” too many samples where \\(h(x)\\) is very small, because they don’t contribute much to the integral. But we don’t want to “miss” – or only sample very rarely – places where \\(h(x)\\) is big, which contribute a lot to the integral.\nLet’s have a look at the graph of \\(h(x) = \\mathrm{e}^{-0.1|x|} \\cos x\\).\n\n\nCode for drawing this graph\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\n\ncurve(\n  integrand, n = 1001, from = -55, to = 55,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(-50,50)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nThis suggests to me that a mean of 0 and a standard deviation of 20 might work quite well, since this will tend to take values in \\([-40,40]\\) or so.\nWe will use R’s function dnorm() for the probability density function of the normal distribution (which saves us from having to remember what that is).\n\nn &lt;- 1e6\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\npdf       &lt;- function(x) dnorm(x, 0, 20)\nphi       &lt;- function(x) integrand(x) / pdf(x)\n\nsamples &lt;- rnorm(n, 0, 20)\nmean(phi(samples))\n\n[1] 0.235336\n\n\n\n\n\nSummary:\n\nThe indicator \\(\\Ind_A(x)\\) function of a set \\(A\\) is 1 if \\(x \\in A\\) or 0 if \\(x \\notin A\\).\nWe can estimate a probability \\(\\mathbb P(X \\in A)\\) by using the Monte Carlo estimate for \\(\\Exg\\Ind_A(X)\\).\nWe can estimate an integral \\(\\int h(x) \\, \\mathrm{d}x\\) by using a Monte Carlo estimate with \\(\\phi(x)\\,f(x) = h(x)\\).\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html",
    "href": "lectures/L03-mc-error-1.html",
    "title": "3  Monte Carlo error I: theory",
    "section": "",
    "text": "\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n\nExample 3.1 Let’s go back to the very first example in the module, Example 1.1, where we were trying to find the expectation of an \\(\\operatorname{Exp}(2)\\) random variable. We used this R code:\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.4997326\n\n\n(Because Monte Carlo estimation is random, this won’t be the exact same estimate we had before, of course.)\nSo if we want to investigate the error, we can use the sample variance of these samples. We will use the sample variance function var() to calculate the sample variance.\n\nvar_est &lt;- var(samples)\nMSEest  &lt;- var_est / n\nRMSEest &lt;- sqrt(MSEest)\nc(var_est, MSEest, RMSEest)\n\n[1] 2.497426e-01 2.497426e-07 4.997425e-04\n\n\nThe first number is var_est \\(= 0.25\\), the sample variance of our \\(\\phi(x_i)\\)s:\n\\[ s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\big(\\phi(x_i) - \\widehat{\\theta}_n^{\\mathrm{MC}}\\big)^2 . \\] This should be a good estimate of the true variance \\(\\operatorname{Var}(\\phi(X))\\). In fact, in this simple case, we know that \\(\\operatorname{Var}(X) = \\frac{1}{2^2} = 0.25\\), so we know that the estimate was good. In calculating this in the code, we used R’s var() function, which calculates the sample variance of some values.\nThe second number is MSEest \\(= 2.5\\times 10^{-7}\\), our estimate of the mean-square error. Since \\(\\operatorname{MSE}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\frac{1}{n} \\operatorname{Var}(\\phi(X))\\), then \\(\\frac{1}{n} s^2\\) should be a good estimate of the MSE.\nThe third number is RMSEest \\(= 5\\times 10^{-4}\\) our estimate of the root-mean square error, which is simply the square-root of our estimate of the mean-square error.\n\n\n\nSummary:\n\nThe Monte Carlo estimator is unbiased.\nThe Monte Carlo estimator has mean-square error \\(\\Var(\\phi(X))/n\\), so the root-mean-square error scales like \\(1/\\sqrt{n}\\).\nThe mean-square error can be estimated by \\(S^2 / n\\), where \\(S^2\\) is the sample variance of \\(\\phi(X)\\).\n\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.2.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "problems/P1.html",
    "href": "problems/P1.html",
    "title": "Problem Sheet 1",
    "section": "",
    "text": "This is Problem Sheet 1, which covers material from Lectures 1 to 6. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Thursday 16 October. If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Thursdays at 1300.\nThis problem sheet is to help you practice material from the module and to help you check your learning. It is not for formal assessment and does not count towards your module mark.\nHowever, if, optionally, you would like some brief informal feedback on Questions 4, 6 and 8 (marked ★), I am happy to provide some. If you want some brief feedback, you should submit your work electronically through Gradescope via the module’s Minerva page by 1400 on Tuesday 14 October. I will return some brief comments on your those two questions by the problems class on Thursday 16 October. Because this informal feedback, not part of the official assessment, I cannot accept late work for any reason – but I am always happy to discuss any of your work on any question in my office hours.\nMany of these questions will require use of the R programming language (for example, by using the program RStudio).\nFull solutions should be released on Friday 17 October.\n\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.      Let \\(X\\) be uniform on \\([-1,2]\\).\n\n(a)   By hand, calculate the exact value of \\(\\Ex X^4\\).\n\nSolution.\n\\[\\int_{-1}^2 x^4\\,\\frac{1}{2-(-1)}\\,\\mathrm{d}x = \\tfrac13 \\Big[\\tfrac15x^5\\Big]_{-1}^2 = \\tfrac13\\Big(\\tfrac{32}{5}-\\big(-\\tfrac15\\big)\\Big) = \\tfrac{33}{15} = \\tfrac{11}{5} = 2.2\\]\n\n\n\n(b)   Using R, calculate a Monte Carlo estimate for \\(\\Ex X^4\\).\n\nSolution. I used the R code\n\nn &lt;- 1e6\nsamples &lt;- runif(n, -1, 2)\nmean(samples^4)\n\n[1] 2.199236\n\n\n\n\n\n\n2.      Let \\(X\\) and \\(Y\\) both be standard normal distributions. Compute a Monte Carlo estimate of \\(\\Exg \\max\\{X,Y\\}\\). (You may wish to investigate R’s pmax() function.)\n\nSolution. By looking at ?pmax (or maybe searching on Google) I discovered that pmax() gives the “parallel maxima” of two (or more vectors). That is the first element is the maximum of all the first elements of the vectors; the second element is the maximum of all the second elements of the vectors; and so on.\nSo I used the R code\n\nn &lt;- 1e6\nxsamples &lt;- rnorm(n)\nysamples &lt;- rnorm(n)\nmean(pmax(xsamples, ysamples))\n\n[1] 0.5645173\n\n\n\n\n\n3.      You are trapped alone on an island. All you have with you is a tin can (radius \\(r\\)) and a cardboard box (side lengths \\(2r \\times 2r\\)) that it fits snugly inside. You put the can inside the box [left picture].\nWhen it starts raining, each raindrop that falls in the cardboard box might fall into the tin can [middle picture], or might fall into the corners of the box outside the can [right picture].\n\n\n(a)   Using R, simulate rainfall into the box. You may take units such that \\(r = 1\\). Estimate the probability \\(\\theta\\) that a uniformly-random raindrop that falls in the cardboard box falls into the tin can also.\n\nSolution. I set things up so that the box is \\([-1, 1]^2\\), centered at the origin. This means that the inside of the can is the set of points is those \\((x,y)\\) such that \\(x^2 + y^2 \\leq 1\\).\n\nn &lt;- 1e6\nrain_x &lt;- runif(n, -1, 1)\nrain_y &lt;- runif(n, -1, 1)\nin_box &lt;- function(x, y) x^2 + y^2 &lt;= 1\nmean(in_box(rain_x, rain_y))\n\n[1] 0.785022\n\n\n\n\n\n(b)   Calculate exactly the probability \\(\\theta\\).\n\nSolution. The area of the box is \\(2r \\times 2r = 4r^2\\). The area of the can is \\(\\pi r^2\\). So the probability a raindrop landing in the box lands in the can is \\[ \\frac{\\text{area of can}}{\\text{area of box}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\approx 0.785. \\]\n\n\n\n(c)   You realise that, despite being trapped all alone on the island, you now have a way of approximating the value of \\(\\pi\\). If you want to calculate \\(\\pi\\) to 6 decimal places, roughly how many raindrops do you need to fall into the box?\n\nSolution. The phrase “to 6 decimal places” isn’t a precise mathematical one. I’m going to interpret this as getting the root-mean-square error below \\(10^{-6}\\). If you interpret it slightly differently that’s fine – for example, getting the width of a 95% confidence interval below \\(10^{-6}\\) could be another, slightly stricter, criterion.\nOne could work this out by hand. Since the variance of a Bernoulli random variable is \\(p(1-p)\\), the mean-square error of our estimator is \\[ \\frac{\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})}{n} \\approx \\frac{0.169}{n} . \\] So we need \\[n = \\frac{0.169}{(10^{-6})^2} \\approx 169 \\text{ billion} . \\]\nThat said, if we are trapped on our desert island, maybe we don’t know what \\(\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})\\) is. In that case we could do this using the can and the box. Our estimate of the variance is\n\nvar_est &lt;- var(in_box(rain_x, rain_y))\nvar_est / (1e-6)^2\n\n[1] 168762628281\n\n\nWe will probably spend a long time waiting for that much rain!\n\n\n\n\n4.     ★ (2024–25 exam, Question 1(d)) A statistician wants to estimate \\(\\mathbb E(\\cos X)\\), where \\(X \\sim \\operatorname{Exp(4)}\\) is an exponential distribution with rate 4. The statistician begins with a pilot study, as shown in the R session below:\n\n   &gt; n &lt;- 1e4\n&gt; samples &lt;- rexp(n, 4)\n&gt; var(cos(samples))\n[1] 0.01416\n\n            The statistician wants to get the root-mean-square error of her estimator down to \\(10^{−4}\\). Approximately how many samples will she need for the full study? Explain your answer.\n\n\n5.      Let \\(h(x) = 1/(x + 0.1)\\). We wish to estimate \\(\\int_0^5 h(x) \\, \\mathrm{d}x\\) using a Monte Carlo method.\n\n(a)   Estimate the integral using \\(X\\) uniform on \\([0,5]\\).\n\nSolution.\n\nn &lt;- 1e6\nintegrand &lt;- function(x) 1 / (x + 0.1)\nsamples1 &lt;- runif(n, 0, 5)\nmean(5 * integrand(samples1))\n\n[1] 3.928643\n\n\n(In fact, the true answer is \\(\\log(5.1) - \\log(0.1) = 3.932\\), so it looks like this is working correctly.)\n\n\n\n(b)   Can you come up with a choice of \\(X\\) that improves on the estimate from (a)?\n\nSolution. Let’s look at a graph of the integrand \\(h\\).\n\n\nCode for drawing this graph\ncurve(\n  integrand, n = 1001, from = 0, to = 5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(0,5)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nWe see that we get a much bigger contribution to the integral from values near 0. So a random variable that picks values nearer to 0 more often might have a chance of giving a more accurate result.\n\nI decided to try an exponential distribution with rate 1, which should sample the smaller values of \\(x\\) more often.\n\npdf &lt;- function(x) dexp(x, 1)\nphi &lt;- function(x) (integrand(x) / pdf(x)) * (x &lt;= 5)\n\nsamples2 &lt;- rexp(n, 1)\nmean(phi(samples2))\n\n[1] 3.932641\n\n\n(I had to include x &lt;= 5 in the expression for \\(\\phi\\), because my exponential distribution will sometimes take samples above 5, but they should count as 0 in an estimate for the integral between 0 and 5.)\nTo see whether or not this was an improvement, I estimated the mean-square error.\n\nvar(5 * integrand(samples1)) / n\n\n[1] 3.341696e-05\n\nvar(phi(samples2)) / n\n\n[1] 6.064075e-06\n\n\nI found that I had reduced the mean-square error by roughly a factor of 5. \n\n\n\n\n\n6.      ★ Show that the indicator functions \\(\\mathbb I_A(X)\\) and \\(\\mathbb I_B(X)\\) have correlation 0 if and only if the events \\(\\{X \\in A\\}\\) and \\(\\{X \\in B\\}\\) are independent.\n\nSolution. Recall that two random variables \\(U\\), \\(V\\) have correlation 0 if and only if their covariance \\(\\Cov(U,V) = \\Ex UV - (\\Ex U)(\\Ex V)\\) is 0 too.\nWe know that \\(\\Exg\\Ind_A(X) = \\mathbb P(X \\in A)\\) and \\(\\Exg \\Ind_B(Y) = \\mathbb P(X \\in B)\\). What about \\(\\Exg \\Ind_A(X) \\Ind_B(X)\\)? Well, \\(\\Ind_A(x) \\Ind_B(x)\\) is 1 if and only if both indicator functions equal 1, which is if and only if both \\(x \\in A\\) and \\(x \\in B\\). So \\(\\Exg \\Ind_A(X) \\Ind_B(X) = \\mathbb P(X \\in A \\text{ and } X \\in B)\\).\nSo the covariance is \\[ \\Cov \\big(\\Ind_A(X), \\Ind_B(X) \\big) = \\mathbb P(X \\in A \\text{ and } X \\in B) - \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B) . \\] If this is 0, then \\(\\mathbb P(X \\in A \\text{ and } X \\in B) = \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B)\\), which is precisely the definition of those two events being independent.\n\n\n\n7.      Let \\(X\\) be an exponential distribution with rate 1.\n\n(a)   Estimate \\(\\mathbb EX^{2.1}\\) using the standard Monte Carlo method.\n\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 1)\nmean(samples^2.1)\n\n[1] 2.19966\n\n\n\n\n\n(b)   Estimate \\(\\mathbb EX^{2.1}\\) using \\(X^2\\) as a control variate. (You may recall that if \\(Y\\) is exponential with rate \\(\\lambda\\) then \\(\\mathbb EY^2 = 2/\\lambda^2\\).)\n\nSolution. We have \\(\\Ex X^2 = 2\\). So, re-using the same sample as before (you don’t have to do this – you could take new samples), our R code is as follows.\n\nmean(samples^2.1 - samples^2) + 2\n\n[1] 2.198221\n\n\n\n\n\n(c)   Which method is better?\n\nSolution. The better answer is the one with the smaller mean-square error.\nFor the basic method,\n\nvar(samples^2.1) / n\n\n[1] 2.807748e-05\n\n\nFor the control variate method,\n\nvar(samples^2.1 - samples^2) / n\n\n[1] 6.881275e-07\n\n\nSo the control variates method is much, much better.\n\n\n\n\n8.      ★ Let \\(Z\\) be a standard normal distribution. A statistician has been investigating Monte Carlo estimates of \\(\\mathbb EZ^k\\) for different positive integers values of \\(k\\). Her colleague suggests using \\(Z' = -Z\\) as an antithetic variable. Without running any R code, explain whether or not this is a good idea (a) when \\(k\\) is even; (b) when \\(k\\) is odd.\n\nSolution.\n(a) When \\(k\\) is even, we have \\(Z^k = (-Z)^k\\). So the antithetic variables method just repeats each sample twice. This is obviously no benefit at all, and just wastes time. Indeed, we have perfect positive correlation \\(\\rho = +1\\), which is the “worst-case scenario”.\n(b) When \\(k\\) is odd, we have \\(Z^k = -(-Z)^k\\). In this case we know that \\(\\Ex Z^k = 0\\), because the results for positive \\(z\\) exactly balance out those for negative \\(z\\), so no Monte Carlo sampling is necessary. If our statistician has somehow forgotten that, though, she will get a pleasant surprise! After just two samples, she will get the estimate \\[\\frac{1}{2} \\big(Z_1^k + (-Z_1)^k \\big) = \\frac{1}{2} (Z_1^k - Z_1^k) = 0 ,\\] Thereby getting the result exactly right. Indeed, we have perfect negative correlation \\(\\rho = -1\\), which is the “best-case scenario”.",
    "crumbs": [
      "Monte Carlo estimation",
      "Problem Sheet 1"
    ]
  }
]