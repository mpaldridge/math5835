[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH5835M Statistical Computing",
    "section": "",
    "text": "About MATH5835",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#organisation-of-math5835",
    "href": "index.html#organisation-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Organisation of MATH5835",
    "text": "Organisation of MATH5835\nThis module is MATH5835M Statistical Computing.\nThis module lasts for 11 weeks from 30 September to 13 December 2024. The exam will take place between 13 and 24 January 2025.\nThe module leader, the lecturer, and the main author of these notes is Dr Matthew Aldridge. (You can call me “Matt”, “Matthew”, or “Dr Aldridge”, pronounced “old-ridge”.) My email address is m.aldridge@leeds.ac.uk, although I much prefer questions in person at office hours (see below) rather than by email.\n\nLectures\nThe main way you will learn new material for this module is by attending lectures. There are three lectures per week:\n\nMondays at 1400 in Roger Stevens LT 13\nThursdays at 1200 in Roger Stevens LT 03\nFridays at 1000 in Rogers Stevens LT 09\n\nI recommend taking your own notes during the lecture. I will put brief summary notes from the lectures on this website, but they will not reflect all the details I say out loud and write on the whiteboard. Lectures will go through material quite quickly and the material may be quite difficult, so it’s likely you’ll want to spend time reading through your notes after the lecture. Lectures should be recorded on the lecture capture system; I find it very difficult to read the whiteboard in these videos, but if you unavoidably miss a lecture, for example due to illness, you may find they are better than nothing.\nIn Weeks 3, 5, 7, 9 and 11, the Thursday lecture will operate as a “problems class” – see more on this below.\nAttendance at lectures in compulsory.\n\n\nProblem sheets and problem classes\nMathematics and statistics are “doing” subjects! To help you learn material for the module and to help you prepare for the exam, I will provide 5 unassessed problem sheets. These are for you to work through in your own time to help you learn; they are not formally assessed and will not be marked by me (or anyone else). You are welcome to discuss work on the problem sheets with colleagues and friends, although my recommendation would be to write-up your “last, best” attempt neatly by yourself.\nYou should work through each problem sheet in preparation for the problems class in the Thursday lecture of Week 3, 5, 7, 9 and 11. In the problems class, you should be ready to discuss your answers to questions you managed to solve, explain your progress on questions you partially solved, and ask for help on questions you got stuck on. You can also ask for extra help or feedback at office hours (see below).\n\n\nCoursework\nThere will be one piece of assessed coursework, which will make up 20% of your module mark. You can read more about the coursework here.\nThe coursework will be in the form of a worksheet. The worksheet will have some questions, mostly computational but also mathematical, and you will have to write a report containing your answers and computations.\nThe assessed coursework will be introduced in the computer practical sessions in Week 9.\nThe deadline for the coursework will be the penultimate day of the Autumn term, Thursday 12 December  at 1400. Feedback and marks will be returned on Monday 13 January, the first day of the Spring term.\n\n\nOffice hours\nI will run a weekly office hours drop-in session for feedback and consultation. You can come along if you want to talk to me about anything on the course, including if you’d like some feedback on your attempts at problem sheet questions. (For extremely short queries, you can approach me before or after lectures, but my response will often be: “Come to my office hours, and we can discuss it there!”)\nOffice hours will happen on Mondays from 1500 to 1600 – so directly after the Monday lecture – in my office, which is EC Stoner 9.10n on the 9th floor of the EC Stoner building. (One way my office is via the doors directly opposite the main entrance to the School of Mathematics. You can also get there from Staircase 1 on the Level 10 “red route” through EC Stoner, next to the Maths Satellite.) If you cannot make this time, contact me for an alternative arrangement.\n\n\nExam\nThere will be one exam, which will make up 80% of your module mark.\nThe exam will be in the January 2025 exam period (13–24 January); the date and time will be announced in December. The exam will be in person and on campus.\nThe exam will last 2 hours and 30 minutes. The exam will consist of 4 questions, all compulsory. You will be allowed to use a basic non-programmable calculator in the exam.",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#content-of-math5835",
    "href": "index.html#content-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Content of MATH5835",
    "text": "Content of MATH5835\n\nNecessary background\nIt is recommended that students should have completed at least two undergraduate level courses in probability and statistics, or something equivalent, although confidence and proficiency in basic material is more important than very deep knowledge. For Leeds undergraduates, the official prerequisite is MATH2715 Statistical Methods, although confidence and proficiency in the material of MATH1710 & MATH1712 Probability and Statistics 1 & 2 is probably more important.\nSome knowledge I will assume:\n\nProbability: Basic rules of probability; random variables, both continuous and discrete; “famous” distributions (especially the normal distribution and the continuous uniform distribution); expectation, variance, covariance, correlation; law of large numbers and central limit theorem.\nStatistics: Estimation of parameters; bias and error; sample mean and sample variance\n\nThis module will also include an material on Markov chains. I won’t assume any pre-existing knowledge of this, and I will introduce all new material we need, but students who have studied Markov chains before (for example in the Leeds module MATH2750 Introduction to Markov Processes) may find a couple of lectures here are merely a reminder of things they already know.\nThe lectures will include examples using the R program language. The coursework and problem sheets will require use of R. The exam, while just a “pencil and paper” exam, will require understanding and writing short portions of R code. We will assume basic R capability – that you can enter R commands, store R objects using the &lt;- assignment, and perform basic arithmetic with numbers and vectors. Other concepts will be introduced as necessary. If you want to use R on your own device, I recommend downloading (if you have not already) the R programming language and the program RStudio. (These lecture notes were written in R using RStudio.)\n\n\nSyllabus\nWe plan to cover the following topics in the module:\n\nMonte Carlo estimation: definition and examples; bias and error; variance reduction techniques: control variates, antithetic variables, importance sampling. [9 lectures]\nRandom number generation: pseudo-random number generation using linear congruential generators; inverse transform method; rejection sampling [7 lectures]\nMarkov chain Monte Carlo (MCMC): [7 lectures]\n\nIntroduction to Markov chains in discrete and continuous space\nMetropolis–Hastings algorithm: definition; examples; MCMC in practice; MCMC for Bayesian statistics\n\nBootstrap: Empirical distribution; definition of the bootstrap; bootstrap error; bootstrap confidence intervals [4 lectures]\nFrequently-asked questions [1 lecture]\n\nTogether with the 5 problems classes, this makes 33 lectures.\n\n\nBook\nThe following book is strongly recommended for the module:\n\nJ Voss, An Introduction to Statistical Computing: A simulation-based approach, Wiley Series in Computational Statistics, Wiley, 2014\n\nThe library has electronic access to this book (and two paper copies).\nDr Voss is a lecturer in the School of Mathematics and the University of Leeds, and has taught MATH5835 many times. An Introduction to Statistical Computing grew out of his lecture notes for this module, so the book is ideally suited for this module. My lectures will follow this book closely – specifically:\n\nMonte Carlo estimation: Sections 3.1–3.3\nRandom number generation: Sections 1.1–1.4\nMarkov chain Monte Carlo: Section 2.3 and Sections 4.1–4.3\nBootstrap: Section 5.2\n\nFor a second look at material, for preparatory reading, for optional extended reading, or for extra exercises, this book comes with my highest recommendation!",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html",
    "href": "lectures/L01-mc-intro.html",
    "title": "1  Introduction to Monte Carlo",
    "section": "",
    "text": "1.1 What is statistical computing?\n“Statistical computing” – or “computational statistics” – refers to the branch of statistics that involves not attacking statistical problems merely with a pencil and paper, but rather by combining human ingenuity with the immense calculating powers of computers.\nOne of the big ideas here is simulation. Simulation is the idea that we can understand the properties of a random model not by cleverly working out the properties using theory – this is usually impossible for anything but the simplest “toy models” – but rather by running the model many times on a computer. From these many simulations, we can observe and measure things like the typical (or “expected”) behaviour, the spread (or “variance”) of the behaviour, and other things. This concept of simulation is at the heart of the module MATH5835M Statistical Computing.\nIn particular, we will look at Monte Carlo estimation. Monte Carlo is about estimating a parameter, expectation or probability related to a random variable by taking many samples of that random variable, then computing a relevant sample mean from those samples. We will study Monte Carlo in its standard “basic” form (Lectures 1–9), but also in the modern Markov chain Monte Carlo form (Lectures 17–23), which has become such a crucial part of Bayesian statistical analysis.\nTo run a simulation, one needs random numbers with the correct distributions. Random number generation (Lectures 10–16) will be an important part of this module. We will look first at how to generate randomness of any sort, and then how to get that randomness into the shape of the distributions we want.\nWhen dealing with a very big data set, traditionally we want to “reduce the dimension” by representing it with a simple parametric model. For example, tens of thousands of datapoints might get reduced just to estimates of the parameters \\(\\mu\\) and \\(\\sigma^2\\) of a normal distribution. But with computational statistics, we don’t need to make such a simplification – we can do inference using the full details of the whole dataset itself, without making extra assumptions. An computational scheme that takes advantage of this idea is the bootstrap (Lectures 24–27).\nMATH5835M Statistical Computing is a mathematics module that will concentrate on the mathematical ideas that underpin statistical computing. It is not a programming module that will go deeply into the practical issues of the most efficient possible coding of the algorithms we study. But we will want to investigate the behaviour of the methods we learn about and to explore their properties, so will be computer programming to help us do that. (We will be using the statistical programming language R, although one could just as easily have used Python or other similar languages.) As my PhD supervisor once told me: “You don’t really understand a mathematical algorithm until you’ve coded it up yourself.”",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html#what-is-monte-carlo-estimation",
    "href": "lectures/L01-mc-intro.html#what-is-monte-carlo-estimation",
    "title": "1  Introduction to Monte Carlo",
    "section": "1.2 What is Monte Carlo estimation?",
    "text": "1.2 What is Monte Carlo estimation?\nLet \\(X\\) be a random variable. We recall the expectation \\(\\Ex X\\) of \\(X\\): if \\(X\\) is discrete with probability mass function (PMF) \\(p\\), then this is \\[ \\Ex X = \\sum_x x\\,p(x) ;\\] while if \\(X\\) is continuous with probability density function (PDF) \\(f\\), then this is \\[ \\Ex X = \\int_{-\\infty}^{+\\infty} x\\,f(x)\\,\\mathrm{d}x . \\] More generally, the expectation of a function \\(\\phi\\) of \\(X\\) is \\[ \\Exg \\phi(X) = \\begin{cases} {\\displaystyle \\sum_x \\phi(x)\\,p(x)} & \\text{for $X$ discrete}\\\\ {\\displaystyle \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x}  & \\text{for $X$ continuous.} \\end{cases}\\] (This matches with the “plain” expectation when \\(\\phi(x) = x\\).)\nBut how do we actually calculate an expectation like one of these? If \\(X\\) is discrete and can only take a small, finite number of values, we can simply add up the sum \\(\\sum_x \\phi(x)\\,p(x)\\). Otherwise, we just have to hope that \\(\\phi\\) and \\(p\\) or \\(f\\) are sufficiently “nice” that we can manage to work out the sum/integral using a pencil and paper. But while this is often the case in the sort of “toy example” one comes across in maths or statistics lectures, this is very rare in “real life”.\nMonte Carlo estimation is the idea that we can get an approximate answer for \\(\\Ex X\\) or \\(\\Exg \\phi(X)\\) if we have access to lots of samples from \\(X\\). For example, if we have access to \\(X_1, X_2 \\dots, X_n\\) , independent and identically distributed (IID) samples with the same distribution as \\(X\\), then we already know that the mean \\[ \\overline X = \\frac{1}{n}(X_1 + X_2 + \\cdots + X_n) = \\frac{1}{n} \\sum_{i=1}^n X_i \\] is usually close to the expectation \\(\\Ex X\\), at least if \\(n\\) is big. Similarly, it should be the case that \\[ \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] should be close to \\(\\Exg \\phi(X)\\).\nIn this module we will write that \\(X_1, X_2, \\dots, X_n\\) is a “random sample from \\(X\\)” to mean that \\(X_1, X_2, \\dots, X_n\\) are IID with the same distribution as \\(X\\).\n\nDefinition 1.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and write \\(\\theta = \\Exg\\phi(X)\\). Then the Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\).\n\nWhile general ideas for estimating using simulation go back a long time, the modern theory of Monte Carlo estimation was developed by the physicists Stanislaw Ulam and John von Neumann. Ulam (who was Polish) and von Neumann (who was Hungarian) moved to the US in the early 1940s to work on the Manhattan project to build the atomic bomb (as made famous by the film Oppenheimer). Later in the 1940s, they worked together in the Los Alamos National Laboratory continuing their research on nuclear weapons, where they used simulations on early computers to help them numerically solve difficult mathematical and physical problems.\nThe name “Monte Carlo” was chosen because the use of randomness to solve such problems reminded them of gamblers in the casinos of Monte Carlo, Monaco. Ulam and von Neumann also worked closely with another colleague Nicholas Metropolis, whose work we will study later in this module.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html#examples",
    "href": "lectures/L01-mc-intro.html#examples",
    "title": "1  Introduction to Monte Carlo",
    "section": "1.3 Examples",
    "text": "1.3 Examples\nLet’s see some simple examples of Monte Carlo estimation using R.\n\nExample 1.1 Let’s suppose we’ve forgotten the expectation of the exponential distribution \\(X \\sim \\operatorname{Exp}(2)\\) with rate 2. In this simple case, we could work out the answer using the PDF \\(f(x) = 2\\mathrm{e}^{-2x}\\) as\n\\[ \\mathbb E X = \\int_0^\\infty x\\,2\\mathrm{e}^{-2x}\\,\\mathrm{d}x \\](and, without too much difficulty, get the answer \\(\\frac12\\)). But instead, let’s do this the Monte Carlo way.\nIn R, we can use the rexp() function to get IID samples from the exponential distribution: the full syntax is rexp(n, rate), which gives n samples from an exponential distribution with rate rate. So our code here should be\n\nn &lt;- 100\nsamples &lt;- rexp(n, 2)\nMCest &lt;- (1 / n) * sum(samples)\nMCest\n\n[1] 0.3978279\n\n\nSo our Monte Carlo estimate is 0.39783, to 5 decimal places.\nTo get a (hopefully) more accurate estimation, we can use more samples. We could also simplify the third line of this code by using the mean() function.\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.5001547\n\n\n(In the second line, 1e6 is R code for the scientific notation \\(1 \\times 10^6\\), or a million. I just picked this as “a big number, but where my code still only took a few seconds to run.”)\nOur new Monte Carlo estimate is 0.50015, which is much closer to the true value of \\(\\frac12\\).\n\nBy the way: all R code “chunks” displayed in the notes should work perfectly if you copy-and-paste them into RStudio. (Indeed, these lecture notes should not appear unless the code runs correctly without errors.) I strongly encourage playing about with the code as a good way to learn this material and explore further!\n\nExample 1.2 Let’s try another example. Let \\(X \\sim \\operatorname{N}(1, 2^2)\\) be a normal distribution with mean 0 and standard deviation 2. Suppose we want to find out \\(\\mathbb E(\\sin X)\\) (for some reason). While it might be possible to somehow calculate the integral \\[ \\mathbb E(\\sin X) = \\int_{-\\infty}^{+\\infty} (\\sin x) \\, \\frac{1}{\\sqrt{2\\pi\\times 2^2}} \\exp\\left(\\frac{(x - 1)^2}{2\\times 2^2}\\right) \\, \\mathrm{d} x , \\] that looks extremely difficult to me.\nInstead, a Monte Carlo estimation of \\(\\Exg(\\sin X)\\) is very straightforward. (Although we must remember that when using the rnorm() function to generate normally distributed variates, the third argument is the standard deviation, here \\(2\\), not the variance, here \\(2^2 = 4\\).)\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n, 1, 2)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.1140989\n\n\nOur Monte Carlo estimate is 0.11410.\n\nNext time: We look at more examples of things we can estimate using the Monte Carlo method.\n\nSummary:\n\nStatistical computing is about solving statistical problems by combining human ingenuity with computing power.\nThe Monte Carlo estimate of \\(\\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, \\dots, X_n\\) are IID random samples from \\(X\\).\nMonte Carlo estimation typically gets more accurate as the number of samples \\(n\\) gets bigger.\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html",
    "href": "lectures/L02-mc-uses.html",
    "title": "2  Uses of Monte Carlo",
    "section": "",
    "text": "2.1 Monte Carlo for probabilities\nQuick recap: Last time we defined the Monte Carlo estimator for an expectation of a function of a random variable \\(\\theta = \\Exg \\phi(X)\\) to be \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are independent random samples from \\(X\\).\nBut what if we want to find a probability, rather than an expectation? What if we want \\(\\mathbb P(X = x)\\) for some \\(x\\), or \\(\\mathbb P(X \\geq a)\\) for some \\(a\\), or, more generally, \\(\\mathbb P(X \\in A)\\) for some set \\(A\\)?\nThe key thing that will help us here is the indicator function. The indicator function simply tells us whether an outcome \\(x\\) is in a set \\(A\\) or not.\nThe set \\(A\\) could just be a single element \\(A = \\{y\\}\\). In that case \\(\\Ind_A(x)\\) is 1 if \\(x = y\\) and 0 if \\(x \\neq y\\). Or \\(A\\) could be a semi-infinite interval, like \\(A = [a, \\infty)\\). In that case \\(\\Ind_A(x)\\) is 1 if \\(x \\geq a\\) and 0 if \\(x &lt; a\\).\nWhy is this helpful? Well \\(\\Ind_A\\) is a function, so let’s think about what the expectation \\(\\Exg \\Ind_A(X)\\) would be for some random variable \\(X\\). Since \\(\\Ind_A\\) can only take two values, 0 and 1, we have \\[ \\begin{align*}\n\\Exg \\Ind_A(X) &= \\sum_{y \\in\\{0,1\\}} y\\,\\mathbb P\\big( \\Ind_A(X) = y \\big) \\\\\\\n  &= 0 \\times \\mathbb P\\big( \\Ind_A(X) = 0 \\big) + 1 \\times \\mathbb P\\big( \\Ind_A(X) = 1 \\big) \\\\\n  &= 0 \\times \\mathbb P(X \\notin A) + 1 \\times \\mathbb P(X \\in A) \\\\\n  &= \\mathbb P(X \\in A) .\n\\end{align*} \\] In line three, we used that \\(\\Ind_A(X) = 0\\) if and only if \\(X \\notin A\\), and that \\(\\Ind_A(X) = 1\\) if and only if \\(X \\in A\\).\nSo the expectation of an indicator function a set is the probability that \\(X\\) is in that set. This idea connects “expectations of functions” back to probabilities: if we want to find \\(\\mathbb P(X \\in A)\\) we can find the expectation of \\(\\Ind_A(X)\\).\nWith this idea in hand, how do we estimate \\(\\theta = \\mathbb P(X \\in A)\\) using the Monte Carlo method? We write \\(\\theta = \\Exg\\Ind_A(X)\\). Then our Monte Carlo estimator is \\[  \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\Ind_A(X_i) . \\]\nWe remember that \\(\\Ind_A(X_i)\\) is 1 if \\(X_i \\in A\\) and 0 otherwise. So if we add up \\(n\\) of these, we count an extra +1 each time we have an \\(X_i \\in A\\). So \\(\\sum_{i=1}^n \\Ind_A(X_i)\\) counts the total number of the \\(X_i\\) that are in \\(A\\). So the Monte Carlo estimator can be written as \\[  \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{\\# \\text{ of } X_i \\text{ that are in $A$}}{n} . \\]\nAlthough we’ve had to do a bit of work to get here, this a totally logical outcome! The right-hand side here is the proportion of the samples for which \\(X_i \\in A\\). And if we want to estimate the probability something happens, looking at the proportion of times it happens in a random sample is very much the “intuitive” estimate to take. And that intuitive estimate is indeed the Monte Carlo estimate!\nWe should explain the third line in the code we used for the Monte Carlo estimation mean(samples &gt; 2). In R, some statements can be answered “true” or “false”: these are often statements involving equality == (that’s a double equals sign) or inequalities like &lt;, &lt;=, &gt;=, &gt;, for example. So 5 &gt; 2 is TRUE but 3 == 7 is FALSE. These can be applied “component by component” to vectors. So, for example, testing which numbers from 1 to 10 are greater than or equal to 7, we get\n1:10 &gt;= 7\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\nsix FALSEs (for 1 to 6) followed by four TRUEs (for 7 to 10).\nWe can also use & (“and”) and | (“or”) in true/false statements like these.\nBut R also knows to treat TRUE like the number 1 and FALSE like the number 0. So if we add up some TRUEs and FALSEs, R simply counts how many TRUEs there are\nsum(1:10 &gt;= 7)\n\n[1] 4\nSo in our Monte Carlo estimation code, samples &gt; 2 was a vector of TRUEs and FALSEs, depending on whether each sample was greater than 2 or not, then mean(samples &gt; 2) took the proportion of the samples that were greater than 2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html#monte-carlo-for-probabilities",
    "href": "lectures/L02-mc-uses.html#monte-carlo-for-probabilities",
    "title": "2  Uses of Monte Carlo",
    "section": "",
    "text": "Definition 2.1 Let \\(A\\) be a set. Then the indicator function \\(\\Ind_A\\) is defined by \\[ \\Ind_A(x) = \\begin{cases} 1 & \\text{if $x \\in A$} \\\\ 0 & \\text{if $x \\notin A$.} \\end{cases} \\]\n\n\n\n\n\n\n\n\nExample 2.1 Let \\(Z \\sim \\operatorname{N}(0,1)\\) be a standard normal distribution. Estimate \\(\\mathbb P(Z &gt; 2)\\).\nThis is a question that it is impossible to answer exactly using a pencil and paper: there’s no closed form for \\[ \\mathbb P(Z &gt; 2) = \\int_2^\\infty \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-z^2/2}\\,\\mathrm{d}z , \\] so we’ll have to use an estimation method.\nThe Monte Carlo estimate means taking a random sample \\(Z_1, Z_2, \\dots, Z_n\\) of standard normals, and calculating what proportion of them are greater than 2. In R, we can do this as follows.\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.0229\n\n\nIn the second line, we could have written rnorm(n, 0, 1). But, if you don’t give the parameters mean and sd to the function rnorm(), R just assumes you want the standard normal with mean = 0 and sd = 1.\nWe can check our answer: R’s inbuilt pnorm() function estimates probabilities for the normal distribution (using a method that, in this specific case, is much quicker and more accurate than Monte Carlo estimation). The true answer is very close to\n\npnorm(2, lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nso our estimate was pretty good.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html#monte-carlo-for-integrals",
    "href": "lectures/L02-mc-uses.html#monte-carlo-for-integrals",
    "title": "2  Uses of Monte Carlo",
    "section": "2.2 Monte Carlo for integrals",
    "text": "2.2 Monte Carlo for integrals\nThere’s another thing – a non-statistics thing – that Monte Carlo estimation is useful for. We can use Monte Carlo estimation to approximate integrals that are too hard to do by hand.\nThis might seem surprising. Estimating the expectation of (a function of) a random variable seems a naturally statistical thing to do. But an integral is just a straight maths problem – there’s not any randomness at all. But actually, integrals and expectations are very similar things.\nLet’s think of an integral: say, \\[ \\int_a^b h(x) \\,\\mathrm{d}x ,\\] the integral of some function \\(h\\) (the “integrand”) between the limits \\(a\\) and \\(b\\). Now let’s compare that to the integral \\(\\Exg \\phi(X)\\) of a continuous random variable that we can estimate using Monte Carlo estimation, \\[ \\Exg \\phi(X) = \\int_{-\\infty}^\\infty \\phi(x)\\,f(x)\\, \\mathrm{d} x. \\] Matching things up, we can see that we if we were to a function \\(\\phi\\) and a PDF \\(f\\) such that \\[ \\phi(x)\\,f(x) = \\begin{cases} 0 & x &lt; a \\\\ h(x) & a \\leq x \\leq b \\\\ 0 & x &gt; b , \\end{cases}  \\tag{2.1}\\] then we would have \\[ \\Exg \\phi(X) = \\int_{-\\infty}^\\infty \\phi(x)\\,f(x)\\, \\mathrm{d} x = \\int_a^b h(x) \\,\\mathrm{d}x, \\] so the value of the expectation would be precisely the value of the integral we’re after. Then we could use Monte Carlo to estimate that expectation/integral.\nThere are lots of choices of \\(\\phi\\) and \\(f\\) that would satisfy this the condition in Equation 2.1. But a “common-sense” choice that often works is to pick \\(f\\) to be the PDF of \\(X\\), a continuous uniform distribution on the interval \\([a,b]\\). (This certainly works when \\(a\\) and \\(b\\) are finite, anyway.) Recall that the continuous uniform distribution means that \\(X\\) has PDF \\[ f(x) = \\begin{cases} 0 & x &lt; a \\\\ \\displaystyle{\\frac{1}{b-a}} & a \\leq x \\leq b \\\\ 0 & x &gt; b . \\end{cases} \\] Comparing this equation with Equation 2.1, we then have to choose \\[\\phi(x) = \\frac{h(x)}{f(x)} = (b-a)h(x).\\]\nPutting this all together, we have \\[ \\Exg \\phi(X) = \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\int_a^b (b-a)h(x)\\,\\frac{1}{b-a}\\,\\mathrm{d}x = \\int_a^b h(x) \\,\\mathrm{d}x ,\\] as required. This can then be estimated using the Monte Carlo method.\n\nDefinition 2.2 Consider an integral \\(\\theta = \\int_a^b h(x)\\,\\mathrm{d}x\\). Let \\(f\\) be the probability density function of a random variable \\(X\\) and let \\(\\phi\\) be function such that Equation 2.1 holds. Then the Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) of the integral \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\).\n\n\nExample 2.2 Suppose we want to approximate the integral \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x . \\]\nSince this is an integral on the finite interval \\([0,2]\\), it would seem to make sense to pick \\(X\\) to be uniform on \\([0,2]\\). This means we should take \\[\\phi(x) = \\frac{h(x)}{f(x)} = (2-0)h(x) = 2\\,x^{1.6}(2-x)^{0.7}.\\] We can then approximate this integral in R using the Monte Carlo estimator \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x = \\operatorname{\\mathbb{E}} \\phi(X) \\approx \\frac{1}{n} \\sum_{i=1}^n 2\\,X_i^{1.6} (2-X_i)^{0.7} . \\]\n\nn &lt;- 1e6\nintegrand &lt;- function(x) x^1.6 * (2 - x)^0.7\na &lt;- 0\nb &lt;- 2\nsamples &lt;- runif(n, a, b)\nmean((b - a) * integrand(samples))\n\n[1] 1.445453\n\n\nYou have perhaps noticed that, here and elsewhere, I tend to split my R code up into lots of small bits, perhaps slightly unnecessarily. After all, those 6 lines of code could simply have been written as just 2 lines\n\nsamples &lt;- runif(1e6, 0, 2)\nmean(2 * samples^1.6 * (2 - samples)^0.7)\n\nThere’s nothing wrong with that. However, I find that code is easier to read if divided into small pieces. It also makes it easier to tinker with, if I want to use it to solve some similar but slightly different problem.\n\n\nExample 2.3 Suppose we want to approximate the integral \\[ \\int_{-\\infty}^{+\\infty}\n\\mathrm{e}^{-0.1|x|} \\cos x \\, \\mathrm{d}x . \\] This one is an integral on the whole real line, so we can’t take a uniform distribution. Maybe we should take \\(f(x)\\) to be the PDF of a normal distribution, and then put \\[ \\phi(x) = \\frac{h(x)}{f(x)} = \\frac{\\mathrm{e}^{-0.1|x|} \\cos x}{f(x)} . \\]\nBut which normal distribution should we take? Well, we’re allowed to take any one – we will still get an accurate estimate in the limit as \\(n \\to \\infty\\). But we’d like an estimator that gives accurate results at moderate-sized \\(n\\), and picking a “good” distribution for \\(X\\) will help that.\nWe’ll probably get the best results if we pick a distribution that is likely to mostly take values where \\(h(x)\\) is big – or, rather, where the absolute value \\(|h(x)|\\) is big, to be precise. That is because we don’t want to “waste” too many samples where \\(h(x)\\) is very small, because they don’t contribute much to the integral. But we don’t want to “miss” – or only sample very rarely – places where \\(h(x)\\) is big, which contribute a lot to the integral.\nLet’s have a look at the graph of \\(h(x) = \\mathrm{e}^{-0.1|x|} \\cos x\\).\n\n\nCode for drawing this graph\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\n\ncurve(\n  integrand, n = 1001, from = -55, to = 55,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(-50,50)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nThis suggests to me that a mean of 0 and a standard deviation of 20 might work quite well, since this will tend to take values in \\([-40,40]\\) or so.\nWe will use R’s function dnorm() for the probability density function of the normal distribution (which saves us from having to remember what that is).\n\nn &lt;- 1e6\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\npdf       &lt;- function(x) dnorm(x, 0, 20)\nphi       &lt;- function(x) integrand(x) / pdf(x)\n\nsamples &lt;- rnorm(n, 0, 20)\nmean(phi(samples))\n\n[1] 0.2151045\n\n\n\nNext time: We will analyse the accuracy of these Monte Carlo estimates.\n\nSummary:\n\nThe indicator \\(\\Ind_A(x)\\) function of a set \\(A\\) is 1 if \\(x \\in A\\) or 0 if \\(x \\notin A\\).\nWe can estimate a probability \\(\\mathbb P(X \\in A)\\) by using the Monte Carlo estimate for \\(\\Exg\\Ind_A(X)\\).\nWe can estimate an integral \\(\\int h(x) \\, \\mathrm{d}x\\) by using a Monte Carlo estimate with \\(\\phi(x)\\,f(x) = h(x)\\).\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html",
    "href": "lectures/L03-mc-error-1.html",
    "title": "3  Monte Carlo error I: theory",
    "section": "",
    "text": "3.1 Estimation error\nToday we are going to analysing the accuracy of Monte Carlo estimation. But before talking about Monte Carlo estimation specifically, let’s first remind ourselves of some concepts about error in statistical estimation more generally. We will use the following definitions.\nUsually, the main goal of estimation is to get the mean-square error of an estimate as small as possible. This is because the MSE measures by what distance we are missing on average. It can be more convenient to discuss the root-mean-square error, as that has the same units as the parameter being measured. (If \\(\\theta\\) and \\(\\widehat{\\theta}\\) are in metres, say, then the MSE is in metres-squared, whereas the RMSE error is in metres again.)\nIt’s nice to have an unbiased estimator – that is, one with bias 0. This is because bias measures any systematic error in a particular direction. However, unbiasedness by itself is not enough for an estimate to be good – we need low variance too. (Remember the old joke about the statistician who misses his first shot ten yards to the left, misses his second shot ten yards to the right, then claims to have “hit the target on average.”)\n(Remember also that “bias” is simply the word statisticians use for \\(\\mathbb E(\\widehat\\theta - \\theta)\\); we don’t mean “bias” in the derogatory way it is sometimes used in political arguments, for example.)\nYou probably also remember the relationship between the mean-square error, the bias, and the variance:\n(There’s a proof in Voss, An Introduction to Statistical Computing, Proposition 3.14, if you’ve forgotten.)\nSince the bias contributes to the mean-square error, that’s another reason to like estimator with low – or preferably zero – bias. (That said, there are some situations where there’s a “bias–variance tradeoff”, where allowing some bias reduces the variance and so can reduce the MSE. It turns out that Monte Carlo is not one of these cases, however.)",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#estimation-error",
    "href": "lectures/L03-mc-error-1.html#estimation-error",
    "title": "3  Monte Carlo error I: theory",
    "section": "",
    "text": "Definition 3.1 Let \\(\\widehat\\theta\\) be an estimator of a parameter \\(\\theta\\). Then we have the following definitions of the estimate \\(\\widehat\\theta\\):\n\nThe bias is \\(\\operatorname{bias}\\big(\\widehat\\theta\\big) = \\mathbb E\\big(\\widehat\\theta - \\theta\\big)  = \\mathbb E\\widehat\\theta - \\theta\\).\nThe mean-square error is \\(\\operatorname{MSE}\\big(\\widehat\\theta\\big) = \\mathbb E \\big(\\widehat\\theta - \\theta\\big)^2\\).\nThe root-mean-square error is the square-root of the mean-square error, \\[\\operatorname{RMSE}\\big(\\widehat\\theta\\big) = \\sqrt{\\operatorname{MSE}(\\widehat\\theta)} = \\sqrt{\\mathbb E (\\widehat\\theta - \\theta)^2} . \\]\n\n\n\n\n\n\n\nTheorem 3.1   \\(\\operatorname{MSE}\\big(\\widehat\\theta\\big) = \\operatorname{bias}\\big(\\widehat\\theta\\big)^2 + \\operatorname{Var}\\big(\\widehat\\theta\\big)\\).",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#bias-and-error-of-the-monte-carlo-estimator",
    "href": "lectures/L03-mc-error-1.html#bias-and-error-of-the-monte-carlo-estimator",
    "title": "3  Monte Carlo error I: theory",
    "section": "3.2 Bias and error of the Monte Carlo estimator",
    "text": "3.2 Bias and error of the Monte Carlo estimator\nIn this lecture, we’re going to be looking more carefully at the size of the errors made by the Monte Carlo estimator \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) . \\]\nOur main result is the following.\n\nTheorem 3.2 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and \\(\\theta = \\Exg\\phi(X)\\). Let \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] be the Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\({\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\[{\\displaystyle \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} = \\frac{1}{\\sqrt{n}} \\operatorname{sd}\\big(\\phi(X)\\big)}. \\]\n\n\nBefore we get to the proof, let’s recap some relevant probability.\nLet \\(Y_1, Y_2, \\dots\\) be IID random variables with common expectation \\(\\mathbb EY_1 = \\mu\\) and common variance \\(\\operatorname{Var}(Y_1) = \\sigma^2\\). Consider the mean of the first \\(n\\) random variables, \\[ \\overline{Y}_n = \\frac{1}{n} \\sum_{i=1}^n Y_i . \\] Then the expectation of \\(\\overline{Y}_n\\) is \\[ \\mathbb E \\overline{Y}_n = \\mathbb E\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\frac{1}{n}\n\\sum_{i=1}^n \\mathbb{E}Y_i = \\frac{1}{n}\\,n\\mu = \\mu . \\] The variance of \\(\\overline{Y}_n\\) is \\[ \\operatorname{Var}\\big(  \\overline{Y}_n \\big)= \\operatorname{Var} \\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\bigg(\\frac{1}{n}\\bigg)^2\n\\sum_{i=1}^n \\operatorname{Var}(Y_i) = \\frac{1}{n^2}\\,n\\sigma^2 = \\frac{\\sigma^2}{n} , \\] where, for this one, we used the independence of the random variables.\n\nProof. Apply the probability facts from above with \\(Y = \\phi(X)\\). This gives:\n\n\\(\\Ex \\widehat{\\theta}_n^{\\mathrm{MC}} = \\Ex \\overline Y_n = \\Ex Y = \\Exg \\phi(X)\\), so \\(\\operatorname{bias}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\Exg \\phi(X) - \\Exg \\phi(X) = 0\\).\n\\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\operatorname{Var}\\big(\\overline Y_n\\big) = \\frac{1}{n} \\operatorname{Var}(Y) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nUsing Theorem 3.1, \\[\\operatorname{MSE}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\operatorname{bias}(\\widehat{\\theta}_n^{\\mathrm{MC}})^2 + \\operatorname{Var}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = 0^2 + \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) . \\]\nTake the square root of part 3.\n\n\nThere’s a problem here, though. The reason we are doing Monte Carlo estimation in the first place is that we couldn’t calculate \\(\\Exg \\phi(X)\\). So it seems very unlikely we’ll be able to calculate the variance \\(\\operatorname{Var}(\\phi(X))\\) either.\nBut we can estimate the variance from our samples too: by taking the sample variance of our samples \\(\\phi(x_i)\\). That is, we can estimate the variance of the Monte Carlo estimator by \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\big(\\phi(X_i) - \\widehat{\\theta}_n^{\\mathrm{MC}} \\big)^2 . \\] Then we can similarly estimate the mean-square and root-mean-square errors by \\[ \\text{MSE} \\approx \\frac{1}{n}S^2 \\qquad \\text{and} \\qquad \\text{RMSE} \\approx \\sqrt{\\frac{1}{n} S^2} = \\frac{1}{\\sqrt{n}}S  \\] respectively.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#example",
    "href": "lectures/L03-mc-error-1.html#example",
    "title": "3  Monte Carlo error I: theory",
    "section": "3.3 Example",
    "text": "3.3 Example\n\nExample 3.1 Let’s go back to the very first example in the module, Example 1.1, where we were trying to find the expectation of an \\(\\operatorname{Exp}(2)\\) random variable. We used this R code:\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.5001026\n\n\n(Because Monte Carlo estimation is random, this won’t be the exact same estimate we had before, of course.)\nSo if we want to investigate the error, we can use the sample variance of these samples. We will use the sample variance function var() to calculate the sample variance.\n\nvar_est &lt;- var(samples)\nMSEest  &lt;- var_est / n\nRMSEest &lt;- sqrt(MSEest)\nc(var_est, MSEest, RMSEest)\n\n[1] 2.508338e-01 2.508338e-07 5.008331e-04\n\n\nThe first number is var_est \\(= 0.251\\), the sample variance of our \\(\\phi(x_i)\\)s: \\[ s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\big(\\phi(x_i) - \\widehat{\\theta}_n^{\\mathrm{MC}}\\big)^2 . \\] This should be a good estimate of the true variance \\(\\operatorname{Var}(\\phi(X))\\). In calculating this, we used R’s var() function, which calculate the sample variance of some data.\nThe second number is MSEest \\(= 2.51\\times 10^{-7}\\), our estimate of the mean-square error. Since \\(\\operatorname{MSE}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\frac{1}{n} \\operatorname{Var}(\\phi(X))\\), then \\(\\frac{1}{n} s^2\\) should be a good estimate of the MSE.\nThe third number is RMSEest \\(= 5.01\\times 10^{-4}\\) our estimate of the root-mean square error, which is simply the square-root of our estimate of the mean-square error.\n\nNext time: We’ll continue analysing Monte Carlo error, with more examples.\n\nSummary:\n\nThe Monte Carlo estimator is unbiased.\nThe Monte Carlo estimator has mean-square error \\(\\Var(\\phi(X))/n\\), so the root-mean-square error scales like \\(1/\\sqrt{n}\\).\nThe mean-square error can be estimated by \\(S^2 / n\\), where \\(S^2\\) is the sample variance of \\(\\phi(X)\\).\n\nOn Problem Sheet 1, you should now be able to answers Questions 1–6, except 2(c).\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.2.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html",
    "href": "lectures/L04-mc-error-2.html",
    "title": "4  Monte Carlo error II: practice",
    "section": "",
    "text": "4.1 Recap\nLet’s recap where we’ve got to. We know that the Monte Carlo estimator for \\(\\theta = \\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) .\\] Last time, we saw that the Monte Carlo estimator is unbiased, and that its mean-square and root-mean-square errors are \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) \\qquad \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} . \\] We saw that these themselves can be estimated as \\(S^2/n\\) and \\(S/\\sqrt{n}\\) respectively, where \\(S^2\\) is the sample variance of the \\(\\phi(X_i)\\)s.\nLet’s do one more example before moving on.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#recap",
    "href": "lectures/L04-mc-error-2.html#recap",
    "title": "4  Monte Carlo error II: practice",
    "section": "",
    "text": "Example 4.1 In Example 2.1, we were estimating \\(\\mathbb P(Z &gt; 2)\\), where \\(Z\\) is a standard normal.\nOur code was\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.02243\n\n\nSo our root-mean-square error can be approximated as\n\nRMSEest &lt;- sqrt(var(samples &gt; 2) / n)\nRMSEest\n\n[1] 0.0001493805",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#confidence-intervals",
    "href": "lectures/L04-mc-error-2.html#confidence-intervals",
    "title": "4  Monte Carlo error II: practice",
    "section": "4.2 Confidence intervals",
    "text": "4.2 Confidence intervals\nSo far, we have described our error tolerance in terms of the MSE or RMSE. But we could have talked about “confidence intervals” or “margins of error” instead. This might be easier to understand for non-mathematicians, for whom “root-mean-square error” doesn’t really mean anything.\nHere, we will want to appeal to the central limit theorem approximation. A bit more probability revision: Let \\(Y_1, Y_2, \\dots\\) be IID again, with expectation \\(\\mu\\) and variance \\(\\sigma^2\\). Write \\(\\overline Y_n\\) for the mean. We’ve already reminded ourselves that \\(\\mathbb E \\overline Y_n = \\mu\\) and \\(\\Var(\\overline{Y}_n) = \\sigma^2/n\\). But the central limit theorem says that the distribution of \\(\\overline Y_n\\) is approximately normally distributed with those parameters, so \\(\\overline Y_n \\approx \\operatorname{N}(\\mu, \\sigma^2/n)\\) when \\(n\\) is large. (This is an informal statement of the central limit theorem: you probably know some more formal ways to more precisely state the it, but this will do for us.)\nRecall that, in the normal distribution \\(\\operatorname{N}(\\mu, \\sigma^2)\\), we expect to be within \\(1.96\\) standard deviations of the mean with 95% probability. More generally, the interval \\([\\mu - q_{1-\\alpha/2}\\sigma, \\mu + q_{1-\\alpha/2}\\sigma]\\), where \\(q_{1-\\alpha/2}\\) is the \\((1- \\frac{\\alpha}{2})\\)-quantile of the normal distribution, contains the true value with probability approximately \\(1 - \\alpha\\).\nWe can form an approximate confidence interval for a Monte Carlo estimate using this idea. We have our Monte Carlo estimator \\(\\widehat{\\theta}_n^\\mathrm{MC}\\) as our estimator of the \\(\\mu\\) parameter, and our estimator of the root-mean-square error \\(S/\\sqrt{n}\\) as our estimator of the \\(\\sigma\\) parameter. So our confidence interval is estimated as \\[\\bigg[ \\widehat{\\theta}_n^\\mathrm{MC} - q_{1-\\alpha/2}\\,\\frac{S}{\\sqrt{n}}, \\ \\widehat{\\theta}_n^\\mathrm{MC} + q_{1-\\alpha/2}\\,\\frac{S}{\\sqrt{n}} \\bigg] . \\]\n\nExample 4.2 We continue the example of Example 2.1 and Example 4.1, where we were estimating \\(\\mathbb P(Z &gt; 2)\\) for \\(Z\\) a standard normal.\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest   &lt;- mean(samples &gt; 2)\nRMSEest &lt;- sqrt(var(samples &gt; 2) / n)\nMCest\n\n[1] 0.022621\n\n\nOur confidence interval is estimates as follows\n\nalpha &lt;- 0.05\nquant &lt;- qnorm(1 - alpha / 2)\nc(MCest - quant * RMSEest, MCest + quant * RMSEest)\n\n[1] 0.02232957 0.02291243",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#how-many-samples-do-i-need",
    "href": "lectures/L04-mc-error-2.html#how-many-samples-do-i-need",
    "title": "4  Monte Carlo error II: practice",
    "section": "4.3 How many samples do I need?",
    "text": "4.3 How many samples do I need?\nIn our examples we’ve picked the number of samples \\(n\\) for our estimator, then approximated the error based on that. But we could do things the other way around – fix an error tolerance that we’re willing to deal with, then work out what sample size we need to achieve it.\nWe know that the root-mean-square error is \\[ \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} \\] So if we want to get the RMSE down to \\(\\epsilon\\), say, then this shows that we need \\[ n = \\frac{1}{\\epsilon^2} \\Var\\big(\\phi(X)\\big) . \\]\nWe still have a problem here, though. We (usually) don’t know \\(\\Var(\\phi(X))\\). But we can’t even estimate \\(\\Var(\\phi(X))\\) until we’ve already taken the samples. But we can use this idea with a three-step process:\n\nRun an initial “pilot” Monte Carlo algorithm with a small number of samples \\(n\\). Use the results of the “pilot” to estimate the variance \\(S^2 \\approx \\Var(\\phi(X))\\). We want \\(n\\) small enough that this runs very quickly, but big enough that we get a reasonably OK estimate of the variance.\nPick a desired RMSE accuracy \\(\\epsilon\\). We now know that we require roughly \\(N = S^2 / \\epsilon^2\\) samples to get our desired accuracy.\nRun the “real” Monte Carlo algorithm with this big number of samples \\(N\\). We will put up with this being quite slow, because we know we’re definitely going to get the error tolerance we need.\n\n(We could potentially use further steps, where we now check the variance with the “real” big-\\(N\\) samples, and, if we learn we had underestimated in Step 1, take even more samples to correct for this.)\n\nExample 4.3 Let’s try this with Example 1.2 from before. We were trying to estimate \\(\\mathbb{E}(\\sin X)\\), where \\(X \\sim \\operatorname{N}(1, 2^2)\\).\nWe’ll start with just \\(n = 1000\\) samples, for our pilot study.\n\nn_pilot &lt;- 1000\nsamples &lt;- rnorm(n_pilot, 1, 2)\nvar_est &lt;- var(sin(samples))\nvar_est\n\n[1] 0.4850801\n\n\nThis was very quick! We won’t have got a super-accurate estimate of \\(\\mathbb E\\phi(X)\\), but we have a reasonable idea of roughly what \\(\\operatorname{Var}(\\phi(X))\\) is. This will allow us to pick out “real” sample size in order to get a root-mean-square error of \\(10^{-4}\\).\n\nepsilon &lt;- 1e-4\nn_real  &lt;- round(var_est / epsilon^2)\nn_real\n\n[1] 48508011\n\n\nThis tells us that we will need about 50 million samples! This is a lot, but now we know we’re going to get the accuracy we want, so it’s worth it. (In this particular case, 50 million samples will only take a few second on a modern computer. But generally, once we know our code works and we know how many samples we will need for the desired accuracy, this is the sort of thing that we could leave running overnight or whatever.)\n\nsamples &lt;- rnorm(n_real, 1, 2)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.1139476\n\nRMSEest &lt;- sqrt(var(sin(samples)) / n_real)\nRMSEest\n\n[1] 0.0001002164\n\n\nThis was very slow, of course. But we see that we have indeed got our Monte Carlo estimate to (near enough) the desired accuracy.\n\nGenerally, if we want a more accurate Monte Carlo estimator, we can just take more samples. But the equation \\[ n = \\frac{1}{\\epsilon^2} \\Var\\big(\\phi(X)\\big) \\] is actually quite bad news. To get an RMSE of \\(\\epsilon\\) we need order \\(1/\\epsilon^2\\) samples. That’s not good. Think of it like this: to double the accuracy we need to quadruple the number of samples. Even worse: to get “one more decimal place of accuracy” means dividing \\(\\epsilon\\) by ten; but that means multiplying the number of samples by one hundred!\nMore samples take more time, and cost more energy and money. Wouldn’t it be nice to have some better ways of increasing the accuracy of a Monte Carlo estimate besides just taking more and more samples?\nNext time: We begin our study of clever “variance reduction” methods for Monte Carlo estimation.\n\nSummary:\n\nWe can approximate confidence intervals for a Monte Carlo estimate by using a normal approximation.\nTo get the root-mean-square error below \\(\\epsilon\\) we need \\(n = \\Var(\\phi(X))/\\epsilon^2\\) samples.\nWe can use a two-step process, where a small “pilot” Monte Carlo estimation allows us to work out how many samples we will need for the big “real” estimation.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 3.2.2–3.2.4.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html",
    "href": "lectures/L05-cv.html",
    "title": "5  Control variate",
    "section": "",
    "text": "5.1 Variance reduction\nLet’s recap where we’ve got to. The Monte Carlo estimator of \\(\\theta = \\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i), \\] where \\(X_1, X_2, \\dots, X_n\\) are IID random samples from \\(X\\). The mean-square error of this estimator is \\[{\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} . \\] If we want a more accurate estimate, we can just take more samples \\(n\\). But the problem is that the root-mean-square error scales like \\(1/\\sqrt{n}\\). To double the accuracy, we need four times as many samples; for one more decimal place of accuracy, we need one hundred times as many samples.\nAre there other ways we could reduce the error of Monte Carlo estimation, so we need fewer samples? That is, can we use some mathematical ingenuity to adapt the Monte Carlo estimate to one with a smaller error?\nWell, the mean-square error is the variance divided by \\(n\\). So if we can’t (or don’t want to) increase \\(n\\), perhaps we can decrease the variance instead? Strategies to do this are called variance reduction strategies. In this module, we will look at three variance reduction strategies:",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html#variance-reduction",
    "href": "lectures/L05-cv.html#variance-reduction",
    "title": "5  Control variate",
    "section": "",
    "text": "Control variate: We can “anchor” our estimate of \\(\\Exg \\phi(X)\\) to a similar but easier-to-calculate value \\(\\Exg \\psi(X)\\). (This lecture)\nAntithetic variables: Instead of using independent samples, we could use correlated samples. If the correlation is negative this can improve our estimate. (Lectures 6 and 7)\nImportance sampling: Instead of sampling from \\(X\\), sample from some other more suitable distribution instead, then readjust the answer we get. (Lectures 8 and 9)",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html#control-variate-estimation",
    "href": "lectures/L05-cv.html#control-variate-estimation",
    "title": "5  Control variate",
    "section": "5.2 Control variate estimation",
    "text": "5.2 Control variate estimation\nIn last Friday’s lecture, I polled the class on this question: Estimate the average time it takes to fly from London to Washington D.C.\n\nThe actual answer is: 8 hours and 29 minutes.\nThe mean guess for the class was: 7 hours and 37 minutes (52 minutes too little)\nThe root-mean-square error for the guesses was: 133 minutes\n\nAfter you’d guessed, I gave the following hint: Hint: The average time it takes to fly from London to New York is 8 hours and 10 minutes. After the hint:\n\nThe mean guess for the class was: 8 hours and 45 minutes (16 minutes too much)\nThe root-mean-square error for the guesses was: 49 minutes\n\nSo after the hint, the error of the class was reduced by 63%.\nWhy did the hint help? We were trying to estimate \\(\\theta^{\\mathrm{DC}}\\), the distance to D.C. But that’s a big number, and the first estimates had a big error (over an hour, on average). After the hint, I expect most people thought something like this: “The answer \\(\\theta^{\\mathrm{DC}}\\) is going to be similar to the \\(\\theta^{\\mathrm{NY}} =\\) 8:10 to New York, but Washington D.C. is a bit further, so I should increase the number a bit, but not too much.”\nTo be more mathematical, we could write \\[\\theta^{\\mathrm{DC}} = \\theta^{\\mathrm{DC}} + \\big(\\theta^{\\mathrm{NY}} - \\theta^{\\mathrm{NY}}\\big)= \\underbrace{\\big(\\theta^{\\mathrm{DC}} - \\theta^{\\mathrm{NY}}\\big)}_{\\text{small}} + \\underbrace{\\theta^{\\mathrm{NY}}\\vphantom{\\big)}}_{\\text{known}} . \\] In that equation, the second term, \\(\\theta^{\\mathrm{NY}} =\\) 8:10 was completely known, so had error 0, while the first term \\(\\theta^{\\mathrm{DC}} - \\theta^{\\mathrm{NY}}\\) (actually 19 minutes) was a small number, so only had a small error.\nThis idea of improving an estimate by “anchoring” it to some known value is called controlled estimation. It is a very useful idea in statistics (and in life!).\nWe can apply this idea to Monte Carlo estimation too. Suppose we are trying to estimate \\(\\theta = \\Exg \\phi(X)\\). We could look for a function \\(\\psi\\) that is similar to \\(\\phi\\) (at least for the values of \\(x\\) that have high probability for the random variable \\(X\\)), but where we know for certain what \\(\\Exg \\psi(X)\\) is. Then we can write \\[ \\theta = \\Exg \\phi(X) = \\Exg \\big(\\phi(X) - \\psi(X) + \\psi(X)\\big) = \\underbrace{\\Exg\\big(\\phi(X) - \\psi(X)\\big)}_{\\text{estimate this with Monte Carlo}} + \\underbrace{\\Exg \\psi(X)\\vphantom{\\big)}}_{\\text{known}} . \\]\nHere, \\(\\psi(X)\\) is known as the control variate.\n\nDefinition 5.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(\\psi\\) be a function such that \\(\\eta = \\Exg\\psi(X)\\) is known. Suppose that \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\). Then the control variate Monte Carlo estimate \\(\\widehat\\theta_n^{\\mathrm{CV}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta . \\]\n\n\nExample 5.1 Let’s try to estimate \\(\\Ex \\cos(X)\\), where \\(X \\sim \\operatorname{N}(0,1)\\) is a standard normal distribution.\nWe could do this the “usual” Monte Carlo way.\n\nn &lt;- 1e6\nphi &lt;- function(x) cos(x)\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(phi(samples))\nMCest\n\n[1] 0.6066079\n\n\nBut we could see if we can do better with a control variate. But what should we pick for the control function \\(\\psi\\)? We want something that’s similar to \\(\\phi(x) = \\cos(x)\\), but where we can actually calculate the expectation.\nHere’s a suggestion. If we remember our Taylor series, we know that, for \\(x\\) near \\(0\\), \\[ \\cos x \\approx 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots . \\] So how about taking the first two nonzero terms in the Taylor series \\[ \\psi(x) = 1 - \\frac{x^2}{2} . \\] That is quite close to \\(\\cos x\\), at least for the values of \\(x\\) near 0 that \\(X \\sim \\operatorname{N}(0,1)\\) is most likely to take.\n\n\nCode for drawing this graph\ncurve(\n  cos(x), from = -4.5, to = 4.5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"\", xlim = c(-4,4), ylim = c(-1.2,1.2)\n)\ncurve(1 - x^2 / 2, add = TRUE, col = \"red\", lwd = 2)\nlegend(\n  \"topright\", c(\"cos x\", expression(1 - x^2 / 2)),\n  lwd = c(3, 2), col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nNot only that, but we know that for \\(Y \\sim \\operatorname{N}(\\mu, \\sigma^2)\\) we have \\(\\Ex Y^2 = \\mu^2 + \\sigma^2\\). So \\[ \\Exg \\psi(X) = \\Exg \\left(1 - \\frac{X^2}{2} \\right) = 1 - \\frac{\\Ex X^2}{2} = 1 - \\frac{0^2 + 1}{2} = \\frac12 . \\]\nSo our control variate estimate is:\n\npsi &lt;- function(x) 1 - x^2 / 2\nCVest &lt;- mean(phi(samples) - psi(samples)) + 1/2\nCVest\n\n[1] 0.6064711",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html#error-of-control-variate-estimate",
    "href": "lectures/L05-cv.html#error-of-control-variate-estimate",
    "title": "5  Control variate",
    "section": "5.3 Error of control variate estimate",
    "text": "5.3 Error of control variate estimate\nWhat is the error in a control variate estimate?\n\nTheorem 5.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and \\(\\theta = \\Exg\\phi(X)\\). Let \\(\\psi\\) be a function such that \\(\\eta \\Exg\\psi(X)\\) is known. Let \\[ \\widehat{\\theta}_n^{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta\\] be the control variate Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is \\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}\\).\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is \\({\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}\\).\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{CV}}\\) is \\({\\displaystyle \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big) = \\frac{1}{\\sqrt{n}} \\sqrt{\\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}}\\).\n\n\n\nProof. This is very similar to Theorem 3.2, so we’ll just sketch the important differences.\nIn part 1, we have \\[\\begin{align*}\n\\Exg \\widehat{\\theta}_n^{\\mathrm{CV}}\n  &= \\Exg \\left(\\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big)\\right) + \\eta \\\\\n  &= \\frac{1}{n}\\Exg \\left(\\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big)\\right) + \\eta \\\\\n  &= \\frac{n}{n}\\Exg\\big(\\phi(X) - \\psi(X)\\big) + \\eta \\\\\n  &= \\Exg\\phi(X) - \\Exg\\psi(X) + \\eta \\\\\n  &= \\Exg\\phi(X) ,\n\\end{align*}\\] since \\(\\eta = \\Exg\\psi(X)\\). So the estimator is unbiased.\nFor part 2, remembering that \\(\\eta = \\Exg \\psi(X)\\) is a constant, so doesn’t affect the variance, we have \\[\\begin{align*}\n\\Var \\big(\\widehat{\\theta}_n^{\\mathrm{CV}}\\big)\n&= \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta \\right) \\\\\n&= \\Big( \\frac{1}{n}\\Big)^2 \\Var \\left(\\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) \\right) \\\\\n&= \\frac{n}{n^2} \\Var \\big(\\phi(X) - \\psi(X)\\big) \\\\\n&= \\frac{1}{n} \\Var \\big(\\phi(X) - \\psi(X)\\big) .\n\\end{align*}\\]\nParts 3 and 4 follow in the usual way.\n\nThis tells us that a control variate Monte Carlo estimate is good when the variance of \\(\\phi(X) - \\psi(X)\\). This variance is likely to be small if \\(\\phi(X) - \\psi(X)\\) is usually small – although, to be more precise, it’s more important for \\(\\phi(X) - \\psi(X)\\) to be consistent, rather than small per se.\nAs before, we can’t usually calculate the variance \\(\\Var(\\phi(X) - \\psi(X))\\) exactly, but we can estimate it from the samples. Again, we use the sample variance \\[S^2 = \\frac{1}{n-1}\\sum_{i=1}^n \\Big(\\big(\\phi(X_i) - \\psi(X_i)\\big) - \\big(\\widehat\\theta_n^{\\mathrm{CV}} + \\eta\\big)\\Big)^2 , \\] and estimate the MSE and RMSE by \\(S^2 / n\\) and \\(S / \\sqrt{n}\\) respectively.\n\nExample 5.2 We return to Example 5.1, where we were estimating \\(\\Ex \\cos(X)\\) for \\(X \\sim \\operatorname{N}(0,1)\\).\nThe naive Monte Carlo estimate had mean-square and root-mean-square error\n\nn &lt;- 1e6\nphi &lt;- function(x) cos(x)\nsamples &lt;- rnorm(n)\nMC_MSE &lt;- var(phi(samples)) / n\nc(MC_MSE, sqrt(MC_MSE))\n\n[1] 1.984850e-07 4.455166e-04\n\n\nThe variance and root-mean-square error of our control variate estimate, on the other hand, are\n\npsi &lt;- function(x) 1 - x^2 / 2\nCV_MSE &lt;- var(phi(samples) - psi(samples)) / n\nc(CV_MSE, sqrt(CV_MSE))\n\n[1] 9.299204e-08 3.049460e-04\n\n\nThis was a success! The mean-square error roughly halved, from \\(2\\times 10^{-7}\\) to \\(9.3\\times 10^{-8}\\). This meant the root-mean-square went down by about a third, from \\(4.5\\times 10^{-4}\\) to \\(3\\times 10^{-4}\\).\nHalving the mean-square error would normally have required doubling the number of samples \\(n\\), so we have effectively doubled the sample size by using the control variate.\n\nNext time: We look at our second variance reduction technique: antithetic variables.\n\nSummary:\n\nVariance reduction techniques attempt to improve on Monte Carlo estimation making the variance smaller.\nIf we know \\(\\eta = \\Exg \\psi(X)\\), then the control variate Monte Carlo estimate is \\[ \\widehat{\\theta}_n^{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta.\\]\nThe mean-square error of the control variate Monte Carlo estimate is \\[{\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}.\\]\n\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.3.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html",
    "href": "lectures/L06-antithetic-1.html",
    "title": "6  Antithetic variables I",
    "section": "",
    "text": "6.1 Estimation with correlation\nThis lecture and the next, we will be looking at our second variance reduction method for Monte Carlo estimation: the use of antithetic variables.” The word “antithetic” refers to using negative correlation to reduce the variance an estimator.\nLet’s start with the simple example of estimating an expectation from \\(n = 2\\) samples. Suppose \\(Y\\) has expectation \\(\\mu = \\Ex Y\\) and variance \\(\\Var(Y) = \\sigma^2\\). Suppose \\(Y_1\\) and \\(Y_2\\) are independent samples from \\(Y\\). Then the Monte Carlo estimator is \\[ \\overline Y = \\tfrac12(Y_1 + Y_2) . \\] This estimator is unbiased, since \\[ \\Ex \\overline Y = \\Ex \\big(\\tfrac12(Y_1 + Y_2)\\big) = \\tfrac12 ( \\Ex Y_1 + \\Ex Y_2 ) = \\tfrac12 (\\mu + \\mu) = \\mu . \\] Thus the mean-square error equals the variance, which is \\[ \\Var \\big( \\overline Y\\big) = \\Var \\big(\\tfrac12(Y_1 + Y_2)\\big) =\\tfrac14 \\big( \\Var(Y_1) + \\Var(Y_2) \\big)= \\tfrac14 (\\sigma^2 + \\sigma^2) = \\tfrac12 \\sigma^2 . \\]\nBut what if \\(Y_1\\) and \\(Y_2\\) still have the same distribution as \\(Y\\) but now are not independent? The expectation is still the same, so the estimator is still unbiased. But the variance (and hence mean-square error) is now \\[ \\Var \\big( \\overline Y\\big) = \\Var \\big(\\tfrac12(Y_1 + Y_2)\\big) =\\tfrac14 \\big( \\Var(Y_1) + \\Var(Y_2) + 2 \\Cov(Y_1, Y_2) \\big) . \\] Write \\(\\rho\\) for the correlation \\[ \\rho = \\Corr(Y_1, Y_2) = \\frac{\\Cov(Y_1, Y_2)}{\\sqrt{\\Var(Y_1) \\Var(Y_2)}} = \\frac{\\Cov(Y_1, Y_2)}{\\sqrt{\\sigma^2 \\times \\sigma^2}} = \\frac{\\Cov(Y_1, Y_2)}{\\sigma^2} . \\] (Remember that \\(-1 \\leq \\rho \\leq +1\\).) Then the variance is \\[ \\Var \\big( \\overline Y\\big) = \\tfrac14 ( \\sigma^2 + \\sigma^2 + 2 \\rho \\sigma^2 ) = \\frac{1+\\rho}{2} \\,\\sigma^2 . \\]\nWe can compare this with the variance \\(\\frac12 \\sigma^2\\) from the independent-sample case:",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html#estimation-with-correlation",
    "href": "lectures/L06-antithetic-1.html#estimation-with-correlation",
    "title": "6  Antithetic variables I",
    "section": "",
    "text": "If \\(Y_1\\) and \\(Y_2\\) are positively correlated, in that \\(\\rho &gt; 0\\), then the variance, and hence the mean-square error, has got bigger. This means the estimator is worse. This is because, with positive correlation, errors compound each other – if one sample is bigger than average, then the other one is likely to be bigger than average too; while if one sample is smaller than average, then the other one is likely to be smaller than average too.\nIf \\(Y_1\\) and \\(Y_2\\) are negatively correlated, in that \\(\\rho &lt; 0\\), then the variance, and hence the mean-square error, has got smaller. This means the estimator is better. This is because, with negative correlation, errors compensate for each other – if one sample is bigger than average, then the other one is likely to be smaller than average, which will help “cancel out” the error.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html#monte-carlo-with-antithetic-variables",
    "href": "lectures/L06-antithetic-1.html#monte-carlo-with-antithetic-variables",
    "title": "6  Antithetic variables I",
    "section": "6.2 Monte Carlo with antithetic variables",
    "text": "6.2 Monte Carlo with antithetic variables\nWe have seen that negative correlation helps improve estimation from \\(n=2\\) samples. How can we make this work in our favour for Monte Carlo simulation with many more samples?\nWe will look at the idea of antithetic pairs. So instead of taking \\(n\\) samples \\[ X_1, X_2, \\dots, X_n \\] that are all independent of each other, we will take \\(n/2\\) pairs of samples \\[ (X_1, X'_1), (X_2, X'_2), \\dots, (X_{n/2}, X'_{n/2}) . \\] (Here, \\(n/2\\) pairs means \\(n\\) samples over all.) Within each pair, \\(X_i\\) and \\(X_i'\\) will not be independent, but between different pairs \\(i \\neq j\\), \\((X_i, X_i')\\) and \\((X_j, X'_j)\\) will be independent.\n\nDefinition 6.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(X'\\) have the same distribution as \\(X\\) (but not necessarily be independent of it). Suppose that \\((X_1, X_1')\\), \\((X_2, X_2')\\), \\(\\dots\\), \\((X_{n/2}, X'_{n/2})\\) are pairs of random samples from \\((X, X')\\). Then the antithetic variables Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{AV}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X'_i) \\big) .\\]\n\nThe expression above for \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) makes it clear that that this is a mean of the sum from each pair. Alternatively, we can rewrite the estimator as \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{2} \\left( \\frac{1}{n/2} \\sum_{i=1}^{n/2} \\phi(X_i) + \\frac{1}{n/2} \\sum_{i=1}^{n/2} \\phi(X_i') \\right) , \\] which highlights that it is the mean of the estimator from the \\(X_i\\)s and the the estimator from the \\(X'_i\\)s.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html#examples",
    "href": "lectures/L06-antithetic-1.html#examples",
    "title": "6  Antithetic variables I",
    "section": "6.3 Examples",
    "text": "6.3 Examples\n\nExample 6.1 Recall Example 2.1 (continued in Example 4.1 and Example 4.2). Here, we were estimating \\(\\mathbb P(Z &gt; 2)\\) for \\(Z\\) a standard normal.\nThe basic Monte Carlo estimate was\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest   &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.022911\n\n\nCan we improve this estimate with an antithetic variable? Well, if \\(Z\\) is a standard normal, then \\(Z' = -Z\\) is also standard normal and is not independent of \\(Z\\). So maybe that could work as an antithetic variable. Let’s try\n\nn &lt;- 1e6\nsamples1 &lt;- rnorm(n / 2)\nsamples2 &lt;- -samples1\nAVest &lt;- (1 / n) * sum((samples1 &gt; 2) + (samples2 &gt; 2))\nAVest\n\n[1] 0.022929\n\n\n\n\nExample 6.2 Let’s consider estimating \\(\\mathbb E \\sin U\\), where \\(U\\) is continuous uniform on \\([0,1]\\).\nThe basic Monte Carlo estimate is\n\nn &lt;- 1e6\nsamples &lt;- runif(n)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.4599953\n\n\nWe used runif(n, min, max) to generate \\(n\\) samples on the interval \\([\\mathtt{min}, \\mathtt{max}]\\). However, if you omit the min and max arguments, then R assumes the default values min = 0, max = 1, which is what we want here.\nIf \\(U\\) is uniform on \\([0,1]\\), then \\(1 - U\\) is also uniform on \\([0,1]\\). We could try using that as an antithetic variable.\n\nn &lt;- 1e6\nsamples1 &lt;- runif(n / 2)\nsamples2 &lt;- 1 - samples1\nAVest &lt;- (1 / n) * sum(sin(samples1) + sin(samples2))\nAVest\n\n[1] 0.4597048\n\n\n\nWe have taken \\(n/2\\) pairs of samples, because that means we have \\(n/2 \\times 2 = n\\) samples over all, which seems like a fair comparison. This is certainly the case if generating the sample and generating its antithetic pair cost roughly the same in terms of time (or energy, or money). However, if generating the first variate of each pair is slow, but then generating the second antithetic variate is much quicker, it might be a fairer comparison to take a full \\(n\\) pairs.\nMore generally, you might put a cost \\(c_1\\) on each first variate and \\(c_2\\) on each antithetic pair. Then one can compare a cost of \\(c_1n\\) for standard Monte Carlo with \\(n\\) samples to a cost of \\((c_1 + c_2)m\\) for antithetic variables Monte Carlo with \\(m\\) pairs.\nAre these antithetic variables estimates an improvement on the basic Monte Carlo estimate? We’ll find out next time.\nNext time: We continue our study of the antithetic variables method with more examples and analysis of the error.\n\nSummary:\n\nEstimation is helped by combining individual estimates that are negatively correlated.\nFor antithetic variables Monte Carlo estimation, we take pairs of non-independent variables \\((X, X')\\), to get the estimator \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X'_i) \\big) . \\]\n\nOn Problem Sheet 1, you should now be able to answer all questions. You should work through this problem sheet in advance of the problems class on Thursday 17 October.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "problems/P1.html",
    "href": "problems/P1.html",
    "title": "Problem Sheet 1",
    "section": "",
    "text": "Full solutions are now available.\n\n\nThis is Problem Sheet 1, which covers material from Lectures 1 to 6. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Thursday 17 October. If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Mondays at 1500.\nThis problem sheet is to help you practice material from the module. It is not for assessment and will not be marked. However, you can get feedback on your work by: (a) being well prepared for the problems class, by making attempts at all questions in advance; (b) speaking up in the problems class, asking for clarification where things aren’t clear and offering ideas on how to solve the questions; (c) discussing your work with colleagues and friends; (d) coming along to my office hours on Mondays at 1500; (e) studying the full solutions when they are released and comparing with your own attempts.\nMany of these questions will require use of the R programming language (for example, by using the program RStudio).\nFull solutions should be released on Friday 18 October.\n\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.      Let \\(X\\) be uniform on \\([-1,2]\\).\n\n(a)   By hand, calculate the exact value of \\(\\Ex X^4\\).\n\nSolution.\n\\[\\int_{-1}^2 x^4\\,\\frac{1}{2-(-1)}\\,\\mathrm{d}x = \\tfrac13 \\Big[\\tfrac15x^5\\Big]_{-1}^2 = \\tfrac13\\Big(\\tfrac{32}{5}-\\big(-\\tfrac15\\big)\\Big) = \\tfrac{33}{15} = \\tfrac{11}{5} = 2.2\\]\n\n\n\n(b)   Using R, calculate a Monte Carlo estimate for \\(\\Ex X^4\\).\n\nSolution. I used the R code\n\nn &lt;- 1e6\nsamples &lt;- runif(n, -1, 2)\nmean(samples^4)\n\n[1] 2.201916\n\n\n\n\n\n\n2.      Let \\(X\\) and \\(Y\\) both be standard normal distributions. Compute a Monte Carlo estimate of \\(\\Exg \\max\\{X,Y\\}\\). (You may wish to investigate R’s pmax() function.)\n\nSolution. By looking at ?pmax (or maybe searching on Google) I discovered that pmax() gives the “parallel maxima” of two (or more vectors). That is the first element is the maximum of all the first elements of the vectors; the second element is the maximum of all the second elements of the vectors; and so on.\nSo I used the R code\n\nn &lt;- 1e6\nxsamples &lt;- rnorm(n)\nysamples &lt;- rnorm(n)\nmean(pmax(xsamples, ysamples))\n\n[1] 0.5632374\n\n\n\n\n\n3.      You are trapped alone on an island. All you have with you is a tin can (radius \\(r\\)) and a cardboard box (side lengths \\(2r \\times 2r\\)) that it fits snugly inside. You put the can inside the box [left picture].\nWhen it starts raining, each raindrop that falls in the cardboard box might fall into the tin can [middle picture], or might fall into the corners of the box outside the can [right picture].\n\n\n(a)   Using R, simulate rainfall into the box. You may take units such that \\(r = 1\\). Estimate the probability \\(\\theta\\) that a uniformly-random raindrop that falls in the cardboard box falls into the tin can also.\n\nSolution. I set things up so that the box is \\([-1, 1]^2\\), centered at the origin. This means that the inside of the can is the set of points is those \\((x,y)\\) such that \\(x^2 + y^2 \\leq 1\\).\n\nn &lt;- 1e6\nrain_x &lt;- runif(n, -1, 1)\nrain_y &lt;- runif(n, -1, 1)\nin_box &lt;- function(x, y) x^2 + y^2 &lt;= 1\nmean(in_box(rain_x, rain_y))\n\n[1] 0.785122\n\n\n\n\n\n(b)   Calculate exactly the probability \\(\\theta\\).\n\nSolution. The area of the box is \\(2r \\times 2r = 4r^2\\). The area of the can is \\(\\pi r^2\\). So the probability a raindrop landing in the box lands in the can is \\[ \\frac{\\text{area of can}}{\\text{area of box}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\approx 0.785. \\]\n\n\n\n(c)   You realise that, despite being trapped all alone on the island, you now have a way of approximating the value of \\(\\pi\\). If you want to calculate \\(\\pi\\) to 6 decimal places, roughly how many raindrops do you need to fall into the box?\n\nSolution. The phrase “to 6 decimal places” isn’t a precise mathematical one. I’m going to interpret this as getting the root-mean-square error below \\(10^{-6}\\). If you interpret it slightly differently that’s fine – for example, getting the width of a 95% confidence interval below \\(10^{-6}\\) could be another, slightly stricter, criterion.\nOne could work this out by hand. Since the variance of a Bernoulli random variable is \\(p(1-p)\\), the mean-square error of our estimator is \\[ \\frac{\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})}{n} \\approx \\frac{0.169}{n} . \\] So we need \\[n = \\frac{0.169}{(10^{-6})^2} \\approx 169 \\text{ billion} . \\]\nThat said, if we are trapped on our desert island, maybe we don’t know what \\(\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})\\) is. In that case we could do this using the can and the box. Our estimate of the variance is\n\nvar_est &lt;- var(in_box(rain_x, rain_y))\nvar_est / (1e-6)^2\n\n[1] 168705613823\n\n\nWe will probably spend a long time waiting for that much rain!\n\n\n\n\n4.      Let \\(h(x) = 1/(x + 0.1)\\). We wish to estimate \\(\\int_0^5 h(x) \\, \\mathrm{d}x\\) using a Monte Carlo method.\n\n(a)   Estimate the integral using \\(X\\) uniform on \\([0,5]\\).\n\nSolution.\n\nn &lt;- 1e6\nintegrand &lt;- function(x) 1 / (x + 0.1)\nsamples1 &lt;- runif(n, 0, 5)\nmean(5 * integrand(samples1))\n\n[1] 3.925967\n\n\n(In fact, the true answer is \\(\\log(5.1) - \\log(0.1) = 3.932\\), so it looks like this is working correctly.)\n\n\n\n(b)   Can you come up with a choice of \\(X\\) that improves on the estimate from (a)?\n\nSolution. Let’s look at a graph of the integrand \\(h\\).\n\n\nCode for drawing this graph\ncurve(\n  integrand, n = 1001, from = 0, to = 5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(0,5)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nWe see that we get a much bigger contribution to the integral from values near 0. So a random variable that picks values nearer to 0 more often might have a chance of giving a more accurate result.\n\nI decided to try an exponential distribution with rate 1, which should sample the smaller values of \\(x\\) more often.\n\npdf &lt;- function(x) dexp(x, 1)\nphi &lt;- function(x) (integrand(x) / pdf(x)) * (x &lt;= 5)\n\nsamples2 &lt;- rexp(n, 1)\nmean(phi(samples2))\n\n[1] 3.936713\n\n\n(I had to include x &lt;= 5 in the expression for \\(\\phi\\), because my exponential distribution will sometimes take samples above 5, but they should count as 0 in an estimate for the integral between 0 and 5.)\nTo see whether or not this was an improvement, I estimated the mean-square error.\n\nvar(5 * integrand(samples1)) / n\n\n[1] 3.349673e-05\n\nvar(phi(samples2)) / n\n\n[1] 6.100181e-06\n\n\nI found that I had reduced the mean-square error by roughly a factor of 5. \n\n\n\n\n5.      When calculating a Monte Carlo estimate \\(\\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\\), one might wish to first generate the \\(n\\) samples \\((x_1, x_2, \\dots, x_n)\\) and store them, and only then, after all samples are generated, finally calculate the estimate. However, when \\(n\\) is extremely large, storing all \\(n\\) samples uses up a lot of space in a computer’s memory. Describe (in words, in R code, or in a mixture of the two) how the Monte Carlo estimate could be produced using much less memory.\n\nSolution. The idea is to keep a “running total” of the \\(\\phi(x_i)\\)s. Then we only have to store that running total, not all the samples. Once this has been done \\(n\\) times, then divide by \\(n\\) to get the estimate.\nIn R code, this might be something like\n\nn &lt;- 1e6\n\ntotal &lt;- 0\nfor (i in 1:n) {\n  sample &lt;- # sampling code for 1 sample\n  total &lt;- total + phi(sample)\n}\n\nMCest &lt;- total / n\n\n\n\n\n6.      Show that the indicator functions \\(\\mathbb I_A(X)\\) and \\(\\mathbb I_B(X)\\) have correlation 0 if and only if the events \\(\\{X \\in A\\}\\) and \\(\\{X \\in B\\}\\) are independent.\n\nSolution. Recall that two random variables \\(U\\), \\(V\\) have correlation 0 if and only if their covariance \\(\\Cov(U,V) = \\Ex UV - (\\Ex U)(\\Ex V)\\) is 0 too.\nWe know that \\(\\Exg\\Ind_A(X) = \\mathbb P(X \\in A)\\) and \\(\\Exg \\Ind_B(Y) = \\mathbb P(X \\in B)\\). What about \\(\\Exg \\Ind_A(X) \\Ind_B(X)\\)? Well, \\(\\Ind_A(x) \\Ind_B(x)\\) is 1 if and only if both indicator functions equal 1, which is if and only if both \\(x \\in A\\) and \\(x \\in B\\). So \\(\\Exg \\Ind_A(X) \\Ind_B(X) = \\mathbb P(X \\in A \\text{ and } X \\in B)\\).\nSo the covariance is \\[ \\Cov \\big(\\Ind_A(X), \\Ind_B(X) \\big) = \\mathbb P(X \\in A \\text{ and } X \\in B) - \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B) . \\] If this is 0, then \\(\\mathbb P(X \\in A \\text{ and } X \\in B) = \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B)\\), which is precisely the definition of those two events being independent.\n\n\n\n7.      Let \\(X\\) be an exponential distribution with rate 1.\n\n(a)   Estimate \\(\\mathbb EX^{2.1}\\) using the standard Monte Carlo method.\n\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 1)\nmean(samples^2.1)\n\n[1] 2.192614\n\n\n\n\n\n(b)   Estimate \\(\\mathbb EX^{2.1}\\) using \\(X^2\\) as a control variate. (You may recall that if \\(Y\\) is exponential with rate \\(\\lambda\\) then \\(\\mathbb EY^2 = 2/\\lambda^2\\).)\n\nSolution. We have \\(\\Ex X^2 = 2\\). So, re-using the same sample as before (you don’t have to do this – you could take new samples), our R code is as follows.\n\nmean(samples^2.1 - samples^2) + 2\n\n[1] 2.196733\n\n\n\n\n\n(c)   Which method is better?\n\nSolution. The better answer is the one with the smaller mean-square error.\nFor the basic method,\n\nvar(samples^2.1) / n\n\n[1] 2.753474e-05\n\n\nFor the control variate method,\n\nvar(samples^2.1 - samples^2) / n\n\n[1] 6.706706e-07\n\n\nSo the control variates method is much, much better.\n\n\n\n\n8.      Let \\(Z\\) be a standard normal distribution. A statistician has been investigating Monte Carlo estimates of \\(\\mathbb EZ^k\\) for different positive integers values of \\(k\\). Her colleague suggests using \\(Z' = -Z\\) as an antithetic variable. Without running any R code, explain whether or not this is a good idea (a) when \\(k\\) is even; (b) when \\(k\\) is odd.\n\nSolution.\n(a) When \\(k\\) is even, we have \\(Z^k = (-Z)^k\\). So the antithetic variables method just repeats each sample twice. This is obviously no benefit at all, and just wastes time. Indeed, we have perfect positive correlation \\(\\rho = +1\\), which is the “worst-case scenario”.\n(b) When \\(k\\) is odd, we have \\(Z^k = -(-Z)^k\\). In this case we know that \\(\\Ex Z^k = 0\\), because the results for positive \\(z\\) exactly balance out those for negative \\(z\\), so no Monte Carlo sampling is necessary. If our statistician has somehow forgotten that, though, she will get a pleasant surprise! After just two samples, she will get the estimate \\[\\frac{1}{2} \\big(Z_1^k + (-Z_1)^k \\big) = \\frac{1}{2} (Z_1^k - Z_1^k) = 0 ,\\] Thereby getting the result exactly right. Indeed, we have perfect negative correlation \\(\\rho = -1\\), which is the “best-case scenario”.",
    "crumbs": [
      "Monte Carlo estimation",
      "Problem Sheet 1"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html",
    "href": "lectures/L07-antithetic-2.html",
    "title": "7  Antithetic variables II",
    "section": "",
    "text": "7.1 Error with antithetic variables\nRecall from last time the antithetic variables Monte Carlo estimator. We take sample pairs \\[ (X_1, X'_1), (X_2, X'_2), \\dots, (X_{n/2}, X_{n/2}') , \\] where samples are independent between different pairs but not independent within the same pair. The estimator of \\(\\theta = \\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X'_i) \\big) .\\] We hope this is better than the standard Monte Carlo estimator if \\(\\phi(X)\\) and \\(\\phi(X')\\) are negatively correlated.\nIn points 2, 3 and 4, generally the first expression, involving the variance \\(\\operatorname{Var}(\\phi(X) + \\phi(X'))\\), is the most convenient for computation. We can estimate this easily from data using the sample variance in the usual way (as we will in the examples below).\nThe second expression, involving the correlation \\(\\rho\\), is usually clearer for understanding. Comparing these to the same results for the standard Monte Carlo estimator (Theorem 3.2), we see that the antithetic variables method is an improvement (that is, has a smaller mean-square error) when \\(\\rho &lt; 0\\), but is worse when \\(\\rho &gt; 0\\). This proves that negative correlation improves our estimator.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html#error-with-antithetic-variables",
    "href": "lectures/L07-antithetic-2.html#error-with-antithetic-variables",
    "title": "7  Antithetic variables II",
    "section": "",
    "text": "Theorem 7.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and \\(\\theta = \\Exg\\phi(X)\\). Let \\(X'\\) have the same distribution as \\(X\\), and write \\(\\rho = \\operatorname{Corr}(\\phi(X_i),\\phi(X'_i))\\). Let \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X_i')\\big) \\] be the antithetic variables Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is \\[ \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big) = \\frac{1+\\rho}{n}\\Var\\big(\\phi(X)\\big). \\]\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big) = \\frac{1+\\rho}{n}\\Var\\big(\\phi(X)\\big). \\]\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{AV}}\\) is \\[ \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{\\sqrt{2n}} \\sqrt{\\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big)} = \\frac{\\sqrt{1+\\rho}}{\\sqrt{n}}\\sqrt{\\Var\\big(\\phi(X)\\big)}. \\]\n\n\n\n\n\nProof. For unbiasedness, we have \\[ \\Ex \\widehat{\\theta}_n^{\\mathrm{AV}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X_i')\\big)\\right) = \\frac{1}{n} \\,\\frac{n}{2} \\big(\\Exg\\phi(X) + \\Exg \\phi(X')) = \\frac{1}{2}(\\theta+ \\theta) = \\theta ,\\] since \\(X'\\) has the same distribution as \\(X\\).\nFor the other three points, each of the first expressions follows straightforwardly in essentially the same way. (You can fill in the details yourself, if you need to.) For the second expressions, we have \\[\\begin{align*}\n\\Var \\big(\\phi(X) + \\phi(X')\\big)\n&= \\Var\\big(\\phi(X)\\big) + \\Var\\big(\\phi(X')\\big) + 2\\operatorname{Cov}\\big(\\phi(X),\\phi(X')\\big) \\\\\n&= \\Var\\big(\\phi(X)\\big) + \\Var\\big(\\phi(X')\\big) + 2\\rho\\sqrt{\\Var\\big(\\phi(X)\\big) \\Var\\big(\\phi(X')\\big)} \\\\\n&= \\Var\\big(\\phi(X)\\big) + \\Var\\big(\\phi(X)\\big) + 2\\rho\\sqrt{\\Var\\big(\\phi(X)\\big) \\Var\\big(\\phi(X)\\big)} \\\\\n&= 2(1+\\rho)\\Var\\big(\\phi(X)\\big) .\n\\end{align*}\\] The results then follow.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html#examples",
    "href": "lectures/L07-antithetic-2.html#examples",
    "title": "7  Antithetic variables II",
    "section": "7.2 Examples",
    "text": "7.2 Examples\nLet’s return to the two examples we tried last time.\n\nExample 7.1 In Example 6.1, we were estimating \\(\\mathbb P(Z &gt; 2)\\) for \\(Z\\) a standard normal.\nThe basic Monte Carlo estimate and its root-mean-square error are\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest   &lt;- mean(samples &gt; 2)\nMC_RMSE &lt;- sqrt(var(samples &gt; 2) / n)\nc(MCest, MC_RMSE)\n\n[1] 0.0227870000 0.0001492239\n\n\nWe then used \\(Z' = -Z\\) as an antithetic variable. its root-mean-square error are\n\nn &lt;- 1e6\nsamples1 &lt;- rnorm(n / 2)\nsamples2 &lt;- -samples1\nAVest &lt;- (1 / n) * sum((samples1 &gt; 2) + (samples2 &gt; 2))\nAV_RMSE &lt;- sqrt(var((samples1 &gt; 2) + (samples2 &gt; 2)) / (2 * n))\nc(AVest, AV_RMSE)\n\n[1] 0.0228490000 0.0001476648\n\n\nThis looked like it made very little difference – perhaps a small improvement. This can be confirmed by looking at the sample correlation with R’s cor() function.\n\ncor(samples1 &gt; 2, samples2 &gt; 2)\n\n[1] -0.02338327\n\n\nWe see there was a very small but negative correlation: the variance, and hence the mean-square error, was reduced by about 2%.\n\n\nExample 7.2 In Example 6.2, we were estimating \\(\\mathbb E \\sin U\\), where \\(U\\) is continuous uniform on \\([0,1]\\).\nThe basic Monte Carlo estimate and its root-mean square error is\n\nn &lt;- 1e6\nsamples &lt;- runif(n)\nMCest &lt;- mean(sin(samples))\nMC_RMSE &lt;- sqrt(var(sin(samples)) / n)\nc(MCest, MC_RMSE)\n\n[1] 0.4601180588 0.0002478952\n\n\nWe then used \\(U' = 1 - U\\) as an antithetic variable\n\nn &lt;- 1e6\nsamples1 &lt;- runif(n / 2)\nsamples2 &lt;- 1 - samples1\nAVest &lt;- (1 / n) * sum(sin(samples1) + sin(samples2))\nAV_RMSE &lt;- sqrt(var(sin(samples1) + sin(samples2)) / (2 * n))\nc(AVest, AV_RMSE)\n\n[1] 0.4596760966 0.0000248325\n\n\nThis time, we see a big improvement: the root-mean-square error has gone down by a whole order of magnitude, from \\(2\\times 10^{-4}\\) to \\(2\\times 10^{-5}\\). It would normally take 100 times as many samples to reduce the RMSE by a factor of 10, but we’ve got the extra 99 million samples for free by using antithetic variables!\nThe benefit here can be confirmed by looking at the sample correlation.\n\ncor(sin(samples1), sin(samples2))\n\n[1] -0.9899601\n\n\nThat’s a very large negative correlation, which shows why the antithetic variables made such a huge improvement.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L07-antithetic-2.html#finding-antithetic-variables",
    "href": "lectures/L07-antithetic-2.html#finding-antithetic-variables",
    "title": "7  Antithetic variables II",
    "section": "7.3 Finding antithetic variables",
    "text": "7.3 Finding antithetic variables\nAntithetic variables can provide a huge advantage compared to standard Monte Carlo, as we saw in the second example above. The downside is that it can often be difficult to find an appropriate antithetic variable.\nTo even be able to try the antithetic variables method, we need to find a random variable \\(X'\\) with the same distribution as \\(X\\) that isn’t merely an independent copy. Both the examples we have seen of this use a symmetric distribution; that is, a distribution \\(X\\) such that \\(X' = a - X\\) has the same distribution as \\(X\\), for some \\(a\\).\n\nWe saw that if \\(X \\sim \\operatorname{N}(0, 1)\\) is a standard normal distribution, then \\(X' = -X \\sim \\operatorname{N}(0, 1)\\) too. More generally, if \\(X\\sim \\operatorname{N}(\\mu, \\sigma^2)\\), then \\(X' = 2\\mu - X \\sim \\operatorname{N}(\\mu, \\sigma^2)\\) can be tried as an antithetic variable.\nWe saw that if \\(U \\sim \\operatorname{U}[0, 1]\\) is a continuous uniform distribution on \\([0,1]\\), then \\(U' = 1-U \\sim \\operatorname{U}[0, 1]\\) too. More generally, if \\(X\\sim \\operatorname{U}[a, b]\\), then \\(X' = (a + b) - X \\sim \\operatorname{U}[a, b]\\) can be tried as an antithetic variable.\n\nLater, when we study the inverse transform method (in Lecture 13) we will see another, more general, way to generate antithetic variables.\nBut to be a good antithetic variable, we need \\(\\phi(X)\\) and \\(\\phi(X')\\) to be negatively correlated too – preferably strongly so. Often, this is a matter of trial-and-error – it’s difficult to set out hard principles. But there are some results that try to formalise the idea that “nice functions of negatively correlated random variables are themselves negatively correlated”, which can be useful. We give one example of such a result here.\n\nTheorem 7.2 Let \\(U \\sim \\operatorname{U}[0, 1]\\) and \\(U' = 1 - U\\). Let \\(\\phi\\) be a monotonically increasing function. Then \\(\\phi(U)\\) and \\(\\phi'(U)\\) are negatively correlated, in that \\(\\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) \\leq 0\\).\n\nIf I were to just write out the proof of this theorem, the proof would be fairly short and easy to understand. But it would be difficult to understand why I had taken the steps I had. So I will try to explain why each of the steps in the proof is a natural one – although doing so may make the proof seem longer and more complicated than it really is.\n\nProof. The covariance in question is \\[ \\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) = \\Exg \\phi(U)\\phi(1-U) - \\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-U)\\big) . \\]\nWe would like to take a single expectation sign all the way outside. But we can’t do this yet, because \\(\\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-U)\\big)\\) and \\(\\Exg\\phi(U)\\phi(1-U)\\) are different things. However, we can do this if we introduce a new random variable \\(V\\) that is also \\(\\operatorname{U}[0,1]\\) but is independent of \\(U\\). Then we so have \\[\\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-U)\\big) = \\big(\\Exg\\phi(U)\\big) \\big(\\Exg\\phi(1-V)\\big) = \\Exg \\phi(U)\\phi(1-V).\\] So now we can write \\[ \\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) = \\Exg \\big(\\phi(U)\\phi(1-U) - \\phi(U)\\phi(1-V)\\big) . \\]\nWe’ve got a slightly odd asymmetry between \\(U\\) and \\(V\\) here, though. Why is the first term \\(\\phi(U)\\phi(1-U)\\) and not \\(\\phi(V)\\phi(1-V)\\)? Why is the second term \\(\\phi(U)\\phi(1-V)\\) and not \\(\\phi(V)\\phi(1-U)\\)? Well, maybe to even things up, we can write them as half of one and half of the other. That is, \\[\\begin{align*}\n\\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) &= \\tfrac12 \\Exg \\big(\\phi(U)\\phi(1-U) +\\phi(V)\\phi(1-V) \\\\\n&\\qquad\\qquad\\quad {}- \\phi(U)\\phi(1-V) - \\phi(V)\\phi(1-U)\\big) . \\end{align*}\\]\nThis expression will factorise nicely. It’s \\[ \\operatorname{Cov}\\big(\\phi(U), \\phi(U')\\big) = \\tfrac12 \\Exg \\big(\\phi(U)-\\phi(V)\\big)\\big(\\phi(1-U) - \\phi(1-V)\\big) . \\]\nWe now claim that this expectation is negative. In fact, we have a stronger result: \\[\\big(\\phi(U) - \\phi(V)\\big)\\big(\\phi(1-U) - \\phi(1-V)\\big) \\tag{7.1}\\] is always negative, so its expectation certainly is. To see this, think separately of the two cases \\(U \\leq V\\) and \\(V \\leq U\\).\n\nIf \\(U \\leq V\\), then \\(\\phi(U) \\leq \\phi(V)\\) too, since \\(\\phi\\) is increasing. But, also this means that \\(1-U \\geq 1-V\\), so \\(\\phi(1 - U) \\geq \\phi(1-V)\\). This means that, in Equation 7.1, the first term is negative and the second term is positive, so the product is negative.\nIf \\(V \\leq U\\), then \\(\\phi(V) \\leq \\phi(U)\\) too, since \\(\\phi\\) is increasing. But, also this means that \\(1-V \\geq 1-U\\), so \\(\\phi(1 - V) \\geq \\phi(1-U)\\). This means that, in Equation 7.1, the first term is positive and the second term is negative, so the product is negative.\n\nThis completes the proof.\n\nNext time: We come to the third, and most important, variance reduction scheme: importance sampling.\n\nSummary:\n\nThe antithetic variables estimator is unbiased and has mean-square error \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{AV}}\\big) = \\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X) + \\phi(X')\\big) = \\frac{1+\\rho}{n}\\Var\\big(\\phi(X)\\big). \\]\nIf \\(U \\sim \\operatorname{U}[0, 1]\\) and \\(\\phi\\) is monotonically increasing, then \\(\\phi(U)\\) and \\(\\phi(1-U)\\) are negatively correlated.\n\nOn Thursday’s lecture, we will be discussing your answers to Problem Sheet 1.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Antithetic variables II</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html",
    "href": "lectures/L08-is-1.html",
    "title": "8  Importance sampling I",
    "section": "",
    "text": "8.1 Sampling from other distributions\nSo far, we have looked at estimating \\(\\Exg \\phi(X)\\) using samples \\(X_1, X_2, \\dots, X_n\\) that are from the same distribution as \\(X\\). Importance sampling is based on the idea of taking samples \\(Y_1, Y_2, \\dots, Y_n\\) from some different distribution \\(Y\\), but then making an appropriate adjustment, so that we’re still estimating \\(\\Exg \\phi(X)\\).\nWhy might we want to do this? There are two main reasons:\nConsider, for example, trying to estimate \\(\\Exg\\phi(X)\\) where \\(X\\) is uniform on \\([0, 20]\\) and \\(\\phi\\) is the function shown below.\nCode for drawing this graph\nphi &lt;- function(x) sin(5 * x) / (5 * x)\ncurve(\n  phi, n = 10001, from = 0, to = 20,\n  lwd = 3, col = \"blue\",\n  xlab = \"x\", ylab = expression(phi(x)), ylim = c(-0.2, 1)\n)\nabline(h = 0)\nWe can see that what happens for small \\(x\\) – say, for \\(x\\) between 0 and 2, or so – will have an important effect on the value of \\(\\Exg \\phi(X)\\), because that where \\(\\phi\\) has the biggest (absolute) values. But what happens for large \\(x\\) – say for \\(x \\geq 10\\) or so – will be much less important for estimating \\(\\Exg\\phi(X)\\). So it seems wasteful to have all values in \\([0, 20]\\) to be sampled equally, and it would seem to make sense to take more samples from small values of \\(x\\).\nThis is all very well in practice, but how exactly should we down-weight those over-sampled areas?\nThink about estimating \\(\\Exg \\phi(X)\\). Let’s assume that \\(X\\) is continuous with probability density function \\(f\\). (Throughout this lecture and the next, we will assume all our random variables are continuous. The arguments for discrete random variables are very similar – just swap probability density functions with probability mass functions and integrals with sums. You can fill in the details yourself, if you like.) Then we are trying to estimate \\[ \\Exg \\phi(X) = \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\int_{-\\infty}^{+\\infty} \\phi(y)\\,f(y)\\,\\mathrm{d}y . \\] (In the second equality, we merely changed the “dummy variable” from \\(x\\) to \\(y\\), as we are at liberty to do.)\nNow suppose we sample from some other continuous distribution \\(Y\\), with PDF \\(g\\). If we estimate \\(\\Exg \\psi(Y)\\), say, for some function \\(\\psi\\), then we are estimating \\[\\Exg \\psi(Y) = \\int_{-\\infty}^{+\\infty} \\psi(y)\\,g(y) \\, \\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\psi(x)\\,g(x) \\, \\mathrm{d}x . \\]\nBut we want to be estimating \\(\\Exg\\phi(X)\\), not \\(\\Exg\\psi(Y)\\). So we will need to pick \\(\\psi\\) such that \\[ \\Exg \\phi(X) = \\int_{-\\infty}^{+\\infty} \\phi(y)\\,f(y)\\,\\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\psi(y)\\,g(y) \\, \\mathrm{d}y = \\Exg \\psi(Y) . \\] So we need to pick \\(\\psi\\) such that \\(\\phi(y)\\,f(y) = \\psi(y)\\,g(y)\\). That means that we should take \\[\\psi(y) = \\frac{\\phi(y) f(y)}{g(y)} = \\frac{f(y)}{g(y)}\\,\\phi(y). \\]\nSo we could build a Monte Carlo estimate for \\(\\Exg \\phi(X)\\) instead as a Monte Carlo estimate for \\[ \\Exg \\psi(Y) = \\Exg \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\]\nThere is one other thing: we need to be careful of division by \\(0\\) errors. So we should make sure that \\(g\\) is only 0 when \\(f\\) is 0. In other words, if it’s possible for \\(X\\) to take some value, then it must be possible for \\(Y\\) to take that value too.\nWe are finally ready to define our estimator.\nWe can think of this as taking a weighted mean of the \\(\\phi(Y_i)\\)s, where the weights are \\(f(Y_i)/g(Y_i)\\). So if a value \\(y\\) is more likely under \\(Y\\) than under \\(X\\), then \\(g(y)\\) is big compared to \\(f(y)\\), so \\(f(y)/g(y)\\) is small, and \\(y\\) gets a low weight. If a value \\(y\\) is less likely under \\(Y\\) than under \\(X\\), then \\(g(y)\\) is small compared to \\(f(y)\\), so \\(f(y) / g(y)\\) is big, and it gets a high weight. Thus we see that the weighting compensates for values that are likely to be over- or under-sampled.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html#sampling-from-other-distributions",
    "href": "lectures/L08-is-1.html#sampling-from-other-distributions",
    "title": "8  Importance sampling I",
    "section": "",
    "text": "First, we might not be able to sample from \\(X\\), so we might be forced into sampling from some other distribution \\(Y\\) instead. So far, \\(X\\) has always been a nice pleasant distribution, like a normal, exponential or continuous uniform distribution, for which we can use R’s built-in sampling function. But what if \\(X\\) were instead a very unusual or awkward distribution? In that case, we might not be able to sample directly from \\(X\\), so would be forced into sampling from a different distribution.\nSecond, we might prefer to sample from a distribution other than \\(Y\\). This might be the case if \\(\\phi(x)\\) varies a lot over different values of \\(x\\). There might be some areas of \\(x\\) where it’s very important to get an accurate estimation, because they contribute a lot to \\(\\Exg\\phi(X)\\), so we’d like to “oversample” (take lots of samples) there; meanwhile, other areas of \\(x\\) where it is not very important to get an accurate estimation, because they contribute very little to \\(\\Exg\\phi(X)\\), so we don’t mind “undersampling” (taking relatively few samples) there. Then we could sample instead from a distribution \\(Y\\) that concentrates on the most important areas for \\(\\phi\\); although we’ll need to make sure to adjust our estimator by “down-weighting” the places that we have oversampled.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 8.1 Let \\(X\\) be a continuous random variable with probability density function \\(f\\), let \\(\\phi\\) be a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(Y\\) be a continuous random variable with probability desnity function \\(g\\), where \\(g(y) &gt; 0\\) for all \\(y\\) where \\(f(y) &gt; 0\\). Then the importance sampling Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(Y_i)}{g(Y_i)}\\, \\phi(Y_i)   ,\\] where \\(Y_1, Y_2, \\dots, Y_n\\) are independent random samples from \\(Y\\).",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html#example",
    "href": "lectures/L08-is-1.html#example",
    "title": "8  Importance sampling I",
    "section": "8.2 Example",
    "text": "8.2 Example\n\nExample 8.1 Let \\(X \\sim \\operatorname{N}(0,1)\\) be a standard normal. Suppose we want to estimate \\(\\mathbb P(X &gt; 4)\\). We could do this the standard Monte Carlo way by sampling from \\(X\\) itself. \\[ \\widehat{\\theta}_n^{\\mathrm{MS}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb I_{[4,\\infty)}(X_i) . \\]\nHowever, this will not be a good estimator. To see the problem, lets run this with \\(n = 100\\,000 = 10^5\\) samples, but do it 10 times, and see what all the estimates are.\n\nn &lt;- 1e5\nMCest &lt;- rep(0, 10)\nfor (i in 1:10) MCest[i] &lt;- mean(rnorm(n) &gt; 4)\nMCest\n\n [1] 2e-05 3e-05 3e-05 5e-05 3e-05 3e-05 5e-05 4e-05 3e-05 2e-05\n\n\nWe see a big range of values. I get different results each time I run it, but anything between \\(1 \\times 10^{-5}\\) and \\(8 \\times 10^{-5}\\), and even \\(0\\), comes out fairly regularly as the estimate. The problem is that \\(X &gt; 4\\) is a very rare event – it only comes out a handful of times (perhaps 0 to 8) out of the 100,000 samples. This means our estimate is (on average) quite inaccurate.\nIt would be better not to sample from \\(X\\), but rather to sample from a distribution that is greater than 4 a better proportion of the time. We could try anything for this distribution \\(Y\\), but to keep things simple, I’m going to stick with a normal distribution with standard deviation 1. I’ll want to increase the mean, though, so that we sample values bigger than 4 more often. Let’s try importance sampling with \\(Y \\sim \\operatorname{N}(4,1)\\).\nThe PDFs of \\(X \\sim \\operatorname{N}(0,1)\\) and \\(Y \\sim \\operatorname{N}(4,1)\\) are \\[f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 x^2\\big) \\qquad g(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-4)^2\\big) , \\] so the relevant weighting of a sample \\(y\\) is \\[ \\frac{f(y)}{g(y)} = \\frac{\\exp\\big(-\\tfrac12 y^2\\big)}{\\exp\\big(-\\tfrac12 (y-4)^2\\big)} = \\exp \\big(\\tfrac12\\big(-y^2 + (y-4)^2\\big)\\big) = \\exp(-4y+8) .  \\] So our importance sampling estimate will be \\[ \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\mathrm{e}^{-4Y_i +8} \\, \\mathbb I_{[4,\\infty)}(Y_i) . \\]\nLet’s try this in R. Although we could use the function \\(\\mathrm{e}^{-4y+8}\\) for the weights, I’ll do this by using the ratios of the PDFs directly in R (just in case I made a mistake…).\n\nn &lt;- 1e5\npdf_x &lt;- function(y) dnorm(y, 0, 1)\npdf_y &lt;- function(y) dnorm(y, 4, 1)\nsamples_y &lt;- rnorm(n, 4, 1)\nISest &lt;- mean((pdf_x(samples_y) / pdf_y(samples_y)) * (samples_y &gt; 4))\nISest\n\n[1] 3.147931e-05",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L08-is-1.html#errors-in-importance-sampling",
    "href": "lectures/L08-is-1.html#errors-in-importance-sampling",
    "title": "8  Importance sampling I",
    "section": "8.3 Errors in importance sampling",
    "text": "8.3 Errors in importance sampling\nThe following theorem should not by now be a surprise.\n\nTheorem 8.1 Let \\(X\\) be a continuous random variable with probability density function \\(f\\), let \\(\\phi\\) be a function, and write \\(\\theta = \\Exg\\phi(X)\\). Let \\(Y\\) another continuous random variable with probability density function with probability density function \\(g\\), such that \\(g(y) = 0\\) only when \\(f(y) = 0\\). Let \\[ \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(Y_i)  \\] be the importance sampling Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is \\[ \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right). \\]\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\]\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{IS}}\\) is \\[ \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{\\sqrt{n}} \\,\\sqrt{\\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right)}. \\]\n\n\n\nProof. Part 1 follows essentially the same argument as our discussion at the beginning of this lecture. We have \\[ \\Ex \\left( \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(Y_i) \\right) = \\frac{1}{n}\\, n\\, \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) . \\] But \\[ \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\int_{-\\infty}^{+\\infty} \\frac{f(y)}{g(y)}\\,\\phi(y)\\,g(y)\\,\\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\phi(y) \\, f(y) \\, \\mathrm{d}y = \\Exg \\phi(X) . \\] This last step is because \\(f\\) is the PDF of \\(X\\); it doesn’t matter whether the dummy variable in the integration is \\(x\\) or \\(y\\). Hence the estimator is unbiased.\nParts 2 to 4 follow in the usual way.\n\nAs we are now used to, we can estimate the variance using the sample variance.\n\nExample 8.2 We continue Example 8.1, where we are estimating \\(\\mathbb P(X &gt; 4)\\) for \\(X \\sim \\operatorname{N}(0,1)\\).\nFor the standard Monte Carlo method, we estimate the root-mean-square error as\n\nn &lt;- 1e5\nMC_MSE &lt;- var(rnorm(n) &gt; 4) / n\nsqrt(MC_MSE)\n\n[1] 1.99997e-05\n\n\nAs before, this still varies a lot, but it seems to usually be about \\(2 \\times 10^{-5}\\).\nFor the importance sampling method, we estimate the mean-square error as\n\nn &lt;- 1e5\npdf_x &lt;- function(x) dnorm(x, 0, 1)\npdf_y &lt;- function(y) dnorm(y, 4, 1)\nsamples_y &lt;- rnorm(n, 4, 1)\nIS_MSE &lt;- var((pdf_x(samples_y) / pdf_y(samples_y)) * (samples_y &gt; 4)) / n\nsqrt(IS_MSE)\n\n[1] 2.118229e-07\n\n\nThis is about \\(2 \\times 10^{-7}\\). This is about 100 times smaller than for the standard method: equivalent to taking about 10,000 times as many samples! That’s a huge improvement, which demonstrates the power of importance sampling.\n\nNext time: We continue our study of importance sampling – and complete our study of Monte Carlo estimation, for now – by considering how to pick a good distribution \\(Y\\).\n\nSummary:\n\nImportance sampling estimates \\(\\Exg \\phi(X)\\) by sampling from a different distribution \\(Y\\).\nThe importance sampling estimator is \\({\\displaystyle \\widehat{\\theta}_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(Y_i)}\\).\nThe importance sampling estimator is unbiased with mean-square error \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\]\n\nSolutions are now available for Problem Sheet 1.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Importance sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html",
    "href": "lectures/L09-is-2.html",
    "title": "9  Importance sampling II",
    "section": "",
    "text": "9.1 Picking a good distribution\nWe’ve now seen that importance sampling can be a very powerful tool, when used well. But how should pick a good distribution \\(Y\\) to sample from?\nLet’s examine the mean-square error more carefully: \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{IS}}\\big) = \\frac{1}{n} \\operatorname{Var}\\left( \\frac{f(Y)}{g(Y)}\\,\\phi(Y) \\right) . \\] So our goal is to try and pick \\(Y\\) such that \\(\\frac{f(Y)}{g(Y)}\\phi(Y)\\) has low variance. (We also, of course, want to be able to sample from \\(Y\\).)\nThe best possible choice, then, would be to pick \\(Y\\) such that \\(\\frac{f(Y)}{g(Y)}\\phi(Y)\\) is constant – and therefore has zero variance! If \\(\\phi\\) is non-negative, then it seems like we should pick \\(Y\\) such that its probability density function is \\(g(y) \\propto f(y)\\phi(y)\\). (Here, \\(\\propto\\) is the “proportional to” symbol.) That is, to have \\[ g(y) = \\frac{1}{Z} f(y)\\phi(y) , \\] for some constant \\(Z\\). Then \\(\\frac{f(Y)}{g(Y)}\\phi(Y) = Z\\) is a constant, has zero variance, and we have a perfect estimator!\nWhat is this constant \\(Z\\)? Well, \\(g\\) is a PDF, so it has to integrate. So we will need to have \\[ 1 = \\int_{-\\infty}^{+\\infty} g(y)\\, \\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\frac{1}{Z} f(y)\\phi(y) \\, \\mathrm{d}y = \\frac{1}{Z} \\int_{-\\infty}^{+\\infty} f(x)\\phi(x) \\, \\mathrm{d}x = \\frac{1}{Z} \\Exg\\phi(X) . \\] (We did the “switching the dummy variable from \\(y\\) to \\(x\\)” thing again.) So \\(Z = \\Exg \\phi(X)\\). But that’s no good: \\(\\theta = \\Exg \\phi(X)\\) was the thing we were trying to estimate in the first place. If we knew that, we wouldn’t have to do Monte Carlo estimation to start with!\nSo, as much as we would like to, we can’t use this “perfect” ideal distribution \\(Y\\). More generally, if \\(\\phi\\) is not always non-negative, it can be shown that \\(g(y) \\propto f(y)\\,|\\phi(y)| = |f(x)\\,\\phi(x)|\\) would be the best possible distribution, but this has the same problems.\nHowever, we can still be guided by this idea – we would like \\(g(y)\\) to be as close to proportional to \\(f(y) \\phi(y)\\) (or \\(|f(y) \\phi(y)|\\)) as we can manage, so that \\(\\frac{f(y)}{g(y)}\\phi(y)\\) is close to being constant, so hopefully has low variance. This tells us that \\(Y\\) should be likely – that is, \\(g(y)\\) should be big – where both \\(f\\) and \\(|\\phi|\\) are both big – that is, where \\(X\\) is likely and also \\(\\phi\\) is big in absolute value. While \\(Y\\) should be unlikely where both \\(X\\) is unlikely and \\(\\phi\\) is small in absolute value.\nAside from the exact theory, in the absence of any better idea, choosing \\(Y\\) to be “in the same distribution family as \\(X\\) but with different parameters” is often a reasonable thing to try. For example:",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html#picking-a-good-distribution",
    "href": "lectures/L09-is-2.html#picking-a-good-distribution",
    "title": "9  Importance sampling II",
    "section": "",
    "text": "Example 9.1 Let’s look again at Example 8.1 (continued in Example 8.2), where we wanted to estimate \\(\\mathbb P(X &gt; 4) = \\Exg\\Ind_{(4,\\infty)}(X)\\) for \\(X \\sim \\operatorname{N}(0,1)\\). We found are estimator was enormously improved when we used instead \\(Y \\sim \\operatorname{N}(4,1)\\).\nIn the figure below, the blue line is \\[f(y)\\,\\phi(y) = f(y)\\,\\Ind_{(4,\\infty)}(y) = \\begin{cases} \\displaystyle\\frac{1}{\\sqrt{2\\pi}} \\,\\mathrm{e}^{-y^2/2} & y &gt; 4 \\\\ 0 & y \\leq 4 \\end{cases} \\] (scaled up, otherwise it would be so close to the axis line you wouldn’t see it).\nThe black line is the PDF \\(f(y)\\) of the original distribution \\(X \\sim \\operatorname{N}(0,1)\\), while the red line is the PDF \\(g(y)\\) of our importance distribution \\(Y \\sim \\operatorname{N}(4,1)\\).\n\n\nCode for drawing this graph\ncurve(\n  dnorm(x, 0, 1), n = 1001, from = -2.5, to = 7.5,\n  col = \"black\", lwd = 2,\n  xlim = c(-2, 7), xlab = \"y\", ylim = c(0, 0.7), ylab = \"\"\n)\ncurve(\n  dnorm(x, 4, 1), n = 1001, from = -2.5, to = 7.5,\n  add = TRUE, col = \"red\", lwd = 3,\n)\ncurve(\n  dnorm(x, 0, 1) * (x &gt; 4) * 5000, n = 1001, from = -2.5, to = 7.5,\n  add = TRUE, col = \"blue\", lwd = 3\n)\nlegend(\n  \"topright\",\n  c(expression(paste(\"f(y)\", varphi, \"(y) [scaled]\")), \"N(0, 1)\", \"N(4, 1)\"),\n  lwd = c(3, 2, 3), col = c(\"blue\", \"black\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nWe have noted that a good distribution will have a PDF that is big when \\(f(x)\\phi(x)\\) (the blue line) is big. Clearly the red line is much better at this then the black line, which is why the importance sampling method was so much better here.\nThere’s scope to do better here, though. Perhaps an asymmetric distribution with a much more quickly-decaying left-tail might be good – for example, a shifted exponential \\(4 + \\operatorname{Exp}(\\lambda)\\) might be worth investigating. Or a thinner, spikier distribution, such as a normal with smaller standard deviation. In both cases, though, we have to be careful – because it’s the ratio \\(f(y)/g(y)\\), we still have to be a bit careful about what happens when both \\(f(y)\\) and \\(g(y)\\) are small absolutely, in case one is proportionally much bigger than the other.\n\n\n\nIf \\(X \\sim \\operatorname{N}(\\mu, \\sigma^2)\\), then try \\(Y \\sim \\operatorname{N}(\\nu, \\sigma^2)\\) for some other value \\(\\nu\\).\nIf \\(X \\sim \\operatorname{Exp}(\\lambda)\\), then try \\(Y \\sim \\operatorname{Exp}(\\mu)\\) for some other value \\(\\mu\\).",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html#bonus-example",
    "href": "lectures/L09-is-2.html#bonus-example",
    "title": "9  Importance sampling II",
    "section": "9.2 Bonus example",
    "text": "9.2 Bonus example\n\nExample 9.2 Let \\(X \\sim \\operatorname{U}[0, 10]\\) be an uniform distribution, so \\(f(x) = \\frac{1}{10}\\) for \\(0 \\leq x \\leq 10\\), and let \\(\\phi(x) = \\mathrm{e}^{-|x-8|}\\). Estimate \\(\\Exg\\phi(X)\\).\nThe standard Monte Carlo estimator and its RMSE are as follows\n\nphi &lt;- function(x) exp(-abs(x - 8))\nn &lt;- 1e6\nsamples &lt;- runif(n, 0, 10)\nMCest &lt;- mean(phi(samples))\nMC_MSE &lt;- var(phi(samples)) / n\nc(MCest, sqrt(MC_MSE))\n\n[1] 0.1863397228 0.0002534953\n\n\nMaybe we can improve on this using importance sampling. Let’s have a look at a graph of \\(f(y)\\,\\phi(y)\\) (blue line).\n\n\nCode for drawing this graph\ncurve(\n  dunif(x, 0, 10) * exp(-abs(x - 8)), n = 1001, from = 0, to = 10,\n  col = \"blue\", lwd = 3,\n  xlim = c(0, 10), xlab = \"y\", ylab = \"\"\n)\ncurve(\n  dnorm(x, 8, 2)*0.36, n = 1001, from = -1, to = 11,\n  add = TRUE, col = \"red\", lwd = 2,\n)\nlegend(\n  \"topleft\",\n  c(expression(paste(\"f(y)\", varphi, \"(y)\")), \"N(8, 4) [scaled]\"),\n  lwd = c(3, 2), col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nAfter some experimentation, I decided that \\(Y \\sim \\operatorname{N}(8, 2^2)\\) (red line; not to same scale) seemed to work quite well, in \\(g(y)\\) being roughly proportional to \\(f(y)\\,\\phi(y)\\). I was tempted to take the standard deviation less than 2, the get the red curve a bit tighter. But I discovered that \\(f(y)/g(y)\\) got very large for \\(y\\) near 0, due to the left tail of \\(g\\) decaying too quickly. The following graph shows \\(\\frac{f(y)}{g(y)} \\phi(y)\\), and shows that the spike at 0 is now under control compared to the “main spike” at \\(y = 8\\).\n\n\nCode for drawing this graph\ncurve(\n  dunif(x, 0, 10) * exp(-abs(x - 8)) / dnorm(x, 8, 2), n = 1001, from = -2, to = 12,\n  col = \"red\", lwd = 3,\n  xlim = c(-1, 11), xlab = \"y\", ylab = \"\"\n)\n\n\n\n\n\n\n\n\n\nSo our importance sampling estimate is as follows.\n\nphi &lt;- function(x) exp(-abs(x - 8))\npdf_x &lt;- function(x) dunif(x, 0, 10)\npdf_y &lt;- function(y) dnorm(y, 8, 2)\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n, 8, 2)\nISest &lt;- mean((pdf_x(samples) / pdf_y(samples)) * phi(samples))\nIS_MSE &lt;- var((pdf_x(samples) / pdf_y(samples)) * phi(samples)) / n\nc(ISest, sqrt(IS_MSE))\n\n[1] 0.1864660680 0.0001351119\n\n\nWe see that the RMSE has roughly halved, which is the equivalent of taking four times as many samples.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L09-is-2.html#summary-of-part-i",
    "href": "lectures/L09-is-2.html#summary-of-part-i",
    "title": "9  Importance sampling II",
    "section": "9.3 Summary of Part I",
    "text": "9.3 Summary of Part I\nThis is our last lecture on Monte Carlo estimation – at least for now, and at least in its standard form. So let’s end this section of the module by summarising the estimators we have learned about. We have been learning how to estimate \\(\\theta = \\Exg \\phi(X)\\)\n\nThe standard Monte Carlo estimator simple takes a sample mean of \\(\\phi(X_i)\\), where \\(X_i\\) are independent random samples from \\(X\\).\nThe control variate Monte Carlo estimator “anchors” the estimator to some known value \\(\\eta = \\Exg \\psi(X)\\), for a function \\(\\psi\\) that is “similar to \\(\\phi\\), but easier to calculate the expectation exactly”.\nThe antithetic variables Monte Carlo estimator uses pairs of samples \\((X_i, X'_i)\\) that both have the same distribution as \\(X\\), but where \\(\\phi(X)\\) and \\(\\phi(X')\\) have negative correlation \\(\\rho &lt; 0\\).\nThe importance sampling Monte Carlo estimator samples not from \\(X\\), with PDF \\(f\\), but from a different distribution \\(Y\\), with PDF \\(Y\\). The distribution \\(Y\\) is chosen to oversample from the most important values, but then gives lower weight to those samples.\n\n\n\n\n\n\n\n\n\n\nEstimator\nMSE\n\n\n\n\nStandard Monte Carlo\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i)}\\)\n\\({\\displaystyle\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\)\n\n\nControl variate\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big)} + \\eta\\)\n\\({\\displaystyle\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X_i) - \\psi(X_i)\\big)}\\)\n\n\nAntithetic variables\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X_i')\\big)}\\)\n\\({\\displaystyle\\frac{1}{2n} \\operatorname{Var}\\big(\\phi(X_i) + \\phi(X_i')\\big)}\\) \\({\\displaystyle {}= \\frac{1+\\rho}{n} \\,\\operatorname{Var}\\big(\\phi(X)\\big)}\\)\n\n\nImportance sampling\n\\({\\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)}\\,\\phi(X_i)}\\)\n\\({\\displaystyle\\frac{1}{n} \\operatorname{Var}\\left(\\frac{f(Y_i)}{g(Y_i)}\\,\\phi(X_i)\\right)}\\)\n\n\n\n \nNext time: We begin the second section of the module, on random number generation.\n\nSummary:\n\nA good importance sampling distribution \\(Y\\) is one whose PDF \\(g(y)\\) is roughly proportional to \\(|f(y)\\,\\phi(y)|\\). Equivalently, \\(\\frac{f(y)}{g(y)}|\\phi(y)|\\) is approximately constant.\n\nYou should now be able to answer Questions 1–3 on Problem Sheet 2.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Importance sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L10-rng-intro.html",
    "href": "lectures/L10-rng-intro.html",
    "title": "10  Generating random numbers",
    "section": "",
    "text": "10.1 Why generate random numbers?\nSo far in this module, we have made a lot of use of generating random numbers – for us, this has been when performing Monte Carlo estimation. We will also need random numbers for many other purposes later in the module. There are lots of other situations in statistics, mathematics and data science where we want to use random numbers to help us solve problems. But where do we get these random numbers from?\nWhen performing Monte Carlo estimation, we have used lots of samples from the distribution of some random variable \\(X\\). We did this using R’s built-in functions for random number generation, like runif(), rnorm(), rexp(), and so on.\nIn this part of the module, we are interested in these questions:\nIt turns out that this will break down into two questions that we can treat largely separately:",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generating random numbers</span>"
    ]
  },
  {
    "objectID": "lectures/L10-rng-intro.html#why-generate-random-numbers",
    "href": "lectures/L10-rng-intro.html#why-generate-random-numbers",
    "title": "10  Generating random numbers",
    "section": "",
    "text": "How do these random number generation functions in R actually work?\nWhat if we want to sample from a distribution for which R doesn’t have a built-in function – how can we do that?\n\n\n\nHow do we generate some randomness – any randomness – in the first place? We usually think of this as generating \\(U \\sim \\operatorname{U}[0,1]\\), a uniform random number between 0 and 1. (We will look at this today and in the next lecture.)\nHow do we transform that uniform randomness \\(U\\) to get it to behave like the particular distribution \\(X\\) that we want to sample from? (We will look at this in Lectures 12–16.)",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generating random numbers</span>"
    ]
  },
  {
    "objectID": "lectures/L10-rng-intro.html#random-numbers-on-computers",
    "href": "lectures/L10-rng-intro.html#random-numbers-on-computers",
    "title": "10  Generating random numbers",
    "section": "10.2 Random numbers on computers",
    "text": "10.2 Random numbers on computers\nWe start by considering the question of how to generate a uniform random number between 0 and 1.\nThe first thing to know is that computers do not perfectly store exact real numbers in decimals of unending length – that’s impossible! Instead, it stores a number to a certain accuracy, in terms of the number of decimal places. To be more precise, computers store numbers in binary; that is, written as a sequence of 0s and 1s. (In the presentation here, we will somewhat simplify matters – computer science experts will be able to spot where I’m lying, or “gently smoothing out the truth”.)\nA number between 0 and 1 could be (approximately) stored as a 32-bit binary number, for example. A 32-bit number is a number like\n0. 00110100 11110100 10001111 10011001\nthat is “0.” followed by a string of 32 binary digits, or “bits” (0s and 1s). A string \\(0.x_1x_2\\cdots x_{31}x_{32}\\) represents the number \\[ x = \\sum_{i=1}^{32} x_i 2^{-i} . \\] So the number above represents \\[ \\begin{align} &0\\times 2^{-1} + 0 \\times 2^{-2} + 1 \\times 2^{-3} + \\cdots + 0 \\times 2^{-31} + 1 \\times 2^{-32} \\\\ &\\qquad\\qquad {}=2^{-3} + 2^{-4} + 2^{-6} + \\dots + 2^{-29} + 2^{-32} \\\\ &\\qquad\\qquad {}= 0.20685670361854135990142822265625.\\end{align}\\]\nWe could generate such a 32-bit (or, more generally, \\(b\\)-bit) number in two different ways:\n\nA sequence of 32 0s and 1s (each being 50:50 likely to be 0 or 1 and each being independent of the others). This then represents a number in its binary expansion, as above.\n\nOr, more generally, we want a string of \\(b\\) 0s and 1s.\n\nA integer at random between \\(0\\) and \\(2^{32} - 1\\), which we can then divide by \\(2^{32}\\) to get a number between \\(0\\) and \\(1\\).\n\nOr, more generally, we want a random integer between \\(0\\) and \\(m - 1\\) for some \\(m\\), which we can then divide by \\(m\\). Usually \\(m = 2^b\\) for some \\(b\\), to give a \\(b\\)-bit number.\n\n\nThere are two ways we can do this. First, we can use true physical randomness. Second, we can use computer-generated “pseudorandomness”.\nTrue physical randomness means randomness by some real-life random process. For example, you could toss a coin 32 times, and write down “1” each time it lands heads and “0” each time it lands tails: this would give a random 32-bit number. While this will be genuinely random, it is, of course, very slow if you need a large amount of randomness. For example, when we have done Monte Carlo estimation, we have often used one million random numbers in about 1 second – it’s obviously completely infeasible to toss 32 million coins that quickly!\nFor a greater amount of physical randomness more quickly, one could look at times between the decay of radioactive particles, thermal noise in electrical circuits, the behaviour of photons in a laser beam, and so on. But these are quite expensive, and even these may not be quick enough for some applications.\nHere’s a cool video by the YouTuber Tom Scott about an internet company that uses a wall of lava lamps for their true physical randomness:",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generating random numbers</span>"
    ]
  },
  {
    "objectID": "lectures/L10-rng-intro.html#prngs",
    "href": "lectures/L10-rng-intro.html#prngs",
    "title": "10  Generating random numbers",
    "section": "10.3 PRNGs",
    "text": "10.3 PRNGs\nA pseudorandom number generator (PRNG) is a computer program that outputs a sequence of numbers that appear to be random. Of course, the numbers are not actually random – a computer program always performs exactly what you tell it to, exactly the same every time. But for a PRNG – or at least for a good PRNG – there should be no obvious patterns in the sequence of numbers produced, so they should act for all practical purposes as if they were random numbers. (“Pseudo” is a prefix that means something like “appears to be, even though it’s not”.)\nMany pseudorandom number generators work by applying a recurrence. Suppose we want (pseudo)random integers between \\(0\\) and \\(m-1\\). Then we have a seed \\(x_1\\), which behaves as a starting point for the sequence, and a function \\(f\\) from \\(\\{0, 1, \\dots, m-1\\}\\) to \\(\\{0, 1, \\dots, m-1\\}\\). Then starting from \\(x_1\\), we apply \\(f\\) to get the next number in the sequence \\(f(x_1) = x_2\\). Then we apply \\(f\\) to that, to get the next point \\(x_3 = f(x_2)\\). Then apply \\(f\\) to that to get the next number, and so on. So the sequence would be \\[ \\begin{align} x_1& & x_2 &= f(x_1) & x_3 &= f(x_2) = f(f(x_1)) \\\\\nx_4 &= f(x_3) = f(f(f(x_1))) & &\\cdots & x_{i+1} &= f(x_i) =f(f(\\cdots (f(x_1))\\cdots))\\end{align} \\] and so on.\nSome functions \\(f\\) would be not produce a very random-looking sequence of numbers: think of a silly example like \\(f(x) = (x+1) \\bmod m\\), so \\(x_{i+1} = (x_i + 1) \\bmod m\\) just increases by 1 each time (before “wrapping around” back to 0 when it gets to \\(m\\)). But mathematicians have come up with lots of examples of functions \\(f\\) which, for all possible practical purposes, seem to have outputs that appear just as random as an actual random sequence.\nOne example of such a function \\(f\\) would be \\(f(x) = (ax+c) \\bmod m\\), so \\(x_{i+1} = (ax_i + c)\\bmod m\\), which can be a very good PRNG for some values of \\(a\\) and \\(c\\). In this case, the PRNG is known as a linear congruential generator (or LCG). We’ll talk more about LCGs in the next lecture.\nR’s default method is of this form – well, it’s almost of this form. It actually uses a method called the Mersenne Twister, which uses a recurrence of the form \\(x_{i+1} = (f(x_i) + i) \\bmod m\\) for a complicated function \\(f\\); here the “plus \\(i\\)”, where \\(i\\) is the step number, means we have a slightly different update rule each timestep.\nBut how do we pick the seed – the starting point \\(x_1\\)? Normally, we use some true physical randomness to pick the seed. The benefit here is that just a small amount of true physical randomness can “start you off” by choosing the seed, and then the PRNG can produces huge amounts of pseudorandomness incredibly quickly. Indeed, that’s what the wall of lava lamps in the video did – the lava lamps were just for producing lots of seeds for the PRNGs.\nHowever, it is possible, and sometimes desirable, to set the seed “by hand”. This is useful if you want to produce the same “random-looking” numbers as someone else (or yourself, previously). This is because if two people set the same seed \\(x_1\\), then the numbers \\(x_2, x_3, \\dots\\) produced after that will still appear to be random, but because the process is completely deterministic after the seed is chosen, both people will actually produce exactly the same sequence of numbers. This can be useful for checking accuracy of code, for example.\nIn R, by default, the seed is set based the current time on your computer, measured down to about 10 milliseconds. However, you can set the seed yourself, using set.seed(). For example, the following code sets the seed to be 123456, then generates 10 uniform pseudorandom numbers.\n\nset.seed(123456)\nrunif(10)\n\n [1] 0.79778432 0.75356509 0.39125568 0.34155670 0.36129411 0.19834473\n [7] 0.53485796 0.09652624 0.98784694 0.16756948\n\n\nThose numbers certainly appear to be random. But if you run those two lines of code on your computer, you should get exactly the same 10 numbers. That’s because you will also start with the same seed 123456, and then R will run the same pseudorandom – that is, completely deterministic – function to generate the next 10 numbers.\nNext time: We’ll take a closer look at pseudorandom number generation using linear congruential generators.\n\nSummary:\n\nRandom number generation has two problems: how to generate uniform random numbers between 0 and 1, and who to convert these to your desired distribution.\nUniform random numbers can be generated from true physical randomness or using a pseudorandom number generator on a computer.\nLinear congruential generators are a type of pseudorandom number generator.\n\nRead more: Voss, An Introduction to Statistical Computing, introduction to Chapter 1, introduction to Section 1.1, and Subsection 1.1.3.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generating random numbers</span>"
    ]
  },
  {
    "objectID": "lectures/L11-lcg.html",
    "href": "lectures/L11-lcg.html",
    "title": "11  LCGs",
    "section": "",
    "text": "11.1 Definition and examples\nLast lecture we introduced the idea of pseudorandom number generators (PRNGs), which are deterministic functions that produce a sequence of numbers that look for all practical purposes as if they are random.\nWe introduced the idea of a recurrence, where we have a function \\(f\\) and start with a seed \\(x_1\\). We then produce a sequence through the recurrence \\(x_{i+1} = f(x_i)\\). So \\(x_2 = f(x_1)\\), \\(x_3 = f(x_2) = f(f(x_1))\\), and so on.\nWe briefly mentioned a class of such PRNGs called linear congruential generators, or LCGs. An LCG generates integers between \\(0\\) and \\(m-1\\) using a recurrence function of the form \\[ f(x) = (ax + c) \\bmod m , \\] so \\[ x_{i+1} = (ax_i + c) \\bmod m . \\]\nHere, “\\(\\mathrm{mod}\\ m\\)” means “modulo \\(m\\)”; that is, we are using modular arithmetic, where when we get to \\(m - 1\\) we wrap back to 0 and start again. (Modular arithmetic is sometimes called “clock arithmetic”, because hours of the day work modulo 12: for example, 3 hours after 11 o’clock is 2 o’clock, because \\(11 + 3 = 14 \\equiv 2 \\bmod m\\).)\nIn the LCG \\(x_{i+1} = (ax_i + c) \\bmod m\\):\nIs these examples, we’ve usually taken \\(m\\) to be a power of 2. There are a few reasons for this:",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LCGs</span>"
    ]
  },
  {
    "objectID": "lectures/L11-lcg.html#definition-and-examples",
    "href": "lectures/L11-lcg.html#definition-and-examples",
    "title": "11  LCGs",
    "section": "",
    "text": "\\(m\\) is called the modulus,\n\\(a\\) is called the multiplier,\n\\(c\\) is called the increment,\n\\(x_1\\), the starting point, is called the seed.\n\n\nExample 11.1 Let us look at two simple LCGs with modulus \\(m = 2^4 = 16\\).\nFirst, let \\(a = 5\\) be the multiplier, \\(c = 3\\) be the increment, and \\(x_1 = 1\\) be the seed. Then we have \\[ \\begin{align} x_2 &= (5x_1 + 3) \\bmod 16 = (5\\times 1 + 3) \\bmod 16 = 8 \\bmod 16 = 8 \\\\\nx_3 &= (5x_2 + 3) \\bmod 16 = (5\\times 8 + 3) \\bmod 16 = 43 \\bmod 16 = 11 \\\\\nx_4 &= (5x_3 + 3) \\bmod 16 = (5\\times 1 + 3) \\bmod 16 = 43 \\bmod 58 = 10 , \\end{align} \\] and so on. The sequence continues \\((1, 8, 11, 10, 5, 12, 15, 14, 9, 0, \\dots)\\). This looks pretty much like a random sequence of numbers between 0 and 15 to me – I certainly don’t see any obvious pattern.\nSecond, let \\(a = 3\\) be the multiplier, \\(c = 4\\) be the increment, and \\(x_1 = 1\\) be the seed. Then we have \\[ \\begin{align} x_2 &= (3x_1 + 2) \\bmod 16 = (3\\times 1 + 4) \\bmod 16 = 7 \\bmod 16 = 7 \\\\\nx_3 &= (3x_2 + 4) \\bmod 16 = (3\\times 7 + 4) \\bmod 16 = 25 \\bmod 16 = 9 . \\end{align} \\] But if we continue this, we find we get the sequence \\((1, 7, 9, 15, 1, 7, 9, 15, 1, 7, \\dots)\\). This does not seem like a good random sequence – we’re just getting the same pattern of four numbers \\((1, 7, 9, 15)\\) repeating over and over again.\nThe example illustrates that, while an LCG can provide a good sequence of pseudorandom numbers, we need to be careful with the parameters we choose.\n\n\nExample 11.2 Of course, it doesn’t make much sense to run LCGs by hand – the whole purpose of LCGs is that they can produce lots of (pseudo)random numbers very fast. So we should run them on computers.\nThe following R code sets up a function for sampling \\(n\\) numbers from an LCG.\n\nlcg &lt;- function(n, modulus, mult, incr, seed) {\n  samples &lt;- rep(0, n)\n  samples[1] &lt;- seed\n  for (i in 1:(n - 1)) {\n    samples[i + 1] &lt;- (mult * samples[i] + incr) %% modulus\n  }\n  return(samples)\n}\n\nIn the fourth line, %% is R’s “mod” operator.\nLet’s look at two examples with modulus \\(m = 2^8 = 256\\).\nFirst, let \\(a = 13\\) be the multiplier, \\(c = 17\\) be the increment, and \\(x_1 = 10\\) be the seed.\n\nm &lt;- 2^8\nsample1 &lt;- lcg(100, m, 13, 17, 10)\nplot(sample1)\n\n\n\n\n\n\n\n\nThat looks like a pretty random collection of numbers to me.\nSecond, we stick with \\(a = 13\\) be the multiplier and \\(x_1 = 10\\) as the seed, be decrease the increment by 1 to \\(c = 16\\).\n\nsample2 &lt;- lcg(100, m, 13, 16, 10)\nplot(sample2)\n\n\n\n\n\n\n\n\nThis doesn’t seem as good. We can see there’s some pattern where there are parallel downward sloping lines. And also there seems to be some sort of pattern within these downward sloping lines, sometimes with quite regularly-spaced points on those lines. And looking more closely, we can see that actually the pattern of numbers repeats exactly every 32 steps\n\nwhich(sample2 == 10)\n\n[1]  1 33 65 97\n\n\nso we only ever see 32 of the possible 256 values. This doesn’t seem to look like a sequence of independent uniformly random points.\nSo again, it seems like LCG can provide a good sequence of pseudorandom numbers, but it seems quite sensitive to a good choice of the parameters.\n\n\n\nWe will want to divide each term in the sequence \\(x_i\\) by \\(m\\) to get a number between \\([0,1]\\). If our number will be stored as a \\(b\\)-bit number, then it is makes sense to have created \\(b\\) bits (for an integer between \\(0\\) and \\(m = 2^{b} - 1\\)) in the first place. This means every integer in \\(\\{0, 1, \\dots, m-1\\}\\) corresponds to exactly one \\(b\\)-bit number. Further this makes the division by \\(m = 2^b\\) extremely simple: you simple add “0.” (zero point…) at the front of the number!\nModular arithmetic modulo a power of 2 is very simple for a computer. For a number in binary, the value modulo \\(2^b\\) is simply the last \\(b\\) bits of the number. So 10111001 modulo \\(2^4\\) is simply 1001, the last four bits.\nHaving \\(m = 2^b\\) (or, more generally, having \\(m\\) being the product of lots of small prime factors) makes it easier to choose parameters such that the LCG is a good pseudorandom number generator … as we shall see in the next section.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LCGs</span>"
    ]
  },
  {
    "objectID": "lectures/L11-lcg.html#periods-of-lcgs",
    "href": "lectures/L11-lcg.html#periods-of-lcgs",
    "title": "11  LCGs",
    "section": "11.2 Periods of LCGs",
    "text": "11.2 Periods of LCGs\nIn an LCG, each number in the sequence depends only on the one before, since \\(x_{i+1} = (ax_i + c) \\bmod m\\). This means if we ever get a single “repeat” in our sequence – that is, if we see a number we have seen at some point before – then the whole sequence from that point on will copy what came before.\nFor example, in the second part of Example 11.1, the sequence started \\((1, 7, 9, 15, 1, \\dots)\\). As soon as we hit that repeat 1, we know we’re going to see that pattern \\(1, 7, 9, 15\\) repeated forever. We say that this LCG has “period” 4. More generally, the period of an LCG is the smallest \\(k \\geq 1\\) such that \\(x_{i+k} = x_i\\) for some \\(i\\).\nWe would like our LCGs to have a big period. If an LCG has a small period, it will not look random, as we will just repeat the same small number of values over and over again.\nThe smallest possible period is 1. That would be an extraordinarily bad LCG, as it would just spit out the same number forever. The biggest possible period is \\(m\\). That is because there are only \\(m\\) possible values in \\(\\{0, 1, \\dots, m-1\\}\\); so after \\(m+1\\) steps we must have had a repeat, by the pigeonhole principle. Having the maximum period \\(m\\) is also called having “full period”.\nGenerally, the only way to find the period of an LCG is to run it for a long time, and see how long it takes to start repeating. But, conveniently, there is a very easy way to tell if an LCG has the maximum possible period \\(m\\), thanks to a result of TE Hull and AR Dobell.\n\nTheorem 11.1 (Hull–Dobell theorem) Consider the linear congruential generator \\(x_{i+1} = (ax_i + c) \\bmod m\\). This LCG has period \\(m\\) if and only if the following three conditions hold:\n\n\\(m\\) and \\(c\\) are coprime;\n\\(a - 1\\) is divisible by all prime factors of \\(m\\);\nif \\(m\\) is divisible by 4, then \\(a - 1\\) is divisible by 4.\n\n\n(We won’t prove this here – it’s some pretty tricky number theory. But see Knuth, The Art of Computer Programming, Volume 2: Seminumerical algorithms, Subsubsection 3.2.1.2 if you really want a proof and have the number theory chops for it.)\nIf \\(m = 2^b\\) is a power of 2 (with \\(b \\geq 2\\)), then the three conditions simplify to a particularly pleasant form:\n\n\\(c\\) is odd;\n\\(a - 1\\) is even;\n\\(a - 1\\) is divisible by 4.\n\nOf course, the third point on the list implies the second. So we only need to check two things: that \\(c\\) is odd and that \\(a\\) is \\(1 \\bmod 4\\) (that is, \\(a\\) is one more than a multiple of 4).\n\nExample 11.3 Let’s go back to the earlier examples, and see if they have full periods or not.\nIn the first LCG of Example 11.1, we had \\(m = 2^4\\), \\(a = 5\\) and \\(c = 3\\). Here, \\(c\\) is odd, and \\(a = 4 + 1\\) is \\(1 \\bmod 4\\). This LCG fulfils both conditions, so it has the maximum possible period of 16.\nIn the second LCG of Example 11.1, we had \\(m = 2^4\\), \\(a = 3\\) and \\(c = 4\\). Here, \\(c\\) is even, and \\(a\\) is \\(3 \\bmod 4\\). This LCG does not fulfil both the conditions – in fact, it fails them both – so it does not have the maximum possible period of 16. (We already saw that it in fact has period 4.)\nIn the first LCG of Example 11.2, we had \\(m = 2^8\\), \\(a = 13\\) and \\(c = 17\\). Here, \\(c\\) is odd, and \\(a = 12 + 1\\) is \\(1 \\bmod 4\\). This LCG fulfils both conditions, so it has the maximum possible period of 256.\nIn the second LCG of Example 11.2, we had \\(m = 2^8\\), \\(a = 13\\) and \\(c = 16\\). Here, \\(c\\) is even, and \\(a = 12 + 1\\) is \\(1 \\bmod 4\\). So although this LCG does fulfil the second condition, it does not fulfil the first, so it does not have the maximum possible period of 256. (We already saw that it in fact has period 32.)\n\nIt normally a good idea to make sure your LCG has full period – if it’s so easy to ensure, then why not? (That said, a very large but not-quite-maximum period may be good enough. For example, if an LCG with modulus \\(2^{64}\\) has a period of “only” \\(2^{60}\\), that might well be enough: one million samples a second for one thousand years is only \\(2^{55}\\) samples, so you’d never actually see a repeat.)\nBut merely having full period (and a large modulus) isn’t enough by itself to guarantee an LCG will make a good pseudorandom number generator. After all, the silly LCG \\(x_{i+1} = x_i + 1\\) has full period, but the sequence \\[(0, 1, 2, 3, \\dots, m-2, m-1, 0, 1, 2, \\dots)\\] will not look random.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LCGs</span>"
    ]
  },
  {
    "objectID": "lectures/L11-lcg.html#statistical-testing",
    "href": "lectures/L11-lcg.html#statistical-testing",
    "title": "11  LCGs",
    "section": "11.3 Statistical testing",
    "text": "11.3 Statistical testing\nBefore being used properly, any pseudorandom number generator is subjected to a barrage of statistical tests, to check if its output seems to “look random”. Alongside checking it has a very large period, the tester will want to check other statistical properties of randomness.  Even the best PRNGs might not pass every single such test. See Voss, An Introduction to Statistical Computing, Subsection 1.1.2 for more on this.\nLCGs were considered state-of-the-art until the late-90s or so. However, it was discovered that \\(m\\) needs be very big and the number of samples used fairly small in order to pass some of the more stringent statistical tests of randomness. For example, it’s suggested that \\(n = 1\\,000\\,000\\) (one million) samples from a full-period LCG with modulus \\(m = 2^{64}\\) might be the limit before it starts “not looking random enough”. In particular, an LCG with a large period may not actually see enough repeats – after all, random numbers will have the occasional one-off repeat, just by chance. Compared to other more modern methods (like R’s default, the Mersenne Twister mentioned in the last lecture, discovered in 1997), an LCG requires quite a lot of computation for only a modest number of samples. So while LCGs are still admired for their simplicity and elegance, they have fallen out of favour for cutting-edge computational work.\nNext time: We’ll use our pseudorandom uniform \\([0,1]\\) random numbers to make random numbers with other discrete or uniform distributions.\n\nSummary:\n\nLinear congruential generators are pseudorandom number generators based on the recurrence \\(x_{n+1} = (ax_n + c) \\bmod m\\).\nAny LCG will eventually repeat with periodic behaviour.\nSuppose \\(m\\) is a power of 2. Then an LCG has period \\(m\\) if and only if \\(c\\) is odd and \\(a = 1 \\bmod 4\\).\n\nYou should now be able to answer all questions on Problem Sheet 2. Your answer will be discussed in the problems class on Thursday 31 October.\nRead more: Voss, An Introduction to Statistical Computing, Subsections 1.1.1 and 1.1.2.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>LCGs</span>"
    ]
  },
  {
    "objectID": "problems/P2.html",
    "href": "problems/P2.html",
    "title": "Problem Sheet 2",
    "section": "",
    "text": "Full solutions are now available.\n\n\nThis is Problem Sheet 2, which covers material from Lectures 7 to 11. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Thursday 31 October. If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Mondays at 1500.\nThis problem sheet is to help you practice material from the module. It is not for assessment and will not be marked.\nFull solutions should be released on Friday 1 November.\n\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.      [2018 exam, Question 4]\n\n(a)   Suppose it is desired to estimate the value of an integral \\[ \\theta = \\int_0^1 h(x)\\,\\mathrm{d}x \\] by Monte Carlo integration\n\n        i.   By splitting \\(h(x) = \\phi(x)f(x)\\), where \\(f\\) is a probability density function, describe how the Monte Carlo method of estimating \\(\\theta\\) works.\n\nSolution. As suggested, we have \\[ \\theta = \\int_0^1 h(x)\\,\\mathrm{d}x = \\int_0^1 \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\Exg \\phi(X) ,\\] where \\(X\\) is a random variable with PDF \\(f\\). To estimate this, we sample \\(X_1, \\dots, X_n\\) from \\(X\\), and use \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) . \\]\n\n\n\n        ii.  Let \\(\\widehat\\theta_n\\) be the Monte Carlo estimate based on \\(n\\) simulations. Find the expectation and variance of \\(\\widehat\\theta_n\\), as a function of \\(n\\).\n\nSolution. As in lectures, we have \\[ \\Exg \\theta_n^{\\mathrm{MC}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n}\\,n\\Exg\\phi(X) = \\Exg\\phi(X) \\] and \\[ \\Var\\big((\\theta_n^{\\mathrm{MC}}\\big) = \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n^2}\\,n\\Var\\big(\\phi(X)\\big) = \\frac{1}{n}\\Var\\big(\\phi(X)\\big) . \\]\n\n\n\n        iii.  What guidelines can be given for the choice of \\(f\\) in practice?\n\nSolution. First, \\(f\\) (or equivalently \\(X\\)) should be easy to sample from. Second, we want to minimise \\(\\Var(\\phi(X))\\), so should pick \\(f\\) approximately proportional to \\(h\\), so that \\(\\phi\\) is roughly constant, and therefore has low variance.\nIn the absence of better options, \\(X\\) being uniform on \\([0, 1]\\) (so \\(f(x) = 1\\) on this interval) is often not a bad choice.\n\n\n\n\n(b)  Consider evaluation the integral \\[ \\int_0^1 x^2\\,\\mathrm{d}x \\] by Monte Carlo integration using \\(f(x) = \\Ind_{[0,1]}(x)\\). Write down the Monte Carlo estimator \\(\\widehat\\theta_n\\).\n\nSolution. Since \\(f(x) = 1\\) on this interval, we must take \\(\\phi(x) = x^2\\). Thus the estimator is \\[\\widehat\\theta_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2 ,\\] where \\(X_i \\sim \\operatorname{U}[0,1]\\) are independent.\n\n          Explain how antithetic variables can be used in this situation, and justify briefly why their use here is guaranteed to improve efficiency.\n\nSolution. Antithetic variables attempt to reduce the variance in Monte Carlo estimation by using pairs of variables \\((X_i, X'_i)\\) that both have the same distribution as \\(X\\), but where \\(\\phi(X)\\) and \\(\\phi(X')\\) are negatively correlated.\nIn this situation, if \\(X_i \\sim \\operatorname{U}[0,1]\\), then \\(X'_i = 1 - X_i\\) has this same distribution. The corresponding antithetic variable estimator is \\[\\widehat\\theta_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(X_i^2 + (1 - X_i)^2\\big) .\\]\nAn anitithetic variables estimator always decreases the variance of a Monte Carlo estimator if the correlation (or, equivalently, the covariance) between \\(\\phi(X)\\) and \\(\\phi(X')\\) is negative. We saw in lectures that, if \\(X \\sim \\operatorname{U}[0,1]\\) and \\(\\phi\\) is monotonically increasing, then \\(\\phi(X)\\) and \\(\\phi(1 - X)\\) have negative correlation. Since \\(x^2\\) is increasing on \\([0,1]\\), that is the case here.\n\n          For \\(U \\sim \\operatorname{U}[0,1]\\), use the results \\[\\Ex U^2 = \\tfrac13 \\qquad \\Ex U^4 = \\tfrac15 \\qquad \\Ex U^2(1-U)^2 = \\tfrac{1}{30} \\] to find the correlation between \\(U^2\\) and \\((1-U)^2\\). Hence, or otherwise, confirm that using antithetic variables reduces the variance of the Monte Carlo estimator by a factor of 8.\n\nSolution. We need to find the correlation between \\(U^2\\) and \\((1 - U)^2\\), where \\(U \\sim \\operatorname{U}[0,1]\\). The covariance is \\[ \\operatorname{Cov}\\big(U^2, (1-U)^2\\big)\n= \\Ex U^2(1-U)^2 - \\big(\\Ex U^2\\big) \\big(\\Ex (1-U)^2\\big)\n= \\tfrac{1}{30} - \\Big(\\tfrac{1}{3}\\Big)^2 = -\\tfrac{7}{90} .\\] The variance are both \\[ \\Var(U^2) = \\Ex U^4 - \\big(\\Ex U^2\\big)^2 = \\tfrac15 - \\Big(\\tfrac{1}{3}\\Big)^2 = \\tfrac{4}{45} . \\] Hence, the correlation is \\[ \\rho = \\operatorname{Corr}\\big(U^2, (1-U)^2\\big) = \\frac{-\\frac{7}{90}}{\\frac{4}{45}} = -\\tfrac78 . \\]\nThe variance of the standard Monte Carlo estimator is \\(\\frac{1}{n}\\Var(\\phi(X))\\), while the variance of the antithetic variables estimator is \\(\\frac{1+\\rho}{n}\\Var(\\phi(X))\\). So the variance changes by a factor of \\(1 + \\rho\\), which here is \\(1 - \\frac{7}{8} = \\frac{1}{8}\\). So the variance reduces by a factor of 8, as claimed.\n\n          (You may use without proof any results about the variance of antithetic variable estimates, but you should clearly state any results you are using.)\n\n\n\n2.     Let \\(X \\sim \\operatorname{N}(0,1)\\). Consider importance sampling estimation for the probability \\(\\theta = \\mathbb P(3 \\leq X \\leq 4)\\) using samples \\(Y_i\\) from the following sample distributions: (i) \\(Y \\sim \\operatorname{N}(1,1)\\); (ii) \\(Y \\sim \\operatorname{N}(2,1)\\); (iii) \\(Y \\sim \\operatorname{N}(3.5,1)\\); (iv) \\(Y \\sim 3 + \\operatorname{Exp}(1)\\).\nEach of these four distributions gives rise to a different importance sampling method. Our aim is to compare the resulting estimates.\n\n(a)  For each of the four methods, estimate the variance \\(\\Var\\big(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\big)\\). Which of these four methods gives the best results?\n\nSolution. I used the following R code\n\nn &lt;- 1e5\nphi   &lt;- function(x) (x &gt;= 3) & (x &lt;= 4)\npdf_x &lt;- function(x) dnorm(x, 0, 1)\n\npdf_y1 &lt;- function(x) dnorm(x, 1, 1)\nsamples_y1 &lt;- rnorm(n, 1, 1)\nvar1 &lt;- var((pdf_x(samples_y1) / pdf_y1(samples_y1)) * phi(samples_y1))\n\npdf_y2 &lt;- function(x) dnorm(x, 2, 1)\nsamples_y2 &lt;- rnorm(n, 2, 1)\nvar2 &lt;- var((pdf_x(samples_y2) / pdf_y2(samples_y2)) * phi(samples_y2))\n\npdf_y3 &lt;- function(x) dnorm(x, 3.5, 1)\nsamples_y3 &lt;- rnorm(n, 3.5, 1)\nvar3 &lt;- var((pdf_x(samples_y3) / pdf_y3(samples_y3)) * phi(samples_y3))\n\npdf_y4 &lt;- function(x) dexp(x - 3, 1)\nsamples_y4 &lt;- 3 + rexp(n, 1)\nvar4 &lt;- var((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4))\n\nsignif(c(var1, var2, var3, var4), 3)\n\n[1] 8.23e-05 1.37e-05 6.64e-06 1.92e-06\n\n\n(For the fourth PDF, we used that the PDF of \\(3 + Z\\) is \\(f_Z(z-3)\\).)\nWe see that the fourth method \\(3 + \\operatorname{Exp}(1)\\) is the most accurate.\n\n\n\n(b)  Determine a good estimate for \\(\\mathbb P(3 \\leq X \\leq 4)\\), and discuss the accuracy of your estimate.\n\nSolution. We’ll use the fourth method, and boost the number of samples to one million.\n\nn &lt;- 1e6\nISest &lt;- mean((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4))\nIS_MSE &lt;- var((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4)) / n\nc(ISest, sqrt(IS_MSE))\n\n[1] 1.324307e-03 1.387280e-06\n\n\nSo our estimate is \\(\\widehat\\theta = 0.00132\\). Since the RMSE is three orders of magnitude less than the estimate, so the estimate is probably accurate to a couple of significant figures.\n\n\n\n(c)  For each of the four methods, approximate how many samples from \\(Y\\) are required to reduce the root-mean-square error of the estimate of \\(\\mathbb P(3 \\leq X \\leq 4)\\) to 1%?\n\nSolution. To get the error to 1% means an absolute error of roughly \\(\\epsilon = 0.01\\widehat\\theta\\). Then we know that the required number of samples is \\[ n = \\frac{\\Var\\big(\\frac{f(Y)}{g(Y)}\\phi(Y)\\big)}{\\epsilon^2} . \\]\n\neps &lt;- 0.01 * ISest\nround(c(var1, var2, var3, var4) / eps^2)\n\n[1] 469455  78347  37833  10974\n\n\n\n\n\n\n3.     [2017 exam, Question 3]\n\n(a)  Defining any notation you use, write down the basic Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) and the importance sampling estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) for an expectation of the form \\(\\theta = \\Exg \\phi(X)\\), where \\(X\\) is a random variable with probability density function \\(f\\).\nWhat is the advantage of importance sampling over the standard Monte Carlo method?\n\nSolution. The basic Monte Carlo estimator is \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n\\phi(X_i) , \\] where the \\(X_i\\) are independent random samples from \\(X\\).\nThe basic importance sampling estimator is \\[ \\widehat\\theta_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) , \\] where the \\(Y_i\\) are independent random samples from a random variable \\(Y\\) with probability density function \\(g\\). We must have \\(g(y) &gt; 0\\) whenever \\(f(y) &gt; 0\\).\nThe main advantage of the importance sampling estimator is that can reduce the variance of the estimator by oversampling the most important areas of \\(y\\), but then downweighting those samples. Another advantage is that importance sampling can be used when it is difficult to sample from \\(X\\).\n\n\n\n(b)  Prove that both the basic Monte Carlo and importance sampling estimates from part (a) are unbiased.\n\nSolution. For the standard Monte Carlo estimator, \\[ \\Exg \\theta_n^{\\mathrm{MC}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n}\\,n\\Exg\\phi(X) = \\Exg\\phi(X) = \\theta .\\]\nFor the importance sampling estimator, first note that \\[ \\mathbb E \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\int_{-\\infty}^{+\\infty} \\frac{f(y)}{g(y)}\\,\\phi(y)\\,g(y)\\,\\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\phi(y) \\,f(y) \\, \\mathrm{d}y = \\Exg\\phi(X)  , \\] since \\(f\\) is the PDF of \\(X\\). Hence \\[ \\Exg \\widehat\\theta_n^{\\mathrm{IS}} = \\Exg \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) \\right) = \\frac{1}{n}\\,n\\,\\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\Exg \\phi(X) = \\theta. \\]\nHence, both estimators are unbiased.\n\n\n\n(c)  Show that the variance of the importance sampling estimator is given by \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\frac{1}{n}\\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y - \\frac{1}{n}\\big(\\Exg \\phi(X)\\big)^2. \\]\n\nSolution. First, note that \\[\\begin{align} \\Var \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right)\n&= \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right)^2 - \\big(\\Exg \\phi(X)\\big)^2 \\\\\n&= \\int_{-\\infty}^{+\\infty} \\left(\\frac{f(y)}{g(y)}\\,\\phi(y)\\right)^2 g(y)\\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2\\\\\n&= \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2}{g(y)^2}\\,\\phi(y)^2 \\,g(y)\\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2\\\\\n&= \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2}{g(y)}\\,\\phi(y)^2 \\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2 \\end{align} \\] Second, we have \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) \\right) = \\frac{1}{n} \\Var \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) . \\] Putting these together proves the result.\n\n\n\n(d)  Let \\(X \\sim \\operatorname{N}(0,2)\\) and \\(a \\in \\mathbb R\\). We want to estimate \\(\\theta = \\Ex \\big(\\sqrt{2}\\exp(-(X-a)^2/4)\\big)\\), using importance sampling with samples \\(Y \\sim \\operatorname{N}(\\mu, 1)\\) for some \\(\\mu \\in \\mathbb R\\). Using the result from part (c), or otherwise, show that in the case the variance of the importance sampling estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) is given by \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\frac{\\exp(\\mu^2 - a\\mu) - \\theta^2}{n} . \\] [Note: This equation has changed since an earlier version of the question.]\n\nSolution. It’s clear, using part (c), that it will suffice to show that \\[ \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y = \\exp(\\mu^2 - a\\mu) . \\] Here, we have \\[ \\begin{align}\nf(y) &= \\frac{1}{\\sqrt{4\\pi}} \\exp\\big(-\\tfrac14 y^2 \\big) \\\\\ng(y) &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-\\mu)^2 \\big) \\\\\n\\phi(y) &= \\sqrt{2} \\exp\\big(-\\tfrac14 (y-a)^2 \\big) .\n\\end{align} \\] Therefore, by a long and painful algebra slog, we have \\[ \\begin{align}\n\\frac{f(y)^2 \\phi(y)^2}{g(y)} &= \\frac{\\frac{1}{4\\pi} \\exp\\big(-\\tfrac12 y^2 \\big)\\times 2\\exp\\big(-\\tfrac12 (y-a)^2 \\big)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-\\mu)^2 \\big)} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y^2 + (y-a)^2 - (y-\\mu)^2 \\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y^2 - 2(a - \\mu)y + a^2 + \\mu^2\\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big((y - (a - \\mu))^2 - (a- \\mu)^2 + a^2 - \\mu^2\\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big((y - (a - \\mu))^2 + 2a\\mu - 2\\mu^2 \\big)\\Big) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\exp(\\mu^2 + a\\mu),\n\\end{align} \\] where we ‘completed the square’ on the fourth line. Thus \\[ \\begin{align}\n\\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y\n&= \\int_{-\\infty}^{+\\infty}  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\exp(\\mu^2 + 2a) \\,\\mathrm{d}y\\\\\n&= \\exp(\\mu^2 + 2a) \\int_{-\\infty}^{+\\infty}  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\,\\mathrm{d}y \\\\\n&= \\exp(\\mu^2 + a\\mu) ,\n\\end{align} \\] since the big integral on the right is the integral of the PDF of a normal \\(\\operatorname{N}(a-\\mu, 1)\\) distribution, so equals 1.\nHence, we have proved the result.\n\n          For fixed \\(n\\) and \\(a\\), find the value of \\(\\mu\\) for which the importance sampling estimator has the smallest mean-square error. Comment on the result.\n\nSolution. Minimising this expression is equivalent to minimising \\(\\mu^2 + a\\mu\\). By differentiating with respect to \\(\\mu\\) (or otherwise), we see that this is at \\(a = \\tfrac12 \\mu\\).\n\n\n\n\n4.     (Answer the following question “by hand”, without using R. You may check your answer with R, if you wish.)\n\n(a)  Consider the LCG with modulus \\(m = 2^4 = 16\\), multiplier \\(a = 5\\), and increment \\(a = 8\\). What is the period of this LCG when started from the seed (i) \\(x_1 = 1\\); (ii) \\(x_1 = 2\\)?\n\nSolution. For (i), we have \\[ \\begin{align}\nx_1 &= 1 \\\\\nx_2 &= (5 \\times 1 + 8) \\bmod 16 = 13 \\bmod 16 = 13 \\\\\nx_3 &= (5 \\times 13 + 8) \\bmod 16 = 73 \\bmod 16 = 9 \\\\\nx_4 &= (5 \\times 9 + 8) \\bmod 16 = 53 \\bmod 16 = 5 \\\\\nx_5 &= (5 \\times 5 + 8) \\bmod 16 = 33 \\bmod 16 = 1 .\n\\end{align}\\] Here, \\(x_5\\) is a repeat of \\(x_1\\), so the period is \\(5-1=4\\).\nFor (ii), we have \\[ \\begin{align}\nx_1 &= 2 \\\\\nx_2 &= (5 \\times 2 + 8) \\bmod 16 = 18 \\bmod 16 = 2 .\n\\end{align}\\] This is an immediate repeat, so the period is 2.\n\n\n\n(b) Consider the LCG with modulus \\(m = 2^4 = 16\\), multiplier \\(a = 2\\), and increment \\(c = 4\\). Start from the seed \\(x_1 = 3\\). (i) When do we first see a repeat output? (ii) What is the period?\n\nSolution. We have \\[ \\begin{align}\nx_1 &= 3 \\\\\nx_2 &= (2 \\times 3 + 4) \\bmod 16 = 10 \\bmod 16 = 10 \\\\\nx_3 &= (2 \\times 10 + 4) \\bmod 16 = 24 \\bmod 16 = 8 \\\\\nx_4 &= (2 \\times 8 + 4) \\bmod 16 = 20 \\bmod 16 = 4 \\\\\nx_5 &= (2 \\times 4 + 4) \\bmod 16 = 12 \\bmod 16 = 12 \\\\\nx_6 &= (2 \\times 12 + 4) \\bmod 16 = 28 \\bmod 16 = 12 .\n\\end{align}\\]\n(i) The first repeat is \\(x_6\\).\n(ii) Since 12 is a fixed point of this LCG, the remainder of the sequence is 12 forever, with period 1.\n\n\n\n\n5.     Consider the following LCGs with modulus \\(m = 2^8 = 256\\):\n (i) \\(a = 31\\), \\(c = 47\\);\n (ii) \\(a = 21\\), \\(c = 47\\);\n (iii) \\(a = 129\\), \\(c = 47\\).\n\n(a)  Without using a computer, work out which of these LCGs have a full period of 256.\n\nSolution.\n (i) Here, \\(a\\) is \\(3 \\bmod 4\\), not \\(1 \\bmod 4\\), so this does not have full period of 256.\n (ii) Here, \\(c\\) is odd and \\(a\\) is \\(1 \\bmod 4\\), so this does have full period of 256.\n (iii) Here, \\(c\\) is odd and \\(a\\) is \\(1 \\bmod 4\\), so this does have full period of 256.\n\n\n\n(b)  Which of these LCGs would make good pseudorandom number generators?\n\nSolution. We will check the random appearance of the outputs using R.\n\nlcg &lt;- function(n, modulus, mult, incr, seed) {\n  samples &lt;- rep(0, n)\n  samples[1] &lt;- seed\n  for (i in 1:(n - 1)) {\n    samples[i + 1] &lt;- (mult * samples[i] + incr) %% modulus\n  }\n  return(samples)\n}\n\n (i) This does not have full period, so is unlikely to be good PRNG. Let’s check\n\nm &lt;- 2^8\nseed &lt;- 1\nplot(lcg(m, m, 31, 47, seed))\n\n\n\n\n\n\n\n\nThe picture confirms that this does not look random, and in fact has very short period of 16.\n (ii) This does have full period, so is a candidate for a good PRNG if the output looks random.\n\nplot(lcg(m, m, 21, 47, seed))\n\n\n\n\n\n\n\n\nThis mostly looks random, but there does appear to be a sort of thick diagonal line in the picture going from bottom left to rop right. I’d might be happy to use this for casual statistical work – the lack of randomness does not seem super-serious – but I would avoid this for cryptographic purposes, for example.\n (iii) This does have full period, so is a candidate for a good PRNG if the output looks random.\n\nplot(lcg(m, m, 129, 47, seed))\n\n\n\n\n\n\n\n\nEven though this has full period, there is a very clear non-random pattern. This is not appropriate for a PRNG.\n\n\n\n\n6.     Consider an LCG with modulus \\(m = 2^{10} = 1024\\), multiplier \\(a = 125\\), and increment \\(c = 5\\). Using R:\n\n(a) Generate 1000 outputs in \\(\\{0, 1, \\dots, 1023\\}\\) from this LCG, starting with the seed \\(x_1 = 1\\).\n[This question originally said 200 outputs, not 1000. It’s fine if you answered that version, but the conclusions to the problem are less interesting that way.]\n\nSolution. Using the same lcg() function from Question 5, we have\n\nn &lt;- 1000\nm &lt;- 2^{10}\nseed &lt;- 1\nsamples1 &lt;- lcg(n, m, 125, 5, seed)\n\n\n\n\n(b)  Convert these to 1000 outputs to pseudorandom uniform samples in \\([0, 1]\\).\n\nTo do this, we simply divide the samples by \\(m\\).\n\nsamples1 &lt;- samples1 / m\n\n\n\n\n(c)  Using these samples, obtain a Monte Carlo estimate for \\(\\mathbb E\\cos(U)\\), where \\(U \\sim \\operatorname{U}[0,1]\\).\n\nSolution.\n\nmean(cos(samples1))\n\n[1] 0.8418218\n\n\nThis is very close to the correct answer \\(\\sin 1 = 0.8414\\)\n\n\n\n(d)  What is the root-mean-square error of your estimate?\n\nSolution.\n\nsqrt(var(cos(samples1)) / n)\n\n[1] 0.004392217\n\n\n\n\n\n(e)  Repeat parts (a) to (d) for the LCG with the same \\(m\\), but now with multiplier \\(a = 127\\) and increment \\(c = 4\\).\n\nSolution.\n\nsamples2 &lt;- lcg(n, m, 127, 4, seed)\nsamples2 &lt;- samples2 / m\nmean(cos(samples2))\n\n[1] 0.868257\n\nsqrt(var(cos(samples2)) / n)\n\n[1] 0.003899394\n\n\nThis does not seem to be quite accurate to the answer to \\(\\sin 1 = 0.841\\), and the reported RMSE to too small to account for the error.\nHowever, the problem is that this LCG is not actually a uniform (pseudo)random number generator – it has period 8.\n\nplot(samples2)\n\n\n\n\n\n\n\n\nThus the estimator is just keeping using the same 8 points over and over again. So this is actually estimating \\(\\mathbb E(\\cos Y)\\), where \\(Y\\) is uniform on the 8 points actually visited by the LCG. So while the correct answer is \\(\\mathbb EU = \\sin 1 = 0.841\\), this is in fact estimating \\[ \\begin{multline}\n\\frac{1}{8} \\bigg(\\cos \\frac{1}{2^{10}} + \\cos \\frac{131}{2^{10}} + \\cos \\frac{257}{2^{10}} + \\cos \\frac{899}{2^{10}} \\\\\n+ \\cos \\frac{513}{2^{10}} + \\cos \\frac{643}{2^{10}} + \\cos \\frac{769}{2^{10}} + \\cos \\frac{387}{2^{10}}\\bigg) = 0.868\n\\end{multline} \\] (where the numerators are the 8 values visited by the LCG), which is not the correct answer.",
    "crumbs": [
      "Random number generation",
      "Problem Sheet 2"
    ]
  },
  {
    "objectID": "lectures/L12-uniform-discrete.html",
    "href": "lectures/L12-uniform-discrete.html",
    "title": "12  Uniform and discrete",
    "section": "",
    "text": "12.1 Uniform random variables\nWe know how to generate \\(U \\sim \\operatorname{U}[0,1]\\). But suppose we want to generate \\(X \\sim \\operatorname{U}[a,b]\\) instead, for some \\(a &lt; b\\); how can we do that.\nWell, the original \\(U\\) has “width” 1, and we want \\(X\\) to have width \\(b - a\\), so the first thing we should do is multiply by \\((b-a)\\). This gives us \\((b-a)U\\), which we expect should be uniform on \\([0,b-a]\\). Then we need to shift it, so it starts not a \\(0\\) but at \\(a\\); we can do this by adding \\(a\\). This gives us \\((b-a)U + a\\), which should be uniform on \\([0 + a, (b-a) + a] = [a, b]\\). So \\(X = (b-a)U + a\\) would seem to give us the desired \\(X \\sim \\operatorname{U}[a,b]\\) random variable.\nWe can check this seems to have worked by using a histogram, for example.\nBut what if we very formally wanted to prove that \\(X = (b-a)U + a\\) definitely has the distribution \\(X \\sim \\operatorname{U}[a,b]\\); how could we do that?\nThe best way to give a formal proof of something like this is to use the cumulative distribution function (CDF). Recall that the CDF \\(F_Y\\) of a distribution \\(Y\\) is the function \\(F_Y(y) = \\mathbb P(Y \\leq y)\\). One benefit of the CDF is it works equally well for both discrete and continuous random variables, so we don’t need to give separate arguments for discrete and continuous cases.\nThe CDF of the standard uniform distribution \\(U \\sim \\operatorname{U}[0,1]\\) is \\[ F_U(u) = \\begin{cases} 0 & u &lt; 0 \\\\ u & 0 \\leq u \\leq 1 \\\\ 1 & u &gt; 1 , \\end{cases}  \\tag{12.1}\\] and the CDF of any uniform distribution \\(X \\sim \\operatorname{U}[a,b]\\) is \\[ F_X(x) = \\begin{cases} 0 & x &lt; a \\\\ \\displaystyle\\frac{x - a}{b-a} & a \\leq x \\leq b \\\\ 1 & x &gt; b . \\end{cases}  \\tag{12.2}\\] So to show that \\(X = (b-a)U + a \\sim \\operatorname{U}[a,b]\\), we take the fact that \\(U\\) has the CDF in Equation 12.1 and try to use it to show that \\(X\\) has the CDF in Equation 12.2.\nIndeed, we have \\[ F_X(x) = \\mathbb P(X \\leq x) = \\mathbb P\\big((b-a)U + a \\leq x\\big) = \\mathbb P \\left( U \\leq \\frac{x - a}{b-a}\\right) . \\] But putting \\(u = (x - a)/(b - a)\\) in Equation 12.1, does indeed give the CDF in Equation 12.2. The lower boundary \\(u &lt; 0\\) becomes \\(x &lt; 0 \\times (b-a) + a = a\\); the upper boundary \\(u &gt; b\\) becomes \\(x &gt; 1 \\times(b-a) + a = b\\); and, in between, the CDF \\(u\\) becomes \\((x - a)/(b - a)\\). Thus we have proven that \\(X\\) has the CDF of the \\(\\operatorname{U}[a,b]\\) distribution, as required.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Uniform and discrete</span>"
    ]
  },
  {
    "objectID": "lectures/L12-uniform-discrete.html#uniform-random-variables",
    "href": "lectures/L12-uniform-discrete.html#uniform-random-variables",
    "title": "12  Uniform and discrete",
    "section": "",
    "text": "Example 12.1 Let \\(U \\sim \\operatorname{U}[0,1]\\). We can generate \\(X \\sim \\operatorname{U}[3,5]\\) by \\(X = 2U + 3\\).\n\nn &lt;- 1e5\nUsamples &lt;- runif(n)\nXsamples &lt;- 2 * Usamples + 3\nhist(Xsamples, probability = TRUE, xlim = c(2, 6), ylim = c(0, 0.6))\ncurve(dunif(x, 3, 5), add = TRUE, n = 1001, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nHere, we used the probability = TRUE argument to the histogram function hist() to plot the density on the \\(y\\)-axis, rather than the raw number of samples. The density should match the probability density function of the random variable \\(X\\). We drew the PDF of \\(X\\) over the histogram in blue, and can see it is a superb match.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Uniform and discrete</span>"
    ]
  },
  {
    "objectID": "lectures/L12-uniform-discrete.html#discrete-random-variables",
    "href": "lectures/L12-uniform-discrete.html#discrete-random-variables",
    "title": "12  Uniform and discrete",
    "section": "12.2 Discrete random variables",
    "text": "12.2 Discrete random variables\nSuppose we want to simulate a Bernoulli trial; that is, a random variable \\(X\\) that is 1 with probability \\(p\\) and 0 with probability \\(1 - p\\), for some \\(p\\) with \\(0 &lt; p &lt; 1\\). As ever, we only have a standard uniform \\(U \\sim \\operatorname{U}[0,1]\\) to work with. How can we form our Bernoulli trial?\nHere are two possible ways:\n\nIf \\(U &lt; p\\), then take \\(X = 1\\); while if \\(U \\geq p\\), take \\(X = 0\\). Note that \\(\\mathbb P(X = 1) = \\mathbb P(U &lt; p) = p\\) and \\(\\mathbb P(X = 0) = \\mathbb P(U \\geq p) = 1 - \\mathbb P(U &lt; p) = 1 - p\\), as required.\nAlternatively: if \\(U \\leq 1 -p\\), take \\(X' = 0\\); while if \\(U &gt; p\\), take \\(X' = 1\\).\n\nThe first method is just the second method with \\(U\\) replaced by \\(1 - U\\). Interestingly, that means we can generate two different Bernoulli trials \\(X\\) and \\(X'\\) from the same \\(U\\), which will therefore be negatively correlated. Although there’s unlikely to be any situation where a Bernoulli trial would be used in Monte Carlo estimation, in theory, the two versions \\((X, X')\\) generated from the same \\(U\\) could potentially be used as antithetic variables.\nWhat about more general discrete random variables? Suppose a random variable takes values \\(x_1, x_2, \\dots\\) with probabilities \\(p_1, p_2, \\dots\\). We can think of these probabilities as splitting up the interval \\([0,1]\\) into subintervals of lengths \\(p_1, p_, \\dots\\), since these probabilities add up to 1.\n\n\n\n\n\nSo the first interval is \\(I_1 = (0, p_1]\\); the second interval is \\((p_1, p_1 + p_2]\\); the third intervals is \\((p_1 + p_2, p_1 + p_2 + p_3]\\), and so on. We then pick a point \\(U\\) from \\([0,1]\\) uniformly at random, and whichever interval \\(I_i\\) it is in, take the corresponding value \\(x_i\\).\nIn terms of the CDF, we have \\(F_X(x_i) = p_1 + p_2 + \\cdots + p_i\\), so the intervals are of the form \\((F_X(x_{i-1}), F_X(x_i)]\\). So we can think of this as rounding \\(F_X(U)\\) up to the next \\(F_X(x_i)\\), then taking that value \\(x_i\\).\n\nExample 12.2 Consider generating a binomial distribution \\(X \\sim \\operatorname{Bin}(5, \\frac12)\\). The PMF and CDF are as shown below\n\n\n\n\n\n\n\n\n\n\n\nvalue \\(x\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\n\n\n\nPMF \\(p(x)\\)\n\\(\\frac{1}{32}\\)\n\\(\\frac{5}{32}\\)\n\\(\\frac{10}{32}\\)\n\\(\\frac{10}{32}\\)\n\\(\\frac{5}{32}\\)\n\n\nCDF \\(F_X(x)\\) (fraction)\n\\(\\frac{1}{32}\\)\n\\(\\frac{6}{32}\\)\n\\(\\frac{16}{32}\\)\n\\(\\frac{26}{32}\\)\n\\(\\frac{31}{32}\\)\n\n\nCDF \\(F_X(x)\\) (decimal)\n0.03125\n0.1875\n0.5\n0.8125\n0.96875\n\n\n\n \nSuppose I want a sample from this, based on the uniform variate \\(u_1 = 0.7980\\). We see that \\(u_1\\) is bigger that \\(F_X(2) = 0.5\\) (the upper end of the “2” interval) but less than \\(F_X(3) = 0.8125\\) (the upper end of the “3” interval), so falls into the “3” interval. So our first binomial variate is \\(x_1 = 3\\).\nIf we then got the next uniform variate \\(u_2 = 0.4353\\), we would see that \\(u_2\\) is between \\(F_X(1) = 0.1875\\) and \\(F_X(2) = 0.5\\), so would take \\(x_2 = 2\\) for our next binomial variate.\n\nNext time: Sampling from continuous distribution using the CDF.\n\nSummary:\n\nIf \\(U \\sim \\operatorname{U}[0,1]\\), then \\(X = (b-a)U + a \\sim \\operatorname{U}[a,b]\\).\nA discrete random variable can be generated by splitting \\([0,1]\\) into subintervals with lengths according to the probabilities, then picking a point from the interval at random.\n\nRemember that your answers to Problem Sheet 2 will be discussed in the problems class on Thursday 31 October.\nRead more: Voss, An Introduction to Statistical Computing, Sections 1.2 and 1.3.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Uniform and discrete</span>"
    ]
  },
  {
    "objectID": "lectures/L13-inverse.html",
    "href": "lectures/L13-inverse.html",
    "title": "13  Inverse transform method",
    "section": "",
    "text": "13.1 Inverse CDF\nWe have started looking at how to transform a standard uniform random variable \\(U \\sim \\operatorname{U}[0,1]\\) into any other distribution \\(X\\).\nLast lecture, we saw how to do this for other uniform distributions and for discrete random variables. Booth involved the cumulative distribution function \\(F(x) = \\mathbb P(X \\leq x)\\). For generating a wider class of random variables (including continuous random variables) the CDF will continue to be important. In fact, it is the inverse of the CDF that will play a crucial role.\nIt seems natural to define the inverse \\(F^{-1}\\) of the CDF in just the same way we would define the inverse of any other function: that \\(F^{-1}(u)\\) is the unique value \\(x\\) such that \\(F(x) = u\\). This is illustrated in the figure below.\n[picture]\nThis definition works fine for a purely continuous distribution that doesn’t have any “gaps” in the set of values it can take. But for a general random variable, there are two problems with this definition.\n[picture]\nIt turns out that the best way to solve this is the following. For the first obstacle, if there are many points \\(x\\) with \\(F(x) = u\\), we take the first one – that is, the smallest \\(x\\) with \\(F(x) = u\\). For the second obstacle, we take the value of the smallest \\(x\\) where \\(F(x)\\) is above \\(u\\), which comes right at the jump. These two cases are illustrated below.\n[picture]\nThese two awkward cases can be encapsulated in the following definition.\nYou should check that you agree this definition matches the discussion above.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inverse transform method</span>"
    ]
  },
  {
    "objectID": "lectures/L13-inverse.html#inverse-cdf",
    "href": "lectures/L13-inverse.html#inverse-cdf",
    "title": "13  Inverse transform method",
    "section": "",
    "text": "If \\(X\\) has any point masses – points \\(x\\) with strictly positive probability \\(\\mathbb P(X = x) &gt; 0\\) of hitting that point exactly – then \\(F\\) may “jump past” the value \\(u\\), so there is no value \\(x\\) such that \\(F(u)\\). (See the blue line on the graph below.)\nIf there is an interval on the line where \\(X\\) has probability zero, then all the \\(x\\)s in that interval have the same value of \\(F(x) = u\\), so there is no unique value \\(x\\) such that \\(F(x) = u\\). (See the red line on the graph below.)\n\n\n\n\n\n\nDefinition 13.1 Let \\(X\\) be a random variable with cumulative distribution function \\(F_X(x) = \\mathbb P(X \\leq x)\\). Then the inverse cumulative distribution function (inverse CDF) \\(F_X^{-1}\\) is defined for \\(u \\in (0,1)\\) by \\[ F^{-1}_X(u) = \\min \\big \\{x : F_X(x) \\geq u \\big\\} . \\]",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inverse transform method</span>"
    ]
  },
  {
    "objectID": "lectures/L13-inverse.html#inverse-transform",
    "href": "lectures/L13-inverse.html#inverse-transform",
    "title": "13  Inverse transform method",
    "section": "13.2 Inverse transform",
    "text": "13.2 Inverse transform\nThe inverse transform method works like this: To generate \\(X\\), simply apply the inverse CDF \\(F^{-1}\\) to a standard uniform random variable \\(U\\).\n\nTheorem 13.1 Let \\(F\\) be a cumulative distribution function, and let \\(F^{-1}\\) be its inverse. Let \\(U \\sim \\operatorname{U}[0,1]\\). Then \\(X = F^{-1}(U)\\) has cumulative distribution function \\(F\\).\n\n\nProof. We need to show that \\(\\mathbb P(X \\leq x) = F(x)\\) when this is in \\((0,1)\\). The proof is very easy if \\(F\\) has no “gaps” or “jumps” as described above. Then, we simply have \\[ \\mathbb P(X \\leq x) = \\mathbb P\\big(F^{-1}(U) \\leq x\\big) = \\mathbb P\\big(U \\leq F(x)\\big) = F(x), \\] where we have simply “undone” the function \\(F^{-1}\\) and used that \\(\\mathbb P(U \\leq u) = u\\) for \\(u \\in (0,1)\\).\nFor the general case, we need to be just a little bit more careful. We have \\[ \\mathbb P(X \\leq x) = \\mathbb P\\big(F^{-1}(U) \\leq x\\big) = \\mathbb P\\big(\\min\\{y : F(y) \\geq U \\} \\leq x \\big) .\\] If \\(x\\) is bigger than the minimum \\(y\\) with \\(F(y) \\geq u\\), then certainly \\(F(x) \\geq u\\) as well; while if \\(F(x) \\geq u\\), then \\(x\\) must be at least as big as the minimum \\(y\\) for which \\(F(y) \\geq u\\). Hence \\(\\min\\{y : F(y) \\geq U \\} \\leq x\\) if and only is \\(F(x) \\geq U\\). So \\[ \\mathbb P(X \\leq x) = \\mathbb P\\big(\\min\\{y : F(y) \\geq U \\} \\leq x \\big) = \\mathbb P\\big(F(x) \\geq U\\big) = \\mathbb P(U \\leq F(x)\\big) = F(x), \\] as before.\n\nThe method – known as the inverse transform method gives a simple way to generate any random variable \\(X\\) for which the inverse CDF \\(F_X^{-1}\\) can be computed easily.\nThis is not all random variables, however. In particular, the inverse CDF of the normal distribution does not have a closed form, so this does not give a way of sampling normal distributions. Later we’ll see other methods that allow us to sample from normal random variables.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inverse transform method</span>"
    ]
  },
  {
    "objectID": "lectures/L13-inverse.html#examples",
    "href": "lectures/L13-inverse.html#examples",
    "title": "13  Inverse transform method",
    "section": "13.3 Examples",
    "text": "13.3 Examples\nLet’s see some examples. The idea for all these problems is “Write \\(U = F(X)\\), then invert, to get \\(X = F^{-1}(U)\\).”\n\nExample 13.1 Let \\(X \\sim \\operatorname{Exp}(\\lambda)\\) be an exponential distribution with rate \\(\\lambda\\). This has PDF \\(f(x) = \\lambda \\ee^{-\\lambda x}\\) for \\(x \\geq 0\\) and CDF \\(F(x) = 1 - \\ee^{-\\lambda x}\\).\nWe write \\(U = F(X)\\) and invert it to make \\(X\\) the subject. So \\(U = 1 - \\ee^{-\\lambda X}\\), and therefore \\[ X = -\\frac{1}{\\lambda} \\log(1 - U) . \\]\nWe should check that this really does have an exponential distribution.\n\nrate &lt;- 0.5\nn &lt;- 1e5\nunif &lt;- runif(n)\nsamples &lt;- -(1 / rate) * log(1 - unif)\n\nhist(samples, probability = TRUE)\ncurve(dexp(x, rate), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nLooks like an accurate sample!\nSince \\(1 - U\\) has the same distribution as \\(U\\), it can be more convenient to simply write \\[ X' = -\\frac{1}{\\lambda} \\log U \\] instead.\nAlternatively, since \\(X\\) and \\(X'\\) are not independent and both have the same exponential distribution, they are a candidate to use as an antithetic pair in Monte Carlo estimation.\n\n\nExample 13.2 Consider \\(X\\) with PDF \\[ f(x) = \\frac{x}{\\gamma} \\,\\exp \\left(-\\frac{x^2}{2\\gamma}\\right)\\] for \\(x \\geq 0\\), and CDF \\[ F(x) = 1 - \\exp\\left(-\\frac{x^2}{2\\gamma}\\right) \\] This is known as the Rayleigh distribution with scale parameter \\(\\gamma\\).\nAgain, we write \\(U = F(X)\\) and invert. so \\[ U = 1 - \\exp\\left(-\\frac{X^2}{2\\gamma}\\right)  \\] so \\[ X = \\sqrt{-2\\gamma\\log(1-U)} . \\]\nAgain, \\(X' = \\sqrt{-2\\gamma\\log U}\\) is a slightly simpler expression, or could be used in an antithetic variables Monte Carlo approach.\n\n\nExample 13.3 Suppose \\(X \\sim \\operatorname{U}[a,b]\\), then \\[ F(x) = \\frac{x-a}{b-a} \\] (except for when \\(F(x) = 0\\) or \\(1\\)).\nWrite \\(U = F(X)\\) and invert. We get \\(X = (b-a)U + a\\). This is precisely the method for generating general uniform distributions that we saw in the last lecture.\n\n\nExample 13.4 Let \\(X\\) be discrete on the values \\(x_1, x_2, \\dots\\) with probabilities \\(p_1, p_2, \\dots\\). The CDF is \\[ F(x) = \\begin{cases} 0 & x &lt; x_1 \\\\\np_1 & x_1 \\leq x &lt; x_2 \\\\\np_1 + p_2 & x_2 \\leq x &lt; x_3 \\\\\np_1 + p_2 + p_3 & x_3 \\leq x &lt; x_4 \\\\\n\\cdots & \\cdots . \\end{cases} \\]\nRemembering the rule for “jumps” in the CDF, we see that the inverse CDF is \\[ F^{-1}(u) = \\begin{cases} x_1 & u &lt; p_1 \\\\\nx_2 & p_1 \\leq x &lt; p_1 + p_2 \\\\\nx_3 & p_1 + p_2 \\leq x &lt; p_1 + p_2 + p_3 \\\\\n\\cdots & \\cdots . \\end{cases} \\] Taking \\(X = F^{-1}(U)\\) gives the same method for generating discrete random variables as we discussed in the last lecture.\n\n\nExample 13.5 Consider a distribution with PDF \\[ f(x) = \\begin{cases} x^2 & 0 \\leq x \\leq 1 \\\\\n\\frac{2}{3} & 1 &lt; x \\leq 2 \\\\\n0 & \\text{otherwise.} \\end{cases} \\] Show how to sample \\(X\\) using a standard uniform random variable \\(U\\).\nThis is a standard sort of question. First we have to find the CDF, then we have to invert it.\nWe find the CDF from the PDF by integrating. For \\(0 \\leq x \\leq 1\\), we have \\[ F(x) = \\int_0^x f(y) \\, \\mathrm{d}y = \\int_0^x y^2 \\,\\mathrm{d}y = \\tfrac13 x^3. \\] Then for \\(1 &lt; x \\leq 2\\), we have \\[ F(x) = F(1) + \\int_1^x f(y) \\, \\mathrm{d}y = \\tfrac13 + \\int_1^x \\tfrac23 \\,\\mathrm{d}y = \\tfrac13 + \\tfrac23 x - \\tfrac23 = \\tfrac23 x - \\tfrac13 . \\]\nFor \\(0 \\leq U &lt; \\leq F(1) = \\tfrac13\\), we have \\(U = \\tfrac13 X^3\\), so \\(X = \\sqrt[3]{3U}\\). For \\(F(1) = \\tfrac13 &lt; U \\leq 1\\), we have \\(U = \\tfrac23 X - \\tfrac13\\), so \\(X = \\tfrac32 U + \\tfrac12\\). So the inverse transform is \\[ X = \\begin{cases} \\sqrt[3]{3U} & U \\leq \\frac13 \\\\ \\tfrac32 U + \\tfrac12 & U &gt; \\tfrac13 . \\end{cases} \\]",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inverse transform method</span>"
    ]
  },
  {
    "objectID": "lectures/L13-inverse.html#boxmuller-transform",
    "href": "lectures/L13-inverse.html#boxmuller-transform",
    "title": "13  Inverse transform method",
    "section": "13.4 Box–Muller transform",
    "text": "13.4 Box–Muller transform\nWhile the inverse transform method is very powerful, we have mentioned that it doesn’t work easily for the normal distribution. One would need first a very good approximation to the inverse CDF, in order to be able to apply it.\nInstead, the Box–Muller transform (discovered by the statistician GEP Box and the computer scientist Marvin E Muller in 1958) is a clever way to easily transform a standard uniform into a normal distribution.\nActually, that’s not quite true. Rather than transforming one standard uniform \\(U\\) into one normal distribution \\(X\\), it instead transforms two independent standard uniforms \\(U, V\\) into two normal distributions \\(X, Y\\). (There is some profound mathematical sense in which the two-dimensional bivariate normal is somehow a more “deeply” important mathematical object than the one-dimensional univariate normal we are used to, but we don’t have time to get into that here.)\nFirst, let’s note it suffices to produce a standard normal \\(X \\sim \\operatorname{N}(0,1)\\). Any other normal distribution \\(W \\sim \\operatorname{N}(\\mu, \\sigma^2)\\) can then be formed as \\(W = \\sigma X + \\mu\\).\nThe idea of the Box–Muller transform is based on converting the two standard normal distributions \\((X, Y)\\) from cartesian coordinates into polar coordinates \\((R, \\Theta)\\). (Those of you who have know how to calculate the Gaussian integral will have seen this idea before.)\n[picture]\n\nTheorem 13.2 Let \\(X, Y \\sim \\operatorname{N}(0,1)\\) be independent standard normal distributions. Write \\[ R = \\sqrt{X^2 + Y^2} \\qquad\\qquad \\Theta = \\tan \\frac{Y}{X} \\] for the radius and the angle of \\((X, Y)\\) in polar coordinates. Then the radius \\(R\\) has a Rayleigh distribution with scalar parameter \\(\\gamma = 1\\), the angle \\(\\Theta\\) has a uniform distribution on \\([0, 2\\pi]\\), and \\(R\\) and \\(\\Theta\\) are independent.\n\n\nProof. The joint PDF of \\((X,Y)\\) is \\[ \\begin{align}\nf_{X,Y}(x, y) \\,\\mathrm{d}x\\,\\mathrm{d}y &= f_X(x) \\,f_Y(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\\\\n&= \\frac{1}{\\sqrt{2\\pi}}\\, \\exp \\big(-\\tfrac12 x^2\\big) \\, \\frac{1}{\\sqrt{2\\pi}} \\,\\exp \\big(-\\tfrac12 y^2\\big) \\,\\mathrm{d}x\\,\\mathrm{d}y \\\\\n&= \\frac{1}{2\\pi} \\,\\exp \\big(-\\tfrac12 (x^2 + y^2)\\big) \\,\\mathrm{d}x\\,\\mathrm{d}y .\n\\end{align} \\tag{13.1}\\] The joint PDF of \\((R, \\Theta)\\) is \\[ \\begin{align}\nf_{R, \\Theta}(r, \\theta) \\,\\mathrm{d}r\\,\\mathrm{d}\\theta &= f_R(r) \\,f_\\Theta(\\theta) \\,\\mathrm{d}r\\,\\mathrm{d}\\theta \\\\\n&= r\\exp \\big(-\\tfrac12 r^2\\big) \\, \\frac{1}{2\\pi}\\, \\mathrm{d}r\\,\\mathrm{d}\\theta\n\\end{align} . \\tag{13.2}\\]\nBut in Equation 13.1, we can substitute \\(r = x^2 + y^2\\) and \\(\\mathrm{d}x\\,\\mathrm{d}y = r\\,\\mathrm{d}r\\,\\mathrm{d}\\theta\\), and get Equation 13.2.\n\nSo we’ve reduced the problem of sample two normal distributions to sampling a Rayleigh (with scale parameter 1) and a uniform (on \\([0, 2\\pi]\\)). But we know how to do this. We saw in Example 13.2 that we can take \\(R = \\sqrt{-2 \\log U}\\), and we saw in the last lecture that we can take \\(\\Theta = 2\\pi V\\). We can then transform back into cartesian coordinates with \\[ \\begin{align}\nX &= R\\cos \\Theta =  \\sqrt{-2 \\log U} \\cos(2\\pi V) \\\\\nY &= R\\sin \\Theta =  \\sqrt{-2 \\log U} \\sin(2\\pi V) .\n\\end{align} \\]\nLet’s check the just the \\(X\\).\n\nn &lt;- 1e5\nunif1 &lt;- runif(n)\nunif2 &lt;- runif(n)\nrad &lt;- sqrt(-2 * log(unif1))\nang &lt;- 2 * pi * unif2\nsamples &lt;- rad * cos(ang)\n\nhist(samples, probability = TRUE, breaks = 50, xlim = c(-4, 4))\ncurve(dnorm(x), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nThis confirms that we get an excellent match to the normal distribution.\nNext time: Sampling using rejection.\n\nSummary:\n\nThe inverse \\(F^{-1}\\) of a CDF \\(F\\) is defined by \\(F^{-1}_X(u) = \\min \\big \\{x : F_X(x) \\geq u \\big\\}\\).\nThe inverse transform method converts \\(U \\sim \\operatorname{U}[0,1]\\) to a random variable with CDF by setting \\(X = F^{-1}(U)\\). That is: Set \\(U = F(X)\\), and rearrange to make \\(X\\) the subject.\nThe Box–Muller transform is a way to generate two independent standard normal distributions. Set \\(R\\) to Rayleigh with scale parameter 1, set \\(\\Theta \\sim \\operatorname{U}[0,2\\pi]\\), then take \\(X = R \\cos \\Theta\\) and \\(Y = R \\sin \\Theta\\).\n\nRead more: Voss, An Introduction to Statistical Computing, Section 1.3.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Inverse transform method</span>"
    ]
  },
  {
    "objectID": "lectures/L14-rejection.html",
    "href": "lectures/L14-rejection.html",
    "title": "14  Rejection sampling",
    "section": "",
    "text": "14.1 Rejection\nIn the last two lectures, we have taken standard uniform \\(U \\sim \\operatorname{U}[0,1]\\) random variables, and have applied a function to them to transform into some other distribution \\(X\\). One \\(U\\) gets turned into one \\(X\\). (Or, for the Box–Muller transform, two \\(U\\)s become two \\(X\\)s.)\nBut so far, we have taken each sample we are given. But another way to get a different distribution is to throw out samples we don’t like and wait until we get a sample we do like. This is called rejection sampling.\nSuppose we want not a \\(\\operatorname{U}[0,1]\\) random variable but instead a \\(\\operatorname{U}[0,\\tfrac12]\\) random variable. One way we’ve already seen to do this is by the inverse transform method: simply multiply \\(U\\) by \\(\\tfrac12\\). But we could also do this by rejection. We start with a proposed sample \\(U \\sim \\operatorname{U}[0,1]\\). If \\(U \\leq \\tfrac12\\), we “accept” the sample, and keep it. But if \\(U &gt; \\tfrac12\\), we “reject” the samples – we throw it away and ask for a new one. We keep proposing samples until we accept one that’s less than \\(\\tfrac12\\). It should be easy to convince yourself that we get a \\(\\operatorname{U}[0,\\tfrac12]\\) random variable this way. (But we’ll prove it later, if not.)\nThe advantage of rejection sampling is that it can help us get samples from some distributions that we couldn’t access with the inverse transform method. The disadvantage is that it can be costly or slow, because we may have to reject lots of samples before finding enough that we can accept. The more often we reject samples, the slower the procedure will be.\nRejection sampling is particularly useful for sampling from a conditional distribution, such as the conditional distribution of \\(Y\\) given that \\(Y \\in A\\): we simply accept a sample \\(y\\) if \\(y \\in A\\) and reject it if not.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "lectures/L14-rejection.html#rejection",
    "href": "lectures/L14-rejection.html#rejection",
    "title": "14  Rejection sampling",
    "section": "",
    "text": "Example 14.1 Let \\(Y \\sim \\operatorname{N}(0, 1)\\). Suppose we wish to use Monte Carlo estimation to estimate \\(\\mathbb E(Y \\mid Y \\geq 1)\\).\nTo do this, we will need samples from the conditional distribution \\(Y \\mid Y \\geq 1\\). So we accept proposed standard normal samples that are at least 1, and reject proposed samples that are less than 1.\nThere are two ways we could run this in practice. First, we could decide to take \\(n\\) proposal samples from \\(Y\\) and just see how many get accepted.\n\nn_prop &lt;- 1e6\nprops   &lt;- rnorm(n_prop)\naccepts &lt;- props[props &gt;= 1]\nlength(accepts)\n\n[1] 158514\n\nMCest1 &lt;- mean(accepts)\nMCest1\n\n[1] 1.526422\n\n\nWe end up accepting around 160,000 samples out of the 1,000,000 proposals we had to start with.\nSecond, we could keep proposing as many samples as needed until we reach some desired number of acceptances.\n\nn_acc &lt;- 1e5\n\nsamples &lt;- rep(0, n_acc)\ncount &lt;- 0\nfor (i in 1:n_acc) {\n  newsample &lt;- 0\n  while (newsample &lt; 1) {\n    newsample &lt;- rnorm(1)\n    count &lt;- count + 1\n  }\n  samples[i] &lt;- newsample\n}\ncount\n\n[1] 627437\n\nMCest2 &lt;- mean(samples)\nMCest2\n\n[1] 1.526969\n\n\nThis required taking about 630,000 proposals to get 100,000 acceptances.\nHere we used a “while” loop to keep taking samples until we go one that was not less than 1. The lines involving count were just so I could see how many proposals ended up being needed – these aren’t an integral part of the code.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "lectures/L14-rejection.html#acceptance-probability",
    "href": "lectures/L14-rejection.html#acceptance-probability",
    "title": "14  Rejection sampling",
    "section": "14.2 Acceptance probability",
    "text": "14.2 Acceptance probability\nSo far, we have looked at always accepting or always rejecting a proposed sample, depending on its value. But we could “perhaps” accept some proposals too. Suppose we are already sampling from some distribution \\(Y\\) (perhaps generated via the inverse transform method, for example). If we see the proposed sample \\(Y = x\\), we could accept it with some acceptance probability \\(\\alpha(x) \\in [0,1]\\). We can control the accepted samples more delicately by adjusting this acceptance function \\(\\alpha\\) to values that aren’t just 0 or 1.\nWhat is the distribution of an accepted sample \\(X\\)?\nWell, using Bayes’ theorem, we have in the discrete case \\[\\mathbb P(X = x) = \\mathbb P(Y = x \\mid \\text{accept}) = \\frac{\\mathbb P(Y = x)\\,\\mathbb P(\\text{accept} \\mid Y = x)}{\\mathbb P(\\text{accept})} = \\frac{1}{Z} \\alpha(x)\\,\\mathbb P(Y = x) .\\] where \\(Z = \\mathbb P(\\text{accept})\\) is the normalising constant. In the continuous case, with \\(g\\) the PDF of the original \\(Y\\) and \\(f\\) the PDF of the accepted \\(X\\), we have \\[ f(x) = g(x \\mid \\text{accept}) = \\frac{g(x)\\,\\mathbb P(\\text{accept} \\mid X = x)}{\\mathbb P(\\text{accept})} = \\frac{1}{Z}\\,\\alpha(x)\\,g(x) , \\] where \\(Z = \\mathbb P(\\text{accept})\\) again.\n\nExample 14.2 Suppose we wish to sample from the distribution \\[ f(x) \\propto \\exp\\big(-\\tfrac12x^2\\big)\\,(\\sin^2 x) . \\tag{14.1}\\] How can we do this?\nWell, we can note that the PDF of the standard normal is \\[ g(x) = \\frac{1}{2\\pi}\\,\\exp\\big(-\\tfrac12x^2\\big) \\propto \\exp\\big(-\\tfrac12x^2\\big) \\] and that \\[ 0 \\leq \\sin^2x\\leq 1 .\\] (Here, \\(\\sin^2 x\\) means \\((\\sin x)^2\\), by the way.) This means that, if we take proposals \\(Y \\sim \\operatorname{N}(0,1)\\), and then accept an proposed sample with probability \\(\\alpha(x) = \\sin^2 x\\), that will give us the distribution Equation 14.1.\n\nn_prop &lt;- 1e6\nprops &lt;- rnorm(n_prop)\naccepts &lt;- props[runif(n_prop) &lt;= sin(props)^2]\nlength(accepts)\n\n[1] 431972\n\nhist(accepts, probability = TRUE, breaks = 50)\ncurve(\n  0.92 * exp(-x^2 / 2) * sin(x)^2, add = TRUE, n = 1001,\n  lwd = 2, col = \"blue\"\n)\n\n\n\n\n\n\n\n\nBy rejecting lots of proposals with values near 0, we turned the unimodal (“one hump”)proposal distribution \\(Y \\sim \\operatorname{N}(0,1)\\) into this interesting bimodal (“two hump”) distribution.\nLet’s explain line 3 more carefully. We want to accept a proposal \\(x\\) with probability \\(\\sin^2 x\\). We saw in Lecture 12 that we can simulate a Bernoulli\\((p)\\) distribution by taking the value 1 is \\(U \\leq p\\) and taking 0 if \\(U &gt; p\\). So in line 3, we are accepting each proposed sample \\(x_i\\) if a standard uniform variate \\(u_i\\) satisfies \\(u_i \\leq \\sin^2 x_i\\).\nIn this example, we found we accepted about 430,000 samples.\n\nIn this example, we managed to sample from the PDF in Equation 14.1, \\[ f(x) = \\frac{1}{Z}\\,\\exp\\big(-\\tfrac12x^2\\big)\\,(\\sin^2 x) ,\\] even though we never found out what the normalising constant \\(Z\\) was. This idea – that we can sample from a distribution even if we only know it up to a multiplicative constant – is a very important one that will come up a lot later in this module.\nWe won’t go into that idea deeply now, but we briefly mention that it is very important in Bayesian statistics. In Bayesian statistics, the posterior distribution is often known only up to proportionality. That’s because we have \\[ \\begin{align}\n\\text{posterior} &\\propto \\text{prior}\\times\\text{likelihood} \\\\\n\\pi(\\theta \\mid x) &\\propto\\, \\pi(x) \\times p(x \\mid \\theta)\n\\end{align} \\] It’s often very difficult to find the normalising constant in this expression – indeed, it can be impossible in practice. So being able to sample from such a posterior distribution without finding the constant is very important for Bayesian statisticians.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "lectures/L14-rejection.html#how-many-samples",
    "href": "lectures/L14-rejection.html#how-many-samples",
    "title": "14  Rejection sampling",
    "section": "14.3 How many samples?",
    "text": "14.3 How many samples?\nWe have mentioned that the downside of rejection sampling is that we may have to take lots of samples to get enough accepted ones. Or, conversely, we may not get enough accepted samples from a fixed number of “proposed” samples. Remember that the accuracy of Monte Carlo estimation, for example, depends on how many samples we get – the mean-square error scales like \\(1/n\\) and the root-mean-square error like \\(1/\\sqrt{n}\\). So it’s important to be able to get a lot of samples \\(n\\).\nLet’s examine these questions a bit closer. Write \\(a = \\mathbb P(\\text{accept})\\) for the probability a sample \\(Y\\) gets accepted (a priori, before we have seen the value \\(Y = x\\) of that sample). In the discrete case, this is \\[ a = \\mathbb P(\\text{accept}) = \\sum_x \\mathbb P(Y = x)\\,\\alpha(x) , \\] and in the discrete case this is \\[ a  = \\mathbb P(\\text{accept}) = \\int_{-\\infty}^{+\\infty} g(x)\\,\\alpha(x)\\,\\mathrm{d}x . \\] In both cases, this can be written more succinctly as \\(a = \\Exg\\alpha(Y)\\).\nLet’s first look at the “fixed number of proposed samples” case.\nIf we take \\(m\\) proposed samples, each is accepted independently with probability \\(a\\). So the total number of accepted samples \\(N\\) follows a binomial distribution \\(N \\sim \\operatorname{Bin}(m, a)\\). This is because the binomial distribution describes the number of successes from \\(m\\) trials. This has expectation \\(am\\) and standard deviation \\(\\sqrt{a(1-a)m}\\).\nFor large \\(m\\), the standard deviation will be very small compared to the expectation (unless \\(a\\) is very close to \\(0\\) or \\(1\\)), so the number of acceptances \\(N\\) will be very close to \\(am\\). If we need a more precise approximation, \\(N\\) can be approximated by a normal distribution \\(N \\approx \\operatorname{N}(am, a(1-a)m)\\). When \\(a\\) is very small, \\(N\\) can be better approximated by a Poisson distribution \\(N \\approx \\operatorname{Po}(am)\\).\nThe second way is the “keep taking proposed samples until we have accepted enough of them” way.\nTo get one acceptance requires a geometric \\(\\operatorname{Geom}(a)\\) number of samples. This is because the geometric distribution counts the number of trials needed until the first success. To get \\(n\\) accepted samples will require the sum of \\(n\\) independent \\(\\operatorname{Geom}(a)\\) distributions – this is sometimes called the negative binomial distribution \\(M \\sim \\operatorname{NegBin}(n, a)\\), which is the number of trials needed until the \\(n\\)th success. This has expectation \\(n/a\\) and standard deviation \\(\\sqrt{(1-a)n}/a\\).\nFor large \\(n\\), the standard deviation will be very small compared to the expectation (unless \\(a\\) is very close to \\(0\\) or \\(1\\)), so the number of proposal \\(M\\) will be very close to \\(n/a\\). If we need a more precise approximation, \\(M\\) can be approximated by a normal distribution \\(M \\approx \\operatorname{N}(n/a, (1-a)n/a^2)\\). When \\(a\\) is very small, \\(N\\) can be better approximated by a Gamma distribution \\(M \\approx \\Gamma(n, a)\\).\n\nExample 14.3 In Example 14.1, we accepted \\(Y \\sim \\operatorname{N}(0,1)\\) if \\(Y \\geq 1\\). In this case, we happen to know the acceptance probability exactly: it’s\\(\\mathbb P(Y \\geq 1) = 0.158\\).\nFrom 1,000,000 proposals, the number of accepted samples we might get is shown below.\n\n\nCode for drawing this graph\nn_prop &lt;- 1e6\nacc_prob &lt;- pnorm(1, lower.tail = FALSE)\nplot(\n  0:2e5, dbinom(0:2e5, n_prop, acc_prob),\n  type = \"l\", col = \"blue\", lwd = 2,\n  xlab = \"number of accepted samples\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nWe see that the number of acceptance will be very close to \\(an = 0.158\\times 1\\,000\\,000 = 158\\,000\\).\nThe number of proposals required for 100,000 acceptances is shown below.\n\n\nCode for drawing this graph\nn_acc &lt;- 1e5\nacc_prob &lt;- pnorm(1, lower.tail = FALSE)\nplot(\n  n_acc + 0:7e5, dnbinom(0:7e5, n_acc, acc_prob),\n  type = \"l\", col = \"blue\", lwd = 2,\n  xlab = \"number of required proposals\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nThe number of required proposals will be very close to \\(n/a = 100\\,000 / 0.158 = 630\\,000\\).\n\n\nExample 14.4 Suppose instead we were trying to sample from a standard normal conditional on \\(Y &gt; 4\\). Again, we take a standard normal proposal and accept if the proposal is greater than 4. This is a different matter, because the acceptance probability is very small: about \\(0.00003\\).\nHere, the number of acceptances from 100,000 proposals is\n\n\nCode for drawing this graph\nn_prop &lt;- 1e5\nacc_prob &lt;- pnorm(4, lower.tail = FALSE)\nplot(\n  0:15, dbinom(0:15, n_prop, acc_prob),\n  type = \"h\", col = \"blue\", lwd = 3,\n  xlab = \"number of accepted samples\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nThere is likely to be a very small number of acceptances – and around a 4% chance there are no acceptances at all. The distribution of the number of acceptances is roughly Poisson.\nThe number of proposals required for just 5 acceptances, on the other hand, is:\n\n\nCode for drawing this graph\nn_acc &lt;- 5\nacc_prob &lt;- pnorm(4, lower.tail = FALSE)\nplot(\n  n_acc + 0:6e5, dnbinom(0:6e5, n_acc, acc_prob),\n  type = \"l\", col = \"blue\", lwd = 2,\n  xlab = \"number of required proposals\", ylab = \"probability\"\n)\n\n\n\n\n\n\n\n\n\nThis has a very wide spread of how many proposals will be required – anything between about 50,000 and 300,000 is reasonably plausible.\nThis shows the danger of running rejection sampling when the probability of acceptance is very small. Not only is the procedure slow and wasteful, but it’s also very unpredictable.\n\n(Annoyingly, R defines the geometric distribution to be the number of failures before the first success – 1 less then our definition – and the negative binomial to be the number of failures before the \\(n\\)th success – \\(n\\) less than our definition. This is why n_acc + dnbinom() was plotted for the two second graphs.)\nNext time. We look closer at rejection sampling, and in particular how we can target rejection sampling at a given distribution using the “envelope” method.\n\nSummary:\n\nIn rejection sampling, we accept a proposed sample \\(Y = x\\) with probability \\(\\alpha(x)\\).\nIf the PDF of a proposed sample is \\(g\\), then the PDF of an accepted sample is proportional to \\(\\alpha(x) \\,g(x)\\).\nWhen the acceptance probability is low, rejection sampling can require a lot of proposed samples to get enough accepted samples.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 1.4.1 and 1.4.3.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Rejection sampling</span>"
    ]
  },
  {
    "objectID": "lectures/L15-envelope-1.html",
    "href": "lectures/L15-envelope-1.html",
    "title": "15  Envelope rejection sampling I",
    "section": "",
    "text": "15.1 Sampling under the curve\nWe will start with a slightly different discussion, to try to motivate our approach. (If at any point you find this section confuses more than it helps, you can skip straight to the definition in the next section.)\nConsider a PDF \\(f\\), and draw \\(f\\) as a curve. We know that \\(f(x)\\) is always positive, so this curve is always above the x-axis. We also know that \\(\\int_{-\\infty}^{+\\infty} f(x)\\,\\mathrm{d}x = 1\\), so the total area under the curve is 1.\nSuppose we pick a point under the curve, uniformly at random, and then look at its x-coordinate \\(X\\). What is the PDF of \\(X\\)?\n[picture]\nWell, the probability that \\(X\\) lies in the interval \\([a,b]\\) is the probability the point we pick lies in the part of the curve between \\(a\\) and \\(b\\), which is the area of that area divided by the total area under the curve 1. So \\[ \\mathbb P(a \\leq X \\leq b) = \\int_a^b f(x) \\,\\mathrm{d}x .\\] But that’s just the probability associated with the PDF \\(f\\), so \\(X\\) has PDF \\(f\\).\n[picture]\nThis gives us a way – in theory, at least – to sample from a PDF \\(f\\). Simply pick a point at random under the curve, and take its x-coordinate. Unfortunately, there is no known way to do this directly in general, so it doesn’t really help.\nHowever, suppose we could find a function \\(h\\) that (a) is positive everywhere, (b) has finite area under the curve, (c is above \\(f\\) everywhere, (d) and that we could sample a point under. We could call such a curve an envelope for \\(f\\). We could then propose a point picked at random under the envelope \\(h\\), reject it if it were above the curve \\(f\\), but accept it if it were under the curve \\(f\\). An accepted point would be uniform under the curve \\(f\\), so its x-coordinate would have PDF \\(f\\), as required.\n[picture]\nBut how can we find such an envelope \\(h\\) we can use?\nIf the area under the envelope \\(h\\) is \\(c\\), with \\(1 \\leq c &lt; \\infty\\) (the area must be at least 1, if the curve is above \\(f\\)), then \\(h\\) must be over the form \\(h(x) = cg(x)\\) for a PDF \\(g\\). But picking the x-coordinate point at random under \\(g\\) is just sampling from the random variable that has PDF \\(g\\). And the probability that a point with x-coordinate \\(x\\) from under the envelope \\(cg(x)\\) also lies under \\(f\\) is \\(f(x)/cg(x)\\), so that would be our acceptance probability.\nThis tells us how to perform envelope rejection sampling.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Envelope rejection sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L15-envelope-1.html#the-envelope-rejection-sampling-algorithm",
    "href": "lectures/L15-envelope-1.html#the-envelope-rejection-sampling-algorithm",
    "title": "15  Envelope rejection sampling I",
    "section": "15.2 The envelope rejection sampling algorithm",
    "text": "15.2 The envelope rejection sampling algorithm\nIf I lost you during my motivational discussion, now is the time to switch back on! We are going to set out the steps of the envelope rejection sampling algorithm.\n\n\nGoal: To sample from a random variable \\(X\\) with PDF \\(f\\).\nWe will need: A random variable \\(Y\\) with PDF \\(g\\) that we can sample from, and a finite constant \\(c\\) such that \\(f(x) \\leq cg(x)\\) for all \\(x\\).\nStep 1: Sample a proposal \\(Y\\) from the PDF \\(g\\).\nStep 2: Accept the proposal \\(Y\\) with probability \\(f(Y) / cg(Y)\\); otherwise reject \\(Y\\). If accepted, end. If rejected, go back to Step 1.\n\n\nNote that for given \\(f\\) and \\(g\\), there’s no guarantee a finite constant \\(c\\) exists such that \\(f(x) \\leq cg(x)\\). Informally, we need \\(g\\) to have tails that are “at least as heavy” as the tails of \\(f\\).\nWe should check this really does sample from \\(f\\). We saw last time that the PDF of a sample had PDF \\(f(x) \\propto g(x) \\,\\alpha(x)\\). Here, that is \\[ f(x) \\propto g(x)\\,\\frac{f(x)}{cg(x)} = \\frac1c\\,f(x) , \\] as required.\nIn envelope rejection sampling, we reject proposals \\(x\\) with probability \\(f(x)/cg(x)\\). But we saw last time that rejecting samples is bad – higher rejection probabilities leave us with fewer accepted samples (or requiring more proposals to get the same number of accepted samples). So, for given \\(f\\) and \\(g\\), it would be good to pick \\(c\\) as small as possible. We must have \\(f(x) \\leq cg(x)\\) for all \\(x\\), so we must have \\(c \\geq f(x) / g(x)\\) for all \\(x\\), and therefore \\(c \\geq \\sup_x f(x)/g(x)\\). (Here, “sup” means “supremum” – it’s like the maximum, except allows for the fact the maximum might only be achieved in a limit, such as \\(x \\to -\\infty\\) or \\(x \\to +\\infty\\).) The best possible \\(c\\), therefore is to take \\(c\\) to equal this maximum, \\(c = \\sup_x f(x) / g(x)\\), if it’s possible to calculate it.\n[Note: I messed this up in the lecture, and accidentally said minimum/infimum, where it should be maximum/supremum.]",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Envelope rejection sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L15-envelope-1.html#examples",
    "href": "lectures/L15-envelope-1.html#examples",
    "title": "15  Envelope rejection sampling I",
    "section": "15.3 Examples",
    "text": "15.3 Examples\n\nExample 15.1 Consider the Wigner semi-circle distribution with PDF \\[ f(x) = \\frac{2}{\\pi}\\sqrt{1 - x^2} \\qquad -1 \\leq x \\leq 1 . \\]\nWe need to choose the envelope that surrounds the PDF \\(f\\). Here’s one choice. Let \\(Y \\sim \\operatorname{U}[-1,1]\\), which has PDF \\[ g(x) = \\tfrac12 \\qquad -1 \\leq x \\leq 1 ,\\] which well give a simple “box” around \\(f\\) for the envelope. We can also generate these samples easily, for example by using the inverse transform \\(Y = 2U - 1\\) for \\(U \\sim \\operatorname{U}[0,1]\\).\nThe maximum point of \\(f\\) is at \\(x = 0\\), where \\(f(0) = \\frac{2}{\\pi}\\) and \\(g(0) = \\tfrac12\\). So we see that \\(c \\geq \\frac{4}{\\pi}\\) will be sufficient to ensure \\(f(x) \\leq cg(x)\\) for all \\(x\\), with the box surrounding the semi-circle. We want to take the smallest possible \\(c\\), so will take \\(c = \\frac{4}{\\pi}\\), with the box just touching the semi-circle at its top.\n\npdf &lt;- function(x) (2 / pi) * sqrt(1 - x^2)\nenv &lt;- function(x) (4 / pi) * (1 / 2) * (-1 &lt;= x & x &lt;= 1)\n\n\n\nCode for drawing this graph\ncurve(\n  pdf, n = 1001, from = -1, to = 1,\n  xlim = c(-1.5, 1.5), ylim = c(0, 0.8), lwd = 3, col = \"blue\"\n)\ncurve(env, n = 1001, add = TRUE, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\nIn my code, I’ve set it up so that env() (the envelope function) is \\(c\\) times \\(g\\).\n\nn_prop &lt;- 1e6\nprops &lt;- 2 * runif(n_prop) - 1\naccepts &lt;- props[runif(n_prop) &lt;= pdf(props) / env(props)]\n\nlength(accepts)\n\n[1] 784769\n\nhist(accepts, probability = TRUE, breaks = 40)\ncurve(pdf, add = TRUE, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nWe see that we accepted about 78% or 79% of proposals, and the histogram is an excellent fit to the semicircle distribution.\n\n\nExample 15.2 Suppose we want to sample from the “half-normal” distribution \\[ f(x) = \\sqrt\\frac{2}{\\pi} \\exp\\big(\\tfrac12 x^2\\big) \\qquad x \\geq 0 . \\] Let’s take \\(Y \\sim \\operatorname{Exp}(1)\\) with \\(g(x) = \\mathrm{e}^{-x}\\). We know this can be easily sampled from, using the inverse transform \\(Y = - \\log U\\). The exponential also has fatter tails than the normal, so this should work.\nTo find an appropriate \\(c\\), we consider \\[ \\frac{f(x)}{g(x)} = \\frac{\\sqrt\\frac{2}{\\pi} \\exp(\\tfrac12 x^2)}{\\exp(-x)} = \\sqrt\\frac{2}{\\pi} \\exp \\big(-\\tfrac12 x^2 + x\\big) . \\] This is maximised where \\(-\\tfrac12 x^2 + x\\) is maximised which (by differentiating and setting equal to 0) is at \\(x = 1\\). So we take the best possible \\(c\\), which is \\[ c = \\frac{f(1)}{g(1)} = \\sqrt\\frac{2}{\\pi} \\exp\\big(-\\tfrac12 + 1\\big) = \\sqrt\\frac{2\\mathrm{e}}{\\pi} . \\]\n\npdf &lt;- function(x) sqrt(2 / pi) * exp(-x^2 / 2)\nenv &lt;- function(x) sqrt(2 * exp(1) / pi) * exp(-x)\n\n\n\nCode for drawing this graph\ncurve(\n  pdf, n = 1001, from = 0, to = 4,\n  xlim = c(0, 3), ylim = c(0, 1.32), lwd = 3, col = \"blue\"\n)\ncurve(env, n = 1001, from = 0, to = 4, add = TRUE, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nn_prop &lt;- 1e6\nprops &lt;- -log(runif(n_prop))\naccepts &lt;- props[runif(n_prop) &lt;= pdf(props) / env(props)]\n\nlength(accepts)\n\n[1] 759899\n\nhist(accepts, probability = TRUE, breaks = 40)\ncurve(pdf, add = TRUE, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nWe accept roughly 760,000 proposals, and get an excellent fit to the half-normal distribution.\n\nNext time. We study envelope rejection sampling more closely, and complete this part of the module on random number generation\n\nSummary:\n\nFor envelope rejection sampling from a PDF \\(f\\), we need a PDF \\(g\\) from which we can sample and a constant \\(c\\) such that \\(f(x) \\leq cg(x)\\) for all \\(x\\).\nWe propose samples from \\(g\\), and accept a proposal \\(x\\) with probability \\(f(x) / cg(x)\\).\nTo keep the acceptance probability high, we want to pick \\(c\\) as small as possible.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 1.4.2.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Envelope rejection sampling I</span>"
    ]
  },
  {
    "objectID": "lectures/L16-envelope-2.html",
    "href": "lectures/L16-envelope-2.html",
    "title": "16  Envelope rejection sampling II",
    "section": "",
    "text": "16.1 Acceptance probability\nConditional on seeing a proposal \\(Y = x\\), we saw that the acceptance probability is \\[ \\alpha(x) = \\frac{f(x)}{cg(x)} . \\] For any particular \\(x\\), this increases as \\(c\\) decreases, which is why we wanted \\(c\\) as small as possible, subject to \\(f(x) \\leq cg(x)\\) for all \\(x\\).\nBut what is the overall acceptance probability – unconditionally, before we’ve seen the value of the proposal?\nAs we saw before, the overall acceptance probability is \\[ \\operatorname{\\mathbb E}\\alpha(Y) = \\int_{-\\infty}^{+\\infty} \\alpha(x)\\,g(x)\\,\\mathrm{d}x . \\] Substituting in our value of \\(\\alpha\\), we have \\[ \\operatorname{\\mathbb E}\\alpha(Y) = \\int_{-\\infty}^{+\\infty} \\frac{f(x)}{cg(x)}\\,g(x)\\,\\mathrm{d}x = \\frac{1}{c} \\int_{-\\infty}^{+\\infty} f(x) \\, \\mathrm{d}x  = \\frac{1}{c} , \\] since the integral of PDF \\(f\\) is 1. Thus the unconditional acceptance probability is therefore \\(1/c\\). To put it another way, the expected number of proposals per acceptance is \\(c\\).\nThis is another even more direct reason why we should want to take \\(c\\) as small as possible.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Envelope rejection sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L16-envelope-2.html#acceptance-probability",
    "href": "lectures/L16-envelope-2.html#acceptance-probability",
    "title": "16  Envelope rejection sampling II",
    "section": "",
    "text": "Example 16.1 In Example 15.1, we took \\(c = \\frac{4}{\\pi}\\), so \\(1/c = \\frac{\\pi}{4} = 0.785\\). We saw that we accepted 78% or 79% of the proposals.\nIn Example 15.2, we took \\[c = \\sqrt\\frac{2\\mathrm{e}}{\\pi} \\qquad \\frac{1}{c} = \\sqrt{\\frac{\\pi}{2\\mathrm{e}}} = 0.760.\\] We saw that we accepted about 76% of proposals.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Envelope rejection sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L16-envelope-2.html#unnormalised-measures",
    "href": "lectures/L16-envelope-2.html#unnormalised-measures",
    "title": "16  Envelope rejection sampling II",
    "section": "16.2 Unnormalised measures",
    "text": "16.2 Unnormalised measures\nWe briefly mentioned in Lecture 14 that it can be useful to be able to be able to sample from PDFs even when we only know the PDF up to proportionality, and that this is particularly useful in Bayesian statistics.\nSuppose we want to sample from a PDF \\[ f(x) = \\frac{1}{Z}\\,\\mu(x) , \\] where the measure \\(\\mu\\) is known, but the normalising constant \\[ Z = \\int_{-\\infty}^{+\\infty}\\mu(x) \\,\\mathrm{d}x \\] is unknown (but finite). Can we do this with envelope rejection sampling?\nThe answer is yes – and algorithm is basically exactly the same, just with \\(f\\) replaced by \\(\\mu\\).\n\n\nGoal: To sample from a random variable \\(X\\) with PDF proportional to the measure \\(\\mu\\).\nWe will need: A random variable \\(Y\\) with PDF \\(g\\) that we can sample from, and a finite constant \\(c\\) such that \\(\\mu(x) \\leq cg(x)\\) for all \\(x\\).\nStep 1: Sample a proposal \\(Y\\) from the PDF \\(g\\).\nStep 2: Accept the proposal \\(Y\\) with probability \\(\\mu(Y) / cg(Y)\\); otherwise reject \\(Y\\). If accepted, end. If rejected, go back to Step 1.\n\n\nTo check that this still works, we remember that the accepted PDF is proportional to \\[ \\alpha(x) \\,g(x) = \\frac{\\mu(x)}{cg(x)}\\,g(x) = \\frac{1}{c} \\mu(x) \\propto \\mu(x) \\propto f(x) . \\]\n(In this case, \\(1/c\\) is only proportional to, not directly equal to, the unconditional accpetance probability.)\n\nExample 16.2 The von Mises distribution is a distribution on \\([0, 2\\pi)\\) with PDF \\[f(x) \\propto \\mu(x) = \\exp\\big(\\kappa \\cos (x-m)\\big), \\]for some parameters \\(m \\in [0, 2\\pi)\\) and \\(\\kappa \\geq 0\\). The von Mises distribution is used to model data on a circle, with \\(x\\) being the angle around the circle. Circular data might be a compass direction (north/south/east/west), or a time of day (with the circle representing the hours from midnight to midnight). The von Mises distribution is sort of a “circular equivalent” of the normal distribution on the line, with \\(m\\) being the mean value and \\(\\kappa\\) playing a similar role \\(1/\\sigma^2\\). There is no closed form for the constant of proportionality.\nWe wish to sample from the von Mises distribution with \\(m = 0\\) and \\(\\kappa = 2\\), where the unnormalised measure is \\(\\mu(x) = \\exp(2\\cos x)\\). We choose the uniform distribution \\(g(x) = \\frac{1}{2\\pi}\\) on \\([0, 2\\pi)\\) for our envelope, which we can easily sample from as \\(Y = 2\\pi U\\). To choose \\(c\\), we note that the maximum of \\(\\mu(x)\\) is at \\(x = 0\\), where it takes the value \\(\\exp(2)\\). So we take \\(c = \\mu(0)/g(0) = 2\\pi\\mathrm{e}^2\\).\n\nmeas &lt;- function(x) exp(2 * cos(x))\nenv  &lt;- function(x) 2 * pi * exp(2) * (1 / (2 * pi)) * (x &lt;= 2 * pi)\n\n\n\nCode for drawing this graph\ncurve(\n  meas, n = 1001, from = 0, to = 2 * pi,\n  lwd = 3, col = \"blue\"\n)\ncurve(env, n = 1001, add = TRUE, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n(This graph might look a little odd, but remember that \\(0\\) and \\(2\\pi\\) are the same part of the circle, so this measure takes its largest values around that “join”.)\n\nn_prop &lt;- 1e6\nprops &lt;- 2 * pi * (runif(n_prop))\naccepts &lt;- props[runif(n_prop) &lt;= meas(props) / env(props)]\n\nlength(accepts)\n\n[1] 308831\n\nhist(accepts, probability = TRUE, breaks = 2 * pi * (0:30) / 30)\ncurve(meas(x) / 14.3, add = TRUE, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nWe accepted about 31% of proposals.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Envelope rejection sampling II</span>"
    ]
  },
  {
    "objectID": "lectures/L16-envelope-2.html#summary-of-part-ii",
    "href": "lectures/L16-envelope-2.html#summary-of-part-ii",
    "title": "16  Envelope rejection sampling II",
    "section": "16.3 Summary of Part II",
    "text": "16.3 Summary of Part II\nThis completes our study of random number generation. This would be a good time to summarise what we have learned.\nFirst we discussed generating randomness.\n\nWe can generate random numbers uniform on \\([0,1]\\) through true physical randomness or by a pseudorandom number generator.\nLCGs are one type of pseudorandom number generator. An LCG is a recurrence \\(x_{n+1} = (ax_n + c) \\bmod m\\).\nConditions for an LCG to have full period of \\(m\\) are given by the Hull–Dobell theorem: if \\(m\\) is a power of 2, then we need \\(c\\) to be odd and \\(a\\) to be \\(1 \\bmod 4\\). [Note: In the lecture, I wrongly said we need \\(c\\) to be even; in fact, we need \\(c\\) to be odd.]\n\nThen we discussed manipulating standard uniform samples into other distributions.\n\nThe inverse transform method uses the cumulative distribution function: set \\(U = F(X)\\) and invert to make \\(X\\) the subject.\nDiscrete random variables can be simulated by splitting up the intervals \\([0,1]\\) into segments of length \\(p(x_i)\\), then seeing which segment \\(U\\) falls into.\nTwo normal distributions can be sampled using the Box–Muller theorem and polar coordinates. Let \\(R\\) be Rayleigh distributed, \\(\\Theta\\) be uniform on \\([0, 2\\pi)\\), then set \\(X = R \\cos \\Theta\\) and \\(Y = R \\sin \\Theta\\).\nWe can also get different distributions by accepting a proposed sample \\(Y = x\\) from \\(g\\) with probability \\(\\alpha(x)\\). The PDF of an accepted sample is \\(f(x) \\propto \\alpha(x)\\,g(x)\\).\nEnvelope rejection sampling is a way to target rejection sampling a PDF \\(f\\), by choosing an “envelope” \\(cg(x)\\), and accepting a sample from \\(g\\) with probability \\(f(x)/cg(x)\\).\nRejection sampling works best when the acceptance probability is made as large as possible.\n\nNext time. We begin our study on MCMC: Markov chain Monte Carlo.\n\nSummary:\n\nEnvelope rejection sampling with envelope \\(cg(x)\\) has unconditional acceptance probability \\(1/c\\).\nEnvelope rejection sampling works even if the desired distribution is only know up to a proportionality constant.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 1.4.2.",
    "crumbs": [
      "Random number generation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Envelope rejection sampling II</span>"
    ]
  },
  {
    "objectID": "problems/P3.html",
    "href": "problems/P3.html",
    "title": "Problem Sheet 3",
    "section": "",
    "text": "Full solutions are now available.\n\n\nThis is Problem Sheet 3, which covers material from Lectures 12 to 16. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Thursday 14 November. If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Mondays at 1500.\nThis problem sheet is to help you practice material from the module. It is not for assessment and will not be marked.\nFull solutions should be released on Friday 15 November.\n\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.     Consider a discrete random variable that takes values \\(1, 2, 3, 4.5\\) with probabilities \\(0.2, 0.2, 0.5, 0.1\\) respectively. Write some R code that will sample from this distribution. (Your code may use the runif() function, but may not use the sample() function.) Check that a large sample from your code really does have the correct distribution.\n\nSolution. There are various ways to do this. With only four outcomes, you can just write a lot of this “by hand”, in a way that wouldn’t be practical if the range were very large, but it pretty easy to write – that’s what I did.\n\nrq1 &lt;- function(n) {\n  x &lt;- c(1, 2, 3, 4.5)\n  prob &lt;- c(0.2, 0.2, 0.5, 0.1)\n  cumprob &lt;- cumsum(prob)\n  unif &lt;- runif(n)\n  ifelse(unif &lt;= cumprob[1], x[1],\n  ifelse(unif &lt;= cumprob[2], x[2],\n  ifelse(unif &lt;= cumprob[3], x[3], x[4])))\n}\n\n                Let’s test its accuracy by drawing a bar plot of a large sample.\n\nn &lt;- 1e6\nsamples &lt;- rq1(n)\nplot(table(samples) / n)\n\n\n\n\n\n\n\n\n                Looks good to me.\n\n\n\n2.     The geometric distribution \\(X \\sim \\operatorname{Geom}(p)\\) represents the number of trials until the first success, where each trial succeeds independently with probability \\(p\\). The probability mass function of \\(X\\) is \\[ p(x) = (1-p)^{x-1}p \\qquad x = 1, 2, \\dots. \\]\n\n(a)  Show that the cumulative distribution function \\(F\\) of \\(X\\) is given by \\[ F(x) = 1 - (1-p)^x \\qquad x = 1, 2, \\dots .\\]\n\nSolution. I can think of two ways to do this. The first way is to just sum the probabilities, using the formula \\[ \\sum_{z=0}^{x-1} a^z = \\frac{1 - a^x}{1-a} \\] for the sum of a geometric progression. We have \\[ F(x) = \\sum_{y=1}^x (1-p)^{y-1} p = p \\sum_{z=0}^{x-1} (1-p)^z = p \\frac{1 - (1-p)^{x}}{1 - (1-p)} = 1 - (1-p)^x , \\] where in the second equality where shifted the index of the sum with \\(z = y-1\\).\nAlternatively (and, in my opinion, better) is to think about what the geometric distribution means. The geometric probability \\(p(x)\\) is the probability the first success occurs on the \\(x\\)th trial. So the complement of the CDF, \\(1 - F(x) = \\mathbb P(X &gt; x)\\) is the probability the first success happens after the \\(x\\)th trial, which is if and only if the first \\(x\\) trials are all failures. This is \\[ 1 - F(x) = \\mathbb P(X &gt; x) = (1-p)^x \\] because each of those \\(x\\) trials fails with probability \\(1 - p\\). This gives the answer.\n\n\n\n(b)  Write down a function – either in mathematical notation or in R code – that will transform a standard uniform random variable \\(U\\) into a geometric distribution. Try to make your function as simple as possible.\n\nSolution. The “dividing lines” between the segments come at the values \\(u\\) where \\(u = 1- (1-p)^x\\). Actually, since we’re trying to make the function simple, we could use \\(v = 1-u\\) (since one minus a standard uniform is still standard uniform), with dividing lines at \\(v = (1-p)^x\\), which corresponds to \\(x = \\log v / \\log(1-p)\\). So the segment corresponding to a uniformly distributed \\(v\\) will correspond to this value rounded up to the next integer.\nSo we can take \\(U \\sim \\operatorname{U}[0,1]\\), and put \\[ X = \\left\\lceil \\frac{\\log U}{\\log(1-p)} \\right\\rceil . \\]\nLet’s check this with R code\n\nrgeom2 &lt;- function(n, p) {\n  unif &lt;- runif(n)\n  ceiling(log(unif) / log(1 - p))\n}\n\nn &lt;- 1e6\np &lt;- 1/3\nsamples &lt;- rgeom2(n, p)\nplot(table(samples) / n, xlim = c(0, 10))\n\n\n\n\n\n\n\n\nWe could alternatively check the probabilities exactly.\n\ntrue &lt;- (1 - p)^{1:8 - 1} * p\nemp &lt;- table(samples)[1:8] / n\nround(rbind(true, emp), 4)\n\n          1      2      3      4      5      6      7      8\ntrue 0.3333 0.2222 0.1481 0.0988 0.0658 0.0439 0.0293 0.0195\nemp  0.3327 0.2225 0.1485 0.0986 0.0658 0.0439 0.0294 0.0197\n\n\nLooks good again.\n\n\n\n\n3.     Consider a Cauchy random variable \\(X\\) with probability density function \\[ f(x) = \\frac{1}{\\pi(1 + x^2)} .\\]\n\n(a)  Show that the cumulative distribution function of \\(X\\) is \\[ F(x) = \\frac12 + \\frac{1}{\\pi}\\arctan x \\]\n\nSolution. The CDF is \\[ \\begin{multline}\nF(x) = \\int_{-\\infty}^x f(y)\\,\\mathrm{d}y\n  = \\frac{1}{\\pi} \\int_{-\\infty}^x \\frac{1}{1+y^2}\\,\\mathrm{d}y\n  = \\frac{1}{\\pi} \\big[\\arctan y\\big]_{-\\infty}^x \\\\ = \\frac{1}{\\pi} \\bigg( \\arctan x - \\Big(-\\frac{\\pi}{2}\\Big)\\bigg) = \\frac{1}{\\pi}\\arctan x + \\frac12,\n\\end{multline}\\] since \\(\\lim_{y\\to-\\infty} \\arctan y = -\\frac{\\pi}{2}\\).\n\n\n\n(b)  Write down a function that will transform a standard uniform random variable \\(U\\) into a Cauchy distribution.\n\nSolution. We use the inverse transform method: write \\(U = F(X)\\) and invert. Here, we have \\[ U = \\frac12 + \\frac{1}{\\pi}\\arctan X .\\] Inverting gives \\[X = \\tan \\bigg(\\pi \\Big(U - \\frac12\\Big) \\bigg) .\\]\nThis can be interpreted as \\(X = \\tan\\Theta\\) where \\(\\Theta\\) is uniform between \\(-\\frac{\\pi}{2}\\) and \\(\\frac{\\pi}{2}\\).\n\n\n\n(c)  Using your answer to part (b), draw a histogram of samples from the Cauchy distribution in R.\n\nSolution. My function for generating samples is the following:\n\nrcauchy2 &lt;- function(n) {\n  unif &lt;- runif(n)\n  tan(pi * (unif - 1/2))\n}\n\nThe Cauchy distribution is a very heavy-tailed distribution. So to make the histogram look sensible, I’m going to throw away the occasional very large or very small sample. If I didn’t do this, the histogram would probably just look like a single spike at 0. (You of course shouldn’t do this when performing Monte Carlo estimation – those rare very large or very small samples can be extremely important in calculations!)\n\nn &lt;- 1e6\nsamples &lt;- rcauchy2(n)\n\nsamples_rest &lt;- samples[abs(samples) &lt; 12]\nhist(samples_rest, probability = TRUE, ylim = c(0, 0.33), breaks = 48)\ncurve(1 / (pi * (1 + x^2)), add = TRUE, n = 1001, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n4.     Let \\(F\\) be a cumulative distribution function and \\(F^{-1}\\) its inverse.\n\n(a)  Prove that \\(F^{-1}\\) is a non-decreasing function.\n\nSolution. We recall the definition \\(F^{-1}(u) = \\min \\{x : F(x) \\geq u\\}\\). As \\(u\\) increases, the set \\(A_u = \\{x : F(x) \\geq u \\}\\) gets smaller – more specifically, for \\(u \\leq v\\), we have \\(A_u \\subseteq A_v\\). Hence the minimum of the set \\(A_u\\) cannot be larger than the minimum of the set \\(A_v\\). Hence \\(F^{-1}(u) = \\min A_u \\leq \\min A_v = F^{-1}(v)\\), as required.\n\n\n\n(b)  Show that \\(X = F^{-1}(U)\\) and \\(X' = F^{-1}(1-U)\\) have negative (or, rather, non-positive) correlation. You may use any results from the module, provided you state them clearly.\n\nSolutions. The relevant result here is Theorem 7.2. This said that if \\(\\phi\\) is an non-decreasing function, then \\(\\phi(U)\\) and \\(\\phi(1-U)\\) have covariance – and therefore correlation – less than or equal to 0. Here, we use \\(F^{-1}\\) as the function \\(\\phi\\).\n\n\n\n\n5.     Let \\(X \\sim \\operatorname{Beta}(3, 2)\\) be a Beta distribution with PDF \\[ f(x) = 12 x^2(1-x) \\qquad 0 \\leq x \\leq 1 . \\] [Note: An earlier version of this question wrongly had the constant at the front as \\(\\frac{1}{12}\\) instead of \\(12\\).]\nShow how you could sample from \\(X\\) using envelope rejection sampling and an optimised value of the constant \\(c\\).\n\nSolution. The obvious suggestion here is to take \\(Y\\) uniformly distributed on \\([0, 1]\\), so \\(g(x) = 1\\) (although you don’t have to choose that).\nTo find the optimal value of \\(c\\), we consider the maximum of \\(f(x)\\). We can find that by differentiating \\[ f'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} (12x^2 - 12x^3) = (24x - 36x^2) , \\] so the maximum is at \\(x = \\frac23\\), where \\(f(x) = \\frac{16}{9}\\). Therefore, we take \\(c = \\frac{16}{9}\\).\nThus, our algorithm is to sample from a standard uniform, and then to accept with probability \\[ \\alpha(x) = \\frac{12x^2(1-x)}{\\frac{16}{9}} = \\frac{27}{4}\\,x^2(1-x) . \\]\n\n\n\n6.     Consider sampling from the half-normal distribution \\[ f(x) = \\sqrt{\\frac{2}{\\pi}} \\exp\\big(-\\tfrac12 x^2\\big) \\qquad x \\geq 0 \\] using envelope rejection sampling with an \\(\\operatorname{Exp}(\\lambda)\\) proposal distribution \\[g(x) = \\lambda \\mathrm{e}^{-\\lambda x} \\qquad x \\geq 0 .\\] You wish to design your envelope rejection sampling algorithm so that the acceptance probability is as high as possible.\n\n(a)  For fixed \\(\\lambda\\), show that the optimal value of \\(c\\) is \\[c = \\sqrt{\\frac{2}{\\pi}}\\,\\frac{\\exp(\\frac12\\lambda^2)}{\\lambda}.\\]\n\nSolution. This is very similar to Example 15.2 in Lecture 15. We have \\[ \\frac{f(x)}{g(x)} = \\frac{\\sqrt\\frac{2}{\\pi} \\exp(\\tfrac12 x^2)}{\\lambda \\exp(-\\lambda x)} = \\sqrt\\frac{2}{\\pi} \\,\\frac{\\exp \\big(-\\tfrac12 x^2 + \\lambda x\\big)}{\\lambda} . \\] We want to pick \\(c\\) to be the maximum value of this. The maximum occurs where \\(-\\frac12x^2 + \\lambda x\\) is maximised. By differentiating this and setting equal to 0, we get \\(-x + \\lambda = 0\\), so \\(x = \\lambda\\) and \\[ c =  \\sqrt\\frac{2}{\\pi} \\,\\frac{\\exp \\big(-\\tfrac12 \\lambda^2 + \\lambda^2\\big)}{\\lambda} =   \\sqrt\\frac{2}{\\pi} \\,\\frac{\\exp \\big(\\tfrac12 \\lambda^2\\big)}{\\lambda}, \\] as required.\n\n\n\n(b)  Show that the optimal value of \\(\\lambda\\) is \\(\\lambda = 1\\).\n\nSolution. Again, our goal is to choose \\(\\lambda\\) get \\(c\\) as small as possible, since \\(1/c\\) is the acceptance probability. Differentiating the expression from part (a) with respect to \\(\\lambda\\) gives \\[ \\frac{\\mathrm d}{\\mathrm d\\lambda} \\,c = \\sqrt\\frac{2}{\\pi} \\frac{\\exp \\big(\\tfrac12 \\lambda^2\\big)(\\lambda^2 - 1)}{\\lambda^2}.\\] This is 0 when \\(\\lambda^2 = 1\\), and since \\(\\lambda \\geq 0\\), the only solution is \\(\\lambda = 1\\). (This can be easily checked to be a minimum by looking at a sketch of \\(c\\) against \\(\\lambda\\), or differentiating twice, or just by thinking about the behaviour of \\(c\\) as a function of \\(\\lambda\\).)\n\n\n\n\n7.      [2016 exam, Question 1]  In this question, we consider generating samples from the distribution with probability density function \\[ f_a(x) = \\frac{1}{Z} \\,\\frac{1}{a(\\cos x + 1) + x^2} \\qquad x \\geq 1,\\] where \\(a\\) is a parameter and \\[Z = \\int_{1}^{\\infty} \\frac{1}{a(\\cos x + 1) + x^2}\\,\\mathrm{d}x \\] is a normalising constant.\n\n(a)  Introducing any notation you use, state the inverse transform method for random number generation.\n\nSolution. Fix a cumulative distribution function (CDF) \\(F(x) = \\mathbb P(X \\leq x)\\). The inverse CDF is defined to be \\(F^{-1}(u) = \\max \\{x : F(x) \\geq u\\}\\). We then have that \\(X = F^{-1}(U)\\) has CDF \\(F\\). We use this typically by writing \\(U = F(X)\\) and inverting to make \\(X\\) the subject.\n\n\n\n(b)  For \\(a = 0\\), explain how the inverse transform method can be used to generate samples with PDF \\(f_0\\).\n\nFor \\(a = 0\\), the PDF is \\[ f_0(x) = \\frac{1}{Z} \\,\\frac{1}{x^2} \\qquad x \\geq 1 .\\]\nWe should start by finding the normalising constant \\(Z\\): it’s \\[ Z = \\int_1^\\infty \\frac{1}{x^2}\\,\\mathrm{d}x = \\big[-x^{-1}\\big]_1^\\infty = 0 -(-1) = 1 , \\] so the PDF is simply \\(f_0(x) = 1/x^2\\).\nNext, we want to find the CDF \\(F\\): it’s \\[ F(x) = \\int_1^x f_0(y)\\,\\mathrm{d}y = \\int_1^x \\frac{1}{y^2}\\,\\mathrm{d}y = \\big[-y^{-1}\\big]_1^x = 1 - \\frac{1}{x} . \\]\nFinally, to perform the inverse transform method, we write \\(U = F(X)\\) and invert. We have \\(U = 1 - 1/x\\), and so \\(X = 1/(1-U)\\).\n\n\n\n(c)  Introducing any notation you use, state the envelope rejection sampling method for random number generation.\n\n\n(d)  For \\(a &gt; 0\\), explain how the envelope rejection sampling method can be used to generate samples with PDF \\(f_a\\).\n\n\n(e)  How does the efficiency of your method in part (d) change as the value of \\(a\\) increases? Justify your answer.",
    "crumbs": [
      "Random number generation",
      "Problem Sheet 3"
    ]
  },
  {
    "objectID": "lectures/L17-markov-intro.html",
    "href": "lectures/L17-markov-intro.html",
    "title": "17  Markov chains in discrete space",
    "section": "",
    "text": "17.1 Markov chains and MCMC\nIn the first part of this module, we looked at Monte Carlo methods, where we estimated \\(\\theta = \\Exg\\phi(X)\\) by \\[ \\widehat{\\theta}_n = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] where \\(X_1, X_2, \\dots, X_n\\) are IID samples from the same distributions as \\(X\\). This worked well for relatively simple distributions \\(X\\), like those that can be sampled using the inverse transform method or rejection sampling (or a built-in R function). These tended to be one-dimensional distributions, sometimes with a simple dependence on one parameter.\nHowever, these methods are often not available when dealing with very complex distributions \\(X\\). For example, these might not be one-dimensional but rather living in some very high-dimensional space. Or they might depend on many parameters – and those parameters might themselves be drawn from random distributions (a so-called “hierarchical model”).\nFor more complicated distributions, we can make progress by loosening the assumption that \\(X_1, X_2, \\dots, X_n\\) are IID copies of \\(X\\).\nThe way will do this is to allow \\(X_1, X_2, \\dots\\) to be a random process (or “stochastic” process) that has a particular dependence structure known as the Markov property. Such a process is known as a Markov chain. Random variables in Markov chains can be shown (under certain conditions) to tend to a certain distribution – we will want to set things up so that that “limiting distribution” is the distribution \\(X\\) we want to sample from.\nUsing a Monte Carlo estimator where the samples \\(X_1, X_2, \\dots\\) are not IID but rather form a Markov chain is known as Markov chain Monte Carlo – although it’s almost always referred to by the abbreviation MCMC. MCMC is one of the most important ideas in statistics in the second-half of the 20th and in the 21st centuries, and is especially important in Bayesian statistics.\nIn this part of the of the module, we will study MCMC in depth. We will take a brief tour through the theory of Markov chains, then talk about how to use the output of a Markov chain for Monte Carlo estimation. We will look specifically at the Metropolis–Hastings algorithm, which is one way of setting up a Markov chain to have a specific distribution as its limiting distribution, and has some properties in common with rejection sampling ideas we have already seen.\nThe schedule will be:\nSome of you may have studied Markov chains before – for example, in the Leeds second-year module MATH2750 Introduction to Markov Processes. If so, you should find that today and the next lecture are just a brief reminder of things you already know, but the rest of the material is likely to be new.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Markov chains in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L17-markov-intro.html#markov-chains-and-mcmc",
    "href": "lectures/L17-markov-intro.html#markov-chains-and-mcmc",
    "title": "17  Markov chains in discrete space",
    "section": "",
    "text": "Rather than the \\(X_i\\) having exactly the same distribution as \\(X\\), we might be willing for them to have approximately the same distribution as \\(X\\), and reach the same distribution in the limit as \\(i \\to \\infty\\).\nRather than having the \\(X_i\\) be independent, we could let them have some dependence, but we will want to set things up so that we limit the dependence so there is only “light” dependence and so that we understand the dependence structure well.\n\n\n\n\n\n\nToday and Lecture 18: Theory of Markov chains in discrete space\nLecture 19: Metropolis–Hastings algorithm in discrete space\nLecture 20: Theory of Markov chains in continuous space\nLecture 21: Metropolis–Hastings algorithm in continuous space\nLectures 22 and 23: MCMC in practice (including for Bayesian statistics).",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Markov chains in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L17-markov-intro.html#introduction-to-markov-chains",
    "href": "lectures/L17-markov-intro.html#introduction-to-markov-chains",
    "title": "17  Markov chains in discrete space",
    "section": "17.2 Introduction to Markov chains",
    "text": "17.2 Introduction to Markov chains\nA Markov chain in discrete time \\(i = 1, 2, \\dots\\) and discrete space \\(\\mathcal S\\) is a sequence of random variables \\((X_1, X_2, X_3, \\dots)\\). The random variables are not independent, but their dependence is limited to just the random variable before it in the list. That is, the next state \\(X_{i+1}\\) can depend on on the current state \\(X_i\\); but, given the current state \\(X_i\\), it has no further dependence on the past states \\(X_{i-1}, X_{i-2}, \\dots, X_2, X_1\\). This is known as the “Markov property”.\nThink of playing a simple board game where you roll a dice and move that many squares along the board. Let \\(X_i\\) be the current square you are on. The the next square you land on, \\(X_{i+1}\\):\n\nis random – because it depends on the roll of the dice;\ndepends on which square \\(X_i\\) you are on now – because the value of dice roll will be added to your current square;\ngiven the square \\(X_i\\) you are on now, it doesn’t depend which sequence of squares \\(X_1, X_2, \\dots, X_{i-1}\\) you previously landed on to get there.\n\n\nDefinition 17.1 A sequence of random variables \\((X_i) = (X_1, X_2, \\dots)\\) taking values in a countable state space \\(\\mathcal S\\) is said to be a Markov chain or to have the Markov property if \\[ \\mathbb P(X_{i+1} = x_{i+1} \\mid X_i = x_i, X_{i-1} = x_{i=1}, \\dots, X_1 = x_1)\n= \\mathbb P(X_{i+1} = x_{i+1} \\mid X_i = x_i)\\] for all \\(i = 1, 2, \\dots\\) and for all \\(x_1, \\dots, x_{i-1}, x_{i}, x_{i+1} \\in \\mathcal S\\) such that the conditional probability is defined.\n\n\nExample 17.1 Consider a simple model of an unreliable printer:\n\nOn day 1, the printer is working.\nIf the printer is working, then the next day there is a 90% chance it is still working, but a 10% chance it has broken.\nIf the printer is broken, then the next day there is a 50% chance it has been mended, but a 50% chance it is still broken.\n\nWe can model this as a Markov chain on the state space \\(\\mathcal S = \\{1, 2\\}\\), where state 1 denotes that the printer is working and state 2 denotes that the printer is working. We have \\[ \\begin{align}\n\\mathbb P(X_{i+1} = 1 \\mid X_i = 1) &= 0.9 & \\mathbb P(X_{i+1} = 2 \\mid X_i = 1) &= 0.1 \\\\\n\\mathbb P(X_{i+1} = 1 \\mid X_i = 2) &= 0.5 & \\mathbb P(X_{i+1} = 2 \\mid X_i = 2) &= 0.5.\n\\end{align} \\]\n\n\nExample 17.2 Consider the simple random walk on \\(\\mathcal S = \\mathbb Z\\). We start at \\(X_1 = 0\\). At each time step, we move up 1 with probability \\(p\\) and down one with probability \\(q = 1-p\\); so \\[ \\mathbb P(X_{i+1} = y \\mid X_i = x) = \\begin{cases} p & \\text{ if }y = x+1 \\\\ q & \\text{ if }y = x-1 \\\\\n0 & \\text{ otherwise}. \\end{cases} \\]\nIf \\(p = q = \\tfrac12\\), this is called the simple symmetric random walk.\nWe can also write this as \\[ X_{i+1} = X_i + Z_i ,  \\tag{17.1}\\] where the \\(Z_i\\) are IID with distribution \\[ Z_i = \\begin{cases} +1 & \\text{ with probability } p \\\\ -1 & \\text{ with probability } q. \\end{cases} \\] Any Markov chain with the structure Equation 17.1 for an IID sequence \\((Z_i)\\) is called a random walk. If the \\(Z_i\\) are symmetric, in that \\(\\mathbb P(Z_i = +z) = \\mathbb P(Z_i = -z)\\) for all \\(z\\), then it is a symmetric random walk.\n\nIn both the Markov chains we have looked at – and, indeed, all the Markov chains we will ever look at – the transition probability \\(p(x, y) = \\mathbb P(X_{i+1} = y \\mid X_i = x)\\) was the same for all \\(i\\). That is, the probability \\(p(x, y)\\) of moving from \\(x\\) to \\(y\\) does not depend on which timestep \\(i\\) we are at. This is called being time homogeneous.\nOnce we have the notation \\(p(x, y)\\) for the transition probability, it will in fact be useful to write them in a matrix \\(\\mathsf P = (p(x,y))\\), called the transition matrix.\nFor the two-state “unreliable printer” Markov chain, the transition matrix is \\[ \\mathsf P = \\begin{pmatrix} 0.9 & 0.1 \\\\ 0.5 & 0.5 \\end{pmatrix} . \\]\nFor the simple random walk, the transition matrix is the “infinite matrix” \\[ \\mathsf P = \\begin{pmatrix}\n\\smash\\ddots      & \\smash\\ddots & \\phantom{\\smash\\ddots}  &  \\phantom{\\smash\\ddots}  & \\phantom{\\smash\\ddots} & \\phantom{\\smash\\ddots} \\\\\n\\smash\\ddots &  0     & p &        & \\\\\n       & q      & 0 & p      & \\\\\n       &        & q & 0      & p \\\\\n       &        &   & q & 0 & \\smash\\ddots \\\\\n       &        &   &  & \\smash\\ddots &  \\smash\\ddots       \\end{pmatrix} \\] This has 0s down the diagonal (representing the probability 0 of staying still), \\(p\\) one place to the right of the diagonal (representing the probability of moving up 1), and \\(q\\) one place to the left of the diagonal (representing the probability of moving down 1). Blank spaces in this matrix denotes 0s.\nThe \\(x\\)th row of a transition matrix represents the probabilities of moving from \\(x\\) to each of the other states. Thus each row must consist of non-negative numbers that add up to 1, as is the case in both of our examples.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Markov chains in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L17-markov-intro.html#simulation-of-markov-chains",
    "href": "lectures/L17-markov-intro.html#simulation-of-markov-chains",
    "title": "17  Markov chains in discrete space",
    "section": "17.3 Simulation of Markov chains",
    "text": "17.3 Simulation of Markov chains\nWe can take \\(n\\) samples from a finite-state Markov chain in R with the following function. In the function rmarkov(), n is the number of samples (or steps) to take, trans is the transition matrix \\(\\mathsf P\\), and initial is the initial state \\(X_1\\).\n\nrmarkov &lt;- function(n, trans, initial) {\n  states &lt;- nrow(trans)\n  MC &lt;- rep(0, n)\n  \n  MC[1] &lt;- initial\n  for (i in 1:(n - 1)) MC[i + 1] &lt;- sample(states, 1, prob = trans[MC[i], ])\n  \n  return(MC)\n}\n\nThe key line is the for loop in the penultimate line. Here, the next state \\(X_{i+1}\\) is chosen by sampling a state with probabilities according to the \\(x\\)th row of the transition matrix, where \\(X_i = x\\) is the current state.\n\nExample 17.3 Let’s simulate the two-state broken printer Markov chain from Example 17.1.\n\ntrans &lt;- matrix(c(0.9, 0.1, 0.5, 0.5), 2, 2, byrow = TRUE)\ninitial &lt;- 1\nMC &lt;- rmarkov(80, trans, initial)\nplot(MC, col = \"blue\", lwd = 2, type = \"b\")\n\n\n\n\n\n\n\n\nIn the first line, we entered a \\(2 \\times 2\\) matrix using the code matrix(P, 2, 2), where P was a vector of length \\(2 \\times 2 = 4\\). R default is to fill up the matrix column at a time; I personally find it more logical (at least when working with Markov chains) to fill up a matrix row at a time, so I used byrow = TRUE to ensure that.\nOur sample shows that printer spent most of the time working (state 1), but when it did break (state 2) it usually got mended pretty quickly.\n\n\nExample 17.4 We can simulate the simple random walk from Example 17.4 with the following code.\n\nrrw &lt;- function(n, up) {\n  RW &lt;- rep(0, n)\n  down &lt;- 1 - up\n  \n  RW[1] &lt;- 0\n  for (i in 1:(n - 1)) {\n    RW[i + 1] &lt;- RW[i] + sample(c(1, -1), 1, prob = c(up, down))\n  }\n  \n  return(RW)\n}\n\nSo with \\(p = 0.6\\), we have\n\nRW &lt;- rrw(100, 0.6)\nplot(RW, col = \"blue\", type = \"l\")\n\n\n\n\n\n\n\n\nwhich goes up on average. With \\(p = 0.3\\), we have\n\nRW &lt;- rrw(100, 0.3)\nplot(RW, col = \"blue\", type = \"l\")\n\n\n\n\n\n\n\n\nwhich goes down on average. The simple symmetric random walk, with \\(p = 0.5\\) can be much more unpredictable.\n\nRW &lt;- rrw(1000, 0.5)\nplot(RW, col = \"black\", type = \"l\", ylim = c(-80, 80))\n\ncols &lt;- c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"darkblue\", \"purple\")\nfor (i in 1:7) {\n  RW &lt;- rrw(1000, 0.5)\n  points(RW, col = cols[i], type = \"l\")\n}\n\n\n\n\n\n\n\n\n\nNext time. We continue our whistle-stop tour of discrete-space Markov chains.\n\nSummary:\n\nA Markov chain is a stochastic process where the next step \\(X_{i+1}\\) depends on the current step \\(X_i\\), but, given current step \\(X_{i}\\), does not depend on the past \\(X_1, \\dots, X_{i-1}\\).\nA Markov chain is governed by its transition probabilities \\(p(x, y) = \\mathbb P(X_{i+1} = y \\mid X_i = x)\\). These are written in the transition matrix \\(\\mathsf P\\), whose rows add up to 1.\nThe simple random walk on the integers at each step goes up 1 with probability \\(p\\) and down 1 with probability \\(q\\). If \\(p = q = \\tfrac12\\), it is a simple symmetric random walk.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 2.3.1; my notes for MATH2750 Introduction to Markov Processes.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Markov chains in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L18-markov-longrun.html",
    "href": "lectures/L18-markov-longrun.html",
    "title": "18  Markov chains in the long run",
    "section": "",
    "text": "18.1 n-step transition probabilities\nLast time we saw that the probability of a “1-step transition” from \\(x\\) to \\(y\\) is \\[ p(x, y) = \\mathbb P(X_{i+1} = y \\mid X_{i} = x)  .\\] But what is the probability of a “2-step transition” \\[ p^{(2)}(x, y) = \\mathbb P(X_{i+2} = y \\mid X_{i} = x) ?\\]\nWell, the first step will be from \\(x\\) to some other state \\(z\\); then the second step will have to go from that \\(z\\) to \\(y\\). Thus we have \\[ \\begin{align}\np^{(2)}(x, y) &= \\mathbb P(X_{i+2} = y \\mid X_{i} = x) \\\\\n&= \\sum_{z \\in \\mathcal S} \\mathbb P(X_{i+1} = z \\mid X_{i} = x) \\,\\mathbb P(X_{i+2} = y \\mid X_{i+1} = z , X_{i} = x) \\\\\n&= \\sum_{z \\in \\mathcal S} \\mathbb P(X_{i+1} = z \\mid X_{i} = x) \\,\\mathbb P(X_{i+2} = y \\mid X_{i+1} = z) \\\\\n&= \\sum_{z \\in \\mathcal S} p(x, z)\\,p(z, y) .\n\\end{align} \\] Here, in the third line we used the Markov property to delete the unnecessary conditioning on \\(X_i\\).\nWhat we have here, though, is the \\((x, y)\\)th entry of the matrix square \\(\\mathsf P^2 = \\mathsf{P}\\,\\mathsf{P}\\). That is, to get the matrix of 2-step transitions, we simply take the second matrix power of the matrix of 1-step transitions.\nIn the same way we can calculate an \\(n\\)-step transition probability \\(p^{(n)}(x, y) = \\mathbb P(X_{i+n} = y \\mid X_i = x)\\) by summing over all the potential paths \\(x \\to z_1 \\to \\cdots \\to z_{n-1} \\to y\\) of length \\(n\\) from \\(x\\) to \\(y\\). This gives \\[ p^{(n)}(x, y) = \\sum_{z_1, \\dots, z_{n-1} \\in \\mathcal S} p(x, z_1)\\,p(z_1, z_2)\\cdots p(z_{n-2},z_{n-1})\\,p(z_{n-1}, y).\\] This is the expression for the \\((x,y)\\)th entry of the \\(n\\)th matrix power \\(\\mathsf{P}^{n} = \\mathsf{P}\\,\\mathsf{P}^{n-1}= \\mathsf{P}^{n-1}\\,\\mathsf{P}\\), so we can find all the \\(n\\)-step transition probabilities from \\(\\mathsf P^n\\).\n(Remember that the matrix power \\(\\mathsf P^n\\) is what we get by multiplying the whole matrix \\(\\mathsf P\\) by itself \\(n\\) times, using the rules for multiplying matrices. It’s not just what we get from taking the \\(n\\)th power of the number in each entry. In R, proper matrix multiplication is P %*% P, while P * P is simply entry-wise multiplication.)\nWe will investigate these phenomena in the next section.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Markov chains in the long run</span>"
    ]
  },
  {
    "objectID": "lectures/L18-markov-longrun.html#n-step-transition-probabilities",
    "href": "lectures/L18-markov-longrun.html#n-step-transition-probabilities",
    "title": "18  Markov chains in the long run",
    "section": "",
    "text": "Example 18.1 Let’s go back to our two-state “unreliable printer” Markov chain. Here, we had \\[ \\mathsf P = \\begin{pmatrix} 0.9 & 0.1 \\\\ 0.5 & 0.5 \\end{pmatrix} . \\] The 2-step transition probabilities are given by \\[ \\mathsf P^2 = \\begin{pmatrix} 0.9 & 0.1 \\\\ 0.5 & 0.5 \\end{pmatrix}\\begin{pmatrix} 0.9 & 0.1 \\\\ 0.5 & 0.5 \\end{pmatrix} = \\begin{pmatrix} 0.86 & 0.14 \\\\ 0.7 & 0.3 \\end{pmatrix} . \\] So if the printer is working today, there’s an 86% probability it’s working in two days’ time, for example.\nFor bigger matrix powers, it’s best to use a computer. In R, the following “quick and dirty” function works well for small powers. (For larger powers, I recommend finding a package with an appropriate built-in matrix power function.)\n\nmatrixpow &lt;- function(M, n) {\n  if (n == 1) return(M)\n  else return(M %*% matrixpow(M, n - 1))\n}\n\nFrom this we find the 10-step transition probability \\[ \\mathsf P^{10} = \\begin{pmatrix} 0.8334 & 0.1667 \\\\ 0.8332 & 0.1668 \\end{pmatrix}. \\] The first row of this matrix denotes the probabilities of where we end up after 10 steps if we start in state 1, and the second row for if we start in state 2. These are very nearly the same – we have a probability \\(\\approx 0.883\\) if being in state 1 and \\(\\approx 0.167\\) of being in state 2, regardless of which state we started in. It’s as if the Markov chain has forgotten where we started.\nSimilarly, if we look at the 11-step transition probability, that comes out as \\[ \\mathsf P^{11} = \\begin{pmatrix} 0.8333 & 0.1667 \\\\ 0.8333 & 0.1667 \\end{pmatrix}, \\] which is virtually the same distribution as after 10 steps. It seems that, after a large number of steps \\(i\\), that we are “settling down” to a “long run distribution” where \\(\\mathbb P(X_i = 1) = 0.8333\\) and \\(\\mathbb P(X_i = 2) = 0.1667\\), not only regardless of which state we started in but also for all large \\(i\\).",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Markov chains in the long run</span>"
    ]
  },
  {
    "objectID": "lectures/L18-markov-longrun.html#stationary-distributions",
    "href": "lectures/L18-markov-longrun.html#stationary-distributions",
    "title": "18  Markov chains in the long run",
    "section": "18.2 Stationary distributions",
    "text": "18.2 Stationary distributions\nSuppose our Markov chain is currently in the distribution \\(\\pi\\) at step \\(i\\). That is, for each \\(x \\in \\mathcal S\\), we have \\(\\mathbb P(X_i = x) = \\pi(x)\\). What is the probability we are in state \\(y\\) at the next step \\(i+1\\)? Well, conditioning on the current step, we have \\[ \\mathbb P(X_{i+1} = y) = \\sum_{x \\in \\mathcal S} \\mathbb P(X_i = x) \\,\\mathbb P(X_{i+1} = y \\mid X_i = x) = \\sum_{x \\in \\mathcal S} \\pi(x) \\,p(x, y) . \\]\nNow if, \\(X_{i+1}\\) also has the distribution \\(\\pi\\) – that is, if \\(\\mathbb P(X_{i+1} = y) = \\pi(y)\\) – then we would remain in the distribution \\(\\pi\\) a time step \\(i+1\\). And, by the same logic, time steps \\(i+2, i+3, \\dots\\) and forever. That seems a bit like what we saw happening in Example 18.1. We call this a stationary distribution.\nA stationary distribution means that the probability of being in state \\(x\\) is staying the same, as \\(\\pi(x)\\). Any particular realisation of the Markov chain will, of course, continue moving between states.\n\nDefinition 18.1 Let \\((X_i)\\) be a Markov chain on a discrete state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P = (p(x,y))\\). Let \\(\\pi\\) be a distribution on \\(\\mathcal S\\), in that \\(\\pi(x) \\geq 0\\) for all \\(x\\) and \\(\\sum_{x \\in \\mathcal S} \\pi(x) = 1\\). If, for all \\(y \\in \\mathcal S\\) we have \\[ \\pi(y) = \\sum_{x \\in \\mathcal S} \\pi(x) \\,p(x, y) ,  \\tag{18.1}\\] then we say that \\(\\pi\\) is a stationary distribution.\n\nIn matrix form, we can write Equation 18.1 as \\(\\boldsymbol\\pi = \\boldsymbol\\pi\\mathsf P\\), where \\(\\boldsymbol\\pi\\) is a row vector (not the more common column vector holding the values of \\(\\pi(x)\\).\nSolving Equation 18.1 or \\(\\boldsymbol\\pi = \\boldsymbol\\pi\\mathsf P\\) can be a bit fiddly. It’s often easier to check something called the detailed balance equations.\n\nTheorem 18.1 Let \\((X_i)\\) be a Markov chain on a discrete state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P = (p(x,y))\\). Let \\(\\pi\\) be a distribution on \\(\\mathcal S\\) that solved the detailed balance equations \\[ \\pi(y) \\,p(y, x) = \\pi(x)\\,p(x,y) \\qquad \\text{for all $x, y \\in \\mathcal S$.} \\] Then \\(\\pi\\) is a stationary distribution.\n\n\nProof. Sum both sides over \\(x\\). The left-hand side becomes \\[ \\sum_{x \\in \\mathcal S} \\pi(y)\\,p(y,x) = \\pi(y) \\sum_{x \\in \\mathcal S} p(y,x) = \\pi(y) , \\] since rows of a transition matrix sum up to 1. The right hand side becomes \\[ \\sum_{x \\in \\mathcal S}\\pi(x)\\,p(x,y) .\\] Hence, we have \\[ \\pi(y) = \\sum_{x \\in \\mathcal S} \\pi(x) \\,p(x, y) , \\] which is the definition of a stationary distribution.\n\n\nExample 18.2 We return to Example 17.1 and Example 18.1. There’s no need to check the detailed balance equations when \\(x = y\\), so we just need \\[ \\pi(2)\\,p(2, 1) = \\pi(1)\\,p(1, 2) \\qquad \\Longrightarrow \\qquad 0.5\\pi(2) = 0.1\\pi(1) .\\] Remembering that \\(\\pi\\) must sum to 1, we get \\(\\pi(1) = \\tfrac16 = 0.1667\\) and \\(\\pi(2) = \\tfrac56 = 0.8333\\).\nLook how that compares with our results for \\(\\mathsf P^{10}\\) and \\(\\mathsf P^{11}\\) – this \\(\\pi\\) was precisely the values we saw in every row of \\(\\mathsf P^n\\) for large \\(n\\).",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Markov chains in the long run</span>"
    ]
  },
  {
    "objectID": "lectures/L18-markov-longrun.html#limit-theorems",
    "href": "lectures/L18-markov-longrun.html#limit-theorems",
    "title": "18  Markov chains in the long run",
    "section": "18.3 Limit theorems",
    "text": "18.3 Limit theorems\nThe big central theorem of Markov chains in discrete space is the following. We will highlight some technical conditions in red that we will return to later.\n\nTheorem 18.2 Let \\((X_i)\\) be a Markov chain on a discrete state space \\(\\mathcal S\\) with transition matrix \\(\\mathsf P\\). Suppose that \\((X_i)\\) is irreducible and positive recurrent.\n\nThere exists a stationary distribution \\(\\pi\\), which is unique.\n(Limit theorem) If \\((X_i)\\) is also aperiodic, then \\(\\mathbb P(X_n = y \\mid X_1 = x) \\to \\pi(y)\\) as \\(n \\to \\infty\\) for all \\(y \\in \\mathcal S\\) , regardless of the starting state \\(X_1 = x\\), where \\(\\pi\\) is the unique stationary distribution.\n(Ergodic theorem, 1) Write \\[V_n(x) = \\frac{1}{n} \\big|\\{i = 1, 2, \\dots, n : X_i = x\\}\\big|\\] for the proportion of the first \\(n\\) steps spent in state \\(x\\). Then \\(V_n(x) \\to \\pi(x)\\) as \\(n \\to \\infty\\) for all \\(x \\in \\mathcal S\\), regardless of the starting state \\(X_1\\), where \\(\\pi\\) is the unique stationary distribution.\n(Ergodic theorem, 2) Let \\(\\phi\\) be a function on the state space \\(\\mathcal S\\). Let \\(X\\) have probability mass function \\(\\pi\\), where \\(\\pi\\) is the unique stationary distribution. Then \\[ \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\to \\operatorname{\\mathbb E}\\phi(X), \\] as \\(n \\to \\infty\\), regardless of the starting state \\(X_1\\).\n\n\n(“Ergodic” is a word mathematicians use when talking about long-run average behaviour.)\nThe precise mathematical statements of this theorem are not important for this module. However, it is important to have a rough idea what the statements mean – especially part 4, which is central to the idea of Markov chain Monte Carlo we will discuss over the next lectures.\nThe first part tells us that (provided the technical conditions are fulfilled) we always have a stationary distribution and there’s always exactly one of them. This allows us to use phrases like “where \\(\\pi\\) is the unique stationary distribution” in the other parts of the theorem.\nThe second part tells us that any \\(n\\)-step transition probability \\(p^{(n)}(x,y)\\) tends to \\(\\pi(y)\\), no matter what the value of \\(x\\). In terms of the \\(n\\)-step transition matrix \\(\\mathsf P^n\\), this means that every row of \\(\\mathsf P^n\\) should end up looking like an identical copy of the row vector \\(\\boldsymbol\\pi\\). That is exactly what we found in Example 18.1.\nThe third part tells us that, in the long run, \\(\\pi\\) describes the proportion of time we spend in each state. In the “unreliable printer” example of Example 18.2, this means that the printer spends 83% of the time working and 17% of the time broken in the long run, regardless of whether it was working or broken on day 1.\nThe fourth part is by far the most important result for us, as it relates Markov chains back to the idea of Monte Carlo estimation. Let’s look at the equation in the fourth part, \\[ \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\to \\operatorname{\\mathbb E}\\phi(X). \\] If the \\(X_i\\) were independent and identically distributed, this would just be the ordinary law of large numbers, which tells us that the Monte Carlo estimator (the left-hand side) is an accurate estimator of \\(\\operatorname{\\mathbb E}\\phi(X)\\), when the number of samples is large. This result tells us that we still get a good estimator when the \\(X_i\\) are not IID, but rather come from a Markov chain whose stationary distribution is the PMF of \\(X\\).\nThis fourth part is what allows us to do Markov chain Monte Carlo (MCMC): Monte Carlo estimation when the \\(X_i\\) are the outputs from a Markov chain. If we want to estimate \\(\\operatorname{\\mathbb E}\\phi(X)\\), we just need to find a Markov chain who stationary distribution is the PMF of \\(X\\), and then form the Monte Carlo estimate in the usual way. In the next lecture, the Metropolis–Hastings algorithm will show us a way to do find a Markov chain with a given stationary distribution.\nA quick word before we end about the technical conditions in Theorem 18.2. The precise definitions are not important here, but let us say the following:\n\nIrreducible roughly means that a Markov chain is “connected up”, and isn’t just two Markov separate Markov chains (for example). Specifically, it must be at least possible to get from any state \\(x\\) to any other state \\(y\\) – maybe not in a single step, but in some finite number of steps, with probability greater than 0.\nAperiodic is another technical condition, but if a Markov chain has a non-zero probability of staying in the same state, then this is fulfilled. The Markov chains we look at for MCMC will all have a strictly postive probability of staying put.\nPositive recurrence is a highly technical condition we won’t get into here.\n\nIf you do want to read more about the theory of Markov chains, and these technical conditions in particular, I recommend my lecture notes for my notes for MATH2750 Introduction to Markov Processes. This is entirely optional, though, and this knowledge is not required for this module and its exam.\nNext time. We we look at Markov chain Monte Carlo; specifically, how to set up a Markov chain that has a given probability mass function as its stationary distribution.\n\nSummary:\n\nThe \\(n\\)-step transition probabilities \\(p^{(n)}(x,y) = \\mathbb{P}(X_{i+n} = y \\mid X_i = x)\\) can be found from the \\(n\\)th matrix power \\(\\mathsf P^n\\).\nA stationary distribution \\(\\pi\\) for a Markov chain satisfies the detailed balance equations \\(\\pi(y)\\,p(y,x) = \\pi(x)\\,p(x,y)\\).\nThe ergodic theorem says that (under certain technical conditions) \\(\\frac{1}{n}\\sum_{i=1}^n \\phi(X_i)\\), where \\(X_i\\) is Markov chain, tends to \\(\\operatorname{\\mathbb E}\\phi(X)\\), where the PMF of \\(X\\) is the unique stationary distribution of the Markov chain.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 2.3.1 and 4.1.2; my notes for MATH2750 Introduction to Markov Processes.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Markov chains in the long run</span>"
    ]
  },
  {
    "objectID": "lectures/L19-mh-1.html",
    "href": "lectures/L19-mh-1.html",
    "title": "19  Metropolis–Hastings in discrete space",
    "section": "",
    "text": "19.1 The Metropolis–Hastings algorithm\nLast time, we looked at the long-run behaviour of a Markov chain \\((X_i)\\). We saw that (under certain technical conditions) the Markov chain has a unique stationary distribution \\(\\pi\\), which we can find by solving the detailed balance equations \\(\\pi(y)\\,p(y,x) = \\pi(x)\\,p(x,y)\\).\nWe then saw the ergodic theorem: If \\(\\phi\\) be a function on the state space \\(\\mathcal S\\) and \\(X\\) has probability mass function \\(\\pi\\), then \\[ \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\to \\operatorname{\\mathbb E}\\phi(X), \\] as \\(n \\to \\infty\\). This means we can do Monte Carlo estimation where the samples \\(X_1, \\dots, X_n\\) are not IID, but are rather the output to a Markov chain with stationary distribution \\(\\pi\\).\nSo, suppose we want to estimate \\(\\operatorname{\\mathbb E} \\phi(X)\\), where \\(X\\) has PDF \\(\\pi\\). Then all we need to find a Markov chain that has \\(\\pi\\) as its stationary distribution. We could try to do that be being clever – just by thinking hard and trying to come up with one. However, that’s rather difficult. Instead, the Metropolis–Hastings algorithm is a method to create such a Markov chain.\nThe Metropolis–Hastings algorithm is based on a similar idea to rejection sampling. From state \\(X_i = x\\), we propose moving to some other state \\(y\\), and then we accept the proposal with some acceptance probability. If we accept the proposal, we move to \\(X_{i+1} = y\\); if we reject the proposal, we stay where we are \\(X_{i+1} = x\\).\nNicholas Metropolis first came up with this idea when he worked with Ulam and von Neumann (of Monte Carlo fame) at the Los Alamos National Laboratories. His original idea was generalised by the Canadian statistician WK Hastings.\nThe Metropolis–Hastings algorithm works like this:\nSo a generic Metropolis–Hastings algorithm on a finite state space in “sort-of-R-code” would look something like this:\n# INPUTS:\n# trans:   proposal transition matrix\n# target:  target stationary distribution\n# initial: initial state\n# n:       number of samples\n\nstates &lt;- nrow(trans)\nMC &lt;- rep(0, n)\naccept &lt;- function(x, y) {\n  ratio &lt;- (target[y] * trans[y, x]) / (target[x] * trans[x, y])\n  min(ratio, 1)\n}\n\nMC[1] &lt;- initial\nfor (i in 1:(n - 1)) {\n  prop &lt;- sample(1:states, 1, prob = trans[MC[i], ])\n  if (runif(1) &lt;= accept(MC[i], prop)) MC[i + 1] &lt;- prop\n  else                                 MC[i + 1] &lt;- MC[i]\n}\nIt’s the last three lines that are important here. In this code MC records the states of our Markov chain. First we propose a move to state prop, according to the row of the transition matrix corresponding to the current state. Second, we accept that proposal move with probability accept(), where the arguments of accept() are the current state and the proposed state; we do this by checking whether a standard uniform runif(1) is less than this acceptance probability or not. If it is, we move to prop (last-but-one line); and if not, we stay where we are (last line).\nWe will show later that this algorithm really does have \\(\\pi\\) as its stationary distribution.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Metropolis--Hastings in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L19-mh-1.html#the-metropolishastings-algorithm",
    "href": "lectures/L19-mh-1.html#the-metropolishastings-algorithm",
    "title": "19  Metropolis–Hastings in discrete space",
    "section": "",
    "text": "We want to define a Markov chain on a state space \\(\\mathcal S\\), which contains the range of the probability mass function \\(\\pi\\) we want to sample from.\nWe have an initial state \\(X_1 = x_1\\) and we choose a transition matrix \\(\\mathsf R = (r(x,y))\\) representing the proposal moves.\n\n\nFrom a current state \\(X_i = x\\) we propose moving to a new state \\(y\\), where \\(y\\) is chosen with probability \\(r(x, y)\\).\nWith probability \\[\\alpha(x,y) = \\min \\left\\{ \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)} , \\, 1\\right\\} , \\] we accept the proposal, and set \\(X_i = y\\); otherwise we stay put, and set \\(X_{i+1} = x\\).\nWe repeat steps 1. and 2. \\(n\\) times to get \\(n\\) samples.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Metropolis--Hastings in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L19-mh-1.html#random-walk-metropolis",
    "href": "lectures/L19-mh-1.html#random-walk-metropolis",
    "title": "19  Metropolis–Hastings in discrete space",
    "section": "19.2 Random walk Metropolis",
    "text": "19.2 Random walk Metropolis\nThere are quite a lot of cases where the proposals are symmetric, meaning that \\(r(x, y) = r(y, x)\\). This is the case if, for example, the proposal probabilities are those of the simple symmetric random walk: \\(r(x, x+1) = \\frac12\\) and \\(r(x, x-1) = \\frac12\\). In the symmetric case, the acceptance probability simplifies to \\[ \\alpha(x, y) = \\min \\left\\{ \\frac{\\pi(y)}{\\pi(x)} , \\, 1\\right\\} . \\]\nThe symmetric case was the version originally considered by Metropolis, before Hastings generalised it to non-symmetric proposals. For this reasons, when \\(\\mathsf R\\) has this symmetry property, we often refer to the resulting algorithm just as the Metropolis algorithm. When the proposal probabilities are those of the simple symmetric random walk, we call it the random walk Metropolis algorithm.\n\nExample 19.1 Let’s do an example of the random walk Metropolis algorithm where we aim to sample from the geometric distribution with parameter \\(\\frac13\\), \\[ \\pi(x) = \\Big(\\frac23\\Big)^{x-1}\\times \\frac13 \\qquad x = 1, 2, \\dots. \\] We will start from \\(X_1 = 1\\).\nSo at each step we propose moving up one with probability \\(\\tfrac12\\) and moving down one with probability \\(\\tfrac12\\). Since the proposals are symmetric, the acceptance probabilities are \\[ \\begin{align}\n\\alpha(x, x+1) = \\min \\left\\{ \\frac{\\pi(x+1)}{\\pi(x)} , \\, 1\\right\\} = \\min \\left\\{\\frac{\\big(\\frac23\\big)^{x}\\times \\frac13}{\\big(\\frac23\\big)^{x-1}\\times \\frac13},\\, 1\\right\\} = \\min \\Big\\{\\frac23, 1\\Big\\} = \\frac23 \\\\\n\\alpha(x, x-1) = \\min \\left\\{ \\frac{\\pi(x-1)}{\\pi(x)} , \\, 1\\right\\} = \\min \\left\\{\\frac{\\big(\\frac23\\big)^{x-1}\\times \\frac13}{\\big(\\frac23\\big)^{x}\\times \\frac13},\\, 1\\right\\} = \\min \\Big\\{\\frac32, 1\\Big\\} = 1 ,\n\\end{align} \\] except for \\[\\alpha(1, 0) = \\min \\left\\{ \\frac{\\pi(0)}{\\pi(1)} , \\, 1\\right\\} = \\min \\left\\{ \\frac{0}{\\frac13} , \\, 1\\right\\} = \\min \\{0,1\\} = 0 .\\] So if the proposal is up one, we accept it with probability \\(\\frac23\\) and otherwise stay where we are. If the proposal is down one, we always accept – except going down from 1 to 0, which we always reject.\nLet’s try it.\n\nn &lt;- 1e6\nMC &lt;- rep(0, n)\n\nMC[1] &lt;- 1\nfor (i in 1:(n - 1)) {\n  prop &lt;- MC[i] + sample(c(+1, -1), 1, prob = c(1/2, 1/2))\n  if      (prop == 0)         MC[i + 1] &lt;- MC[i]\n  else if (prop == MC[i] - 1) MC[i + 1] &lt;- MC[i] - 1\n  else if (prop == MC[i] + 1) MC[i + 1] &lt;- MC[i] + (runif(1) &lt;= 2/3)\n}\n\nIf we look at a graph of the first 250 steps of this Markov chain, we see that these aren’t at all random samples from the geometric distribution – each step is either the same as the one before, one bigger, or one smaller.\n\n\nCode for drawing this graph\nplot(\n  MC[1:250],\n  type = \"l\", col = \"red\", lwd = 2,\n  xlab = \"time step\", ylab = \"value\"\n)\n\n\n\n\n\n\n\n\n\nBut if we look at the overall graph of which samples came up overall, we see that their proportions (red) are extremely close to what we would expect from the true geometric distribution (blue).\n\n\nCode for drawing this graph\nplot(\n  table(MC)/n,\n  xlim = c(0, 10), col = \"red\", ylim = c(0, 0.35),\n  xlab = \"value\", ylab = \"probability\"\n)\npoints(1:10 + 0.05, (2/3)^(1:10 - 1) * (1/3), col = \"blue\", type = \"h\", lwd = 2)\n\n\n\n\n\n\n\n\n\nSuppose we wanted to estimate \\(\\mathbb EX^2\\), where \\(X \\sim \\operatorname{Geom}(\\frac{1}{3})\\). We already know how to do this the “basic” Monte Carlo way. But we can now do it the Markov chain Monte Carlo (MCMC) way, by using the output to this Markov chain.\nOur estimate is the following.\n\nmean(MC^2) \n\n[1] 15.03181\n\n\nThe true answer is 15, so we are in the right area, but probably not as accurate as the basic Monte Carlo estimator with the same sample size would have been. This suggests that the dependence structure in a Markov chain might be a slight disadvantage, and may make the variance of our estimator bigger. The real strength of MCMC is when a basic Monte Carlo estimate is impossible to get – when basic MCMC is possible (such as for simple distributions like this geometric) we should probably stick with it.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Metropolis--Hastings in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L19-mh-1.html#proof-of-stationary-distribution",
    "href": "lectures/L19-mh-1.html#proof-of-stationary-distribution",
    "title": "19  Metropolis–Hastings in discrete space",
    "section": "19.3 Proof of stationary distribution",
    "text": "19.3 Proof of stationary distribution\nWe have defined the Metropolis–Hastings Markov chain in terms of the proposal transition probabilities \\(r(x, y)\\) and the acceptance probability \\(\\alpha(x, y)\\). But what are the actual transition probability \\(p(x, y)\\) of this Markov chain?\nWell, to move from \\(x\\) to \\(y \\neq x\\), we first have to propose that move, then we have to accept it. So we have \\[ p(x, y) = r(x, y) \\,\\alpha(x, y) = r(x, y) \\,\\min \\left\\{ \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)} , \\, 1\\right\\} . \\tag{19.1}\\] (We can find \\(p(x,x)\\), if we need it, by using the fact that \\(\\sum_y p(x,y) = 0\\).)\nWe should check that the Metropolis–Hastings algorithm really does give a Markov chain with stationary distribution \\(\\pi\\).\n\nTheorem 19.1 Let \\(\\pi\\) be a probability mass function on a discrete state space \\(\\mathcal S\\), and let \\(\\mathsf R = (r(x,y))\\) be a transition matrix on \\(\\mathcal S\\). Let \\((X_i)\\) be the Metropolis–Hastings Markov chain with proposal transition matrix \\(\\mathsf R\\) and acceptance probability \\[ \\alpha(x, y) = \\min \\left\\{ \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)},\\,1\\right\\} . \\] Then \\(\\pi\\) is a stationary distribution for \\((X_i)\\).\n\nWe say “a” stationary distribution. But provided the Markov chain fulfils the technical conditions in Theorem 18.2, we know that this will be the unique stationary distribution, and that the ergodic theorem will hold.\n\nProof. We need to check the detailed balance equations \\[ \\pi(y) \\,p(y, x) = \\pi(x) \\,p(x, y) \\] for \\(y \\neq x\\). By Equation 19.1, the detailed balance equations are \\[ \\pi(y)\\,r(y, x) \\,\\min \\left\\{ \\frac{\\pi(x)\\,r(x,y)}{\\pi(y)\\,r(y,x)} , \\, 1\\right\\} = \\pi(x)\\,r(x, y) \\,\\min \\left\\{ \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)} , \\, 1\\right\\} .  \\tag{19.2}\\] Note that the two fractions in the first terms on the minimums are reciprocals of each other. So one of these will be greater than equal to 1, and the minimum will be 1; and one of the will be less than or equal to 1, and the minimum will be that fraction.\nSuppose first that \\[\\frac{\\pi(x)\\,r(x,y)}{\\pi(y)\\,r(y,x)} \\geq 1 \\qquad\\text{and} \\qquad \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)} \\leq 1 .\\] Then Equation 19.2 becomes \\[ \\pi(y)\\,r(y, x) \\times 1 = \\pi(x)\\,r(x, y) \\times \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)}  . \\] On the right-hand side, the two \\(\\pi(x)\\,r(x,y)\\) terms cancel, so we have equality, and the detailed balance equations hold.\nIf, on the other hand \\[\\frac{\\pi(x)\\,r(x,y)}{\\pi(y)\\,r(y,x)} \\leq 1 \\qquad\\text{and} \\qquad \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)} \\geq 1 ,\\] then the same argument works the other way around.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Metropolis--Hastings in discrete space</span>"
    ]
  },
  {
    "objectID": "lectures/L20-markov-cont.html",
    "href": "lectures/L20-markov-cont.html",
    "title": "20  Markov chains in continuous space",
    "section": "",
    "text": "20.1 Markov chains with densities\nIn the last three lectures, we have looked at Markov chains and the Metropolis–Hastings algorithm on a discrete state space \\(\\mathcal S\\). This allowed us to form Markov chain Monte Carlo (MCMC) estimators for discrete random variables. But very often we want to sample from continuous random variables. So in the next two lectures we will look at Markov chains and Metropolis–Hastings in continuous space. (We are still in discrete time, though.)\nThe general theory of Markov chains on continuous state spaces can get very complicated – there are lots of technical conditions, and you have to deal with a “measure theoretic” approach that looks at least as much like analysis from pure mathematics as it does probability and statistics. However, this technical material is not necessary to me able to understand and use the basic ideas of MCMC in continuous space. For this reason, we will often make broad simplifying assumptions, not get into the precise definitions of technical terms, and occasionally just outright lie. (If you want to get into more of the theoretical details, I recommend Subsection 2.3.2 and Sections 4.1 and 4.2 of Voss, An Introduction to Statistical Computing as a good place to begin your studies.)\nThe concept of a Markov chain and the Markov property remain the same: the next step \\(X_{i+1}\\) may depende on the current step \\(X_i\\), but, given the current step \\(X_i\\), may not depend any further on on the past steps \\(X_{i-1}, X_{i-2}, \\dots, X_2, X_1\\).\nTherefore, the steps of such a Markov chain will depend only on the initial state and the transition rule from moving from \\(X_i = x\\) to \\(X_{i+1} = y\\). We will make the simplification that this transition rule always has a density; that is, that there exists a conditional probability density function \\(p\\) where \\(p(x, y)\\) is the probability density of \\(X_{i+1}\\) around \\(y\\) given \\(X_i = x\\). Formally, we have \\[ \\mathbb P(X_{i+1} \\in A \\mid X_i = x) = \\int_A p(x, y)\\,\\mathrm{d} y . \\] This transition density \\(p = p(x, y)\\) behaves a lot like the transition probabilities \\(\\mathsf P = (p(x,y))\\) in discrete space.\nIn the discrete case, we had \\[\\sum_{y \\in \\mathcal S} p(x, y) = 1 \\qquad \\text{for all } x\\in \\mathcal S \\] (“rows of the transition matrix sum to 1”). Similarly, in the discrete case we have \\[ \\int_{\\mathcal S} p(x, y) \\,\\mathrm{d}y = 1 \\qquad \\text{for all } x\\in \\mathcal S ,\\] by the same argument that we have to go somewhere from \\(x\\).",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Markov chains in continuous space</span>"
    ]
  },
  {
    "objectID": "lectures/L20-markov-cont.html#gaussian-random-walk",
    "href": "lectures/L20-markov-cont.html#gaussian-random-walk",
    "title": "20  Markov chains in continuous space",
    "section": "20.2 Gaussian random walk",
    "text": "20.2 Gaussian random walk\nThe most important Markov chain in continuous space is the following.\n\nExample 20.1 Consider the Gaussian random walk with drift \\(\\mu\\) and volatility \\(\\sigma\\). This is a random walk with transitions given by the rule \\(X_{i+1} = X_i + Z_i\\), where the \\(Z_i \\sim \\operatorname{N}(\\mu, \\sigma^2)\\) are IID. So at each time step, the position of the random walk is shifted by a normally distributed amount. When \\(\\mu = 0\\), we call this the symmetric Gaussian walk, since it moves up and down symmetrically. (The name “Gaussian random walk” is because the “Gaussian distribution” is alternative name for the normal distribution.) This can be used as the model of the price of a stock each day or (in higher dimensions) the position of a gas particle in a room.\nAnother way to write this is that \\(X_{i+1} \\sim \\operatorname{N}(X_i + \\mu, \\sigma^2)\\). So the transition density is \\[ p(x, y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\, \\exp \\Bigg(- \\frac{\\big(y - (x + \\mu)\\big)^2}{2\\sigma^2} \\Bigg) . \\]\nWe can simulate this in R in a similar way to the simple random walk on the integers.\n\nrgrw &lt;- function(n, mean, sd) {\n  GRW &lt;- rep(0, n)\n  GRW[1] &lt;- 0\n  for (i in 1:(n - 1)) GRW[i + 1] &lt;- GRW[i] + rnorm(1, mean, sd)\n  return(GRW)\n}\n\nYou can play around with this function yourself, but generally we see similar behaviour to the simple random walk: for \\(\\mu &gt; 0\\) (as with \\(p &gt; \\frac12\\)) it trends fairly predictably upwards; for \\(\\mu &lt; 0\\) (as with \\(p &lt; \\frac12\\)) it trends fairly predictably downwards; and for \\(\\mu = 0\\) (as with with \\(p = \\frac12\\)) it is more unpredictable.\n\nGRW &lt;- rgrw(1000, 0, 1)\nplot(GRW, col = \"black\", type = \"l\", ylim = c(-70, 70))\n\ncols &lt;- c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"darkblue\", \"purple\")\nfor (i in 1:7) {\n  GRW &lt;- rgrw(1000, 0, 1)\n  points(GRW, col = cols[i], type = \"l\")\n}",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Markov chains in continuous space</span>"
    ]
  },
  {
    "objectID": "lectures/L20-markov-cont.html#long-run-behaviour",
    "href": "lectures/L20-markov-cont.html#long-run-behaviour",
    "title": "20  Markov chains in continuous space",
    "section": "20.3 Long-run behaviour",
    "text": "20.3 Long-run behaviour\nMost of the properties about long-run behaviour of Markov chains in discrete space continue to hold in continuous space: we just replace the transition probabiltiy by the transition density and sums by integrals.\nWe can find the two-step transition density \\(p^{(2)}(x, y)\\) be integrating over all possible intermediate steps \\(z\\) \\[ p^{(2)}(x, y) = \\int_{\\mathcal S} p(x, z)\\,p(z, y)\\,\\mathrm{d} z .\\] Similarly, we get an \\(n\\)-step transition density from \\[ p^{(n)}(x, y) = \\int_{\\mathcal S^{n-1}} p(x, z_1)\\,p(z_1,z_2)\\cdots p(z_{n-2},z_{n-1})\\,p(z_{n-1}, y)\\, \\mathrm{d}z_1 \\, \\mathrm{d}z_2 \\cdots \\mathrm{d}z_{n-1} . \\]\nA stationary density \\(\\pi\\) is a PDF on \\(\\mathcal S\\) (so \\(\\pi(x) \\geq 0\\) and \\(\\int_{\\mathcal S}\\pi(x)\\,\\mathrm{d}x = 0\\)) such that \\[ \\pi(y) = \\int_{\\mathcal S} \\pi(x) \\,p(x,y) \\,\\mathrm{d}x \\qquad \\text{for all } y \\in \\mathcal S . \\] This is often easier to find by solving the detailed balance equations \\[ \\pi(y)\\,p(y, x) = \\pi(x)\\,p(x, y) \\qquad \\text{for all } x,y \\in \\mathcal S . \\]",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Markov chains in continuous space</span>"
    ]
  },
  {
    "objectID": "lectures/L21-mh-2.html",
    "href": "lectures/L21-mh-2.html",
    "title": "21  Metropolis–Hastings in continuous space",
    "section": "",
    "text": "21.1 The Metropolis–Hastings algorithm again\nLast time, we looked at Markov chains \\((X_i)\\) in continuous space, as defined by a transition density \\(p(x, y)\\). We saw (under certain technical conditions we didn’t get into) that we have convergence to a unique stationary probability density \\(\\pi\\). We further saw that we have an ergodic theorem \\[ \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\to \\operatorname{\\mathbb E}\\phi(X) , \\] where \\(X\\) has PDF \\(\\pi\\).\nAs before, this opens up to us the possibility of MCMC: find a Markov chain with stationary distribution equal to the PDF you want to sample from, then use the output of the Markov chain as the samples in a Monte Carlo estimator. And, yet again, the Metropolis–Hastings algorithm gives us a way to find such a Markov chain. The continuous Metropolis–Hastings algorithm is essentially the same as the discrete time one, but with densities instead of probabilities.\n(Note that the acceptance probability \\(\\alpha(x, y)\\) really is a probability, not a density.)\nThis can be proved to have \\(\\pi\\) as a stationary density by checking the detailed balance equations. The proof is identical to the discrete case, so we won’t write it out again.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Metropolis--Hastings in continuous space</span>"
    ]
  },
  {
    "objectID": "lectures/L21-mh-2.html#the-metropolishastings-algorithm-again",
    "href": "lectures/L21-mh-2.html#the-metropolishastings-algorithm-again",
    "title": "21  Metropolis–Hastings in continuous space",
    "section": "",
    "text": "We want to define a Markov chain on a continuous state space \\(\\mathcal S\\), which contains the range of the probability density function \\(\\pi\\) we want to sample from.\nWe have an initial state \\(X_1 = x_1\\) and we choose a transition density \\(r = r(x,y)\\) representing the proposal moves.\n\n\nFrom a current state \\(X_i = x\\) we propose moving to a new state \\(y\\), where \\(y\\) is chosen according to the probability density \\(r(x, y)\\).\nWith probability \\[\\alpha(x,y) = \\min \\left\\{ \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)} , \\, 1\\right\\} , \\] we accept the proposal, and set \\(X_i = y\\); otherwise we stay put, and set \\(X_{i+1} = x\\).\nWe repeat steps 1. and 2. \\(n\\) times to get \\(n\\) samples.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Metropolis--Hastings in continuous space</span>"
    ]
  },
  {
    "objectID": "lectures/L21-mh-2.html#random-walk-metropolis-again",
    "href": "lectures/L21-mh-2.html#random-walk-metropolis-again",
    "title": "21  Metropolis–Hastings in continuous space",
    "section": "21.2 Random walk Metropolis again",
    "text": "21.2 Random walk Metropolis again\nAs with the discrete case, when the transition density is symmetric, in that \\(r(y, x) = r(x, y)\\) for all \\(x, y \\in \\mathcal S\\), then the acceptance probability simplifies to \\[\\alpha(x,y) = \\min \\left\\{ \\frac{\\pi(y)}{\\pi(x)} , \\, 1\\right\\} , \\] and we call it just the Metropolis algorithm.\nWhen the proposal density is that of a Gaussian random walk with drift \\(\\mu = 0\\), we call this Random walk Metropolis.\n\nExample 21.1 Suppose we wish to sample from an exponential distribution \\(X \\sim \\operatorname{Exp}(\\lambda)\\), which has PDF \\(\\pi(x) = \\lambda \\mathrm{e}^{-\\lambda x}\\) for \\(x \\geq 0\\).\nWe can sample from this using the random walk Metropolis algorithm. From \\(X_i = x\\), we propose a move to \\(y = x + \\operatorname{N}(0, \\sigma^2) = \\operatorname{N}(x, \\sigma^2)\\). If \\(y \\geq 0\\), we accept the proposed move with probability \\[\\alpha(x,y) = \\min \\left\\{ \\frac{\\pi(y)}{\\pi(x)} , \\, 1\\right\\} =  \\min \\left\\{ \\frac{\\lambda\\mathrm{e}^{-\\lambda y}}{\\lambda\\mathrm{e}^{-\\lambda x}} , \\, 1\\right\\} = \\min \\big\\{ \\mathrm{e}^{\\lambda(x - y)},\\, 1\\big\\} .\\] So if \\(0 \\leq y \\leq x\\), then we always accept the move with probability 1, while if \\(y &gt; x\\) then we accept with probability \\(\\alpha(x, y) = \\mathrm{e}^{-\\lambda(y-x)}\\). If \\(y &lt; 0\\), then \\(\\pi(y) = 0\\), so \\(\\alpha(x, y) = 0\\), and we always reject.\nThe following R function carries this out.\n\nmetroexp &lt;- function(n, rate, sigma, initial) {\n  MC &lt;- rep(0, n)\n  accept &lt;- function(x, y) exp(-rate * (y - x))\n\n  MC[1] &lt;- initial\n  for (i in 1:(n - 1)) {\n    prop &lt;- MC[i] + rnorm(1, 0, sigma)\n    if (prop &lt; 0)  MC[i + 1] &lt;- MC[i]\n    else if (runif(1) &lt;= accept(MC[i], prop)) MC[i + 1] &lt;- prop\n    else MC[i + 1] &lt;- MC[i]\n  }\n  \n  return(MC)\n}\n\nWe used a cunning trick to slightly simplify the above code. If \\(\\pi(y) / \\pi(x) &gt; 1\\), then the acceptance probability is 1, from the “min” in the definition of \\(\\alpha(x, y)\\). But the implementation to accept if \\(U \\leq \\pi(y) / \\pi(x)\\) still works. This makes the else if line in the above code a bit simpler, as we didn’t have to deal with this case separately.\nLet’s try it out for \\(\\lambda = 0.1\\) and \\(\\sigma = 15\\).\n\nRWM &lt;- metroexp(1e6, 0.1, 15, 0)\n\nhist(RWM, probability = TRUE, xlim = c(0, 50), ylim = c(0, 0.1), breaks = 100)\ncurve(dexp(x, 0.1), add = TRUE, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nThis looks like an excellent match to the \\(\\operatorname{Exp}(0.1)\\) distribution.\nIn the random walk Metropolis algorithm, we had to pick the value for the parameter \\(\\sigma\\). In this algorithm, \\(\\sigma\\) can be interpreted as a “typical step size”, in that the standard deviation of the step proposal \\(y - x\\) is \\(\\sigma\\). The ergodic theorem will still hold for any value of \\(\\sigma\\) in the limit as \\(n \\to \\infty\\), but the practical performance at finite \\(n\\) may be different for different values.\nLet’s have a look at how the Markov chain moved with our step size of \\(\\sigma = 15\\)\n\nplot(RWM[1:1000], lwd = 2, ylim = c(0, 40), col = \"red\", type = \"l\") \n\n\n\n\n\n\n\n\nIt’s clear this isn’t an independent sample, since this is not purely “exponentially distributed noise”. But it seems to exploring the range of different values an \\(\\operatorname{Exp}(0.1)\\) distribution is likely to take very rapidly.\nWhat if we had chosen a much larger step size, like \\(\\sigma = 400\\), or a much smaller one, like \\(\\sigma = 1\\)?\n\nRWMbig &lt;- metroexp(1000, 0.1, 400, 0)\nplot(RWMbig, lwd = 2, ylim = c(0, 40), col = \"blue\", type = \"l\")\n\n\n\n\n\n\n\nRWMsmall &lt;- metroexp(1000, 0.1, 1, 0)\nplot(RWMsmall, lwd = 2, ylim = c(0, 40), col = \"green\", type = \"l\")\n\n\n\n\n\n\n\n\nWe can see that when \\(\\sigma = 400\\) (blue), there are lots of “flat parts” of the graph. This is where the Markov chain did not move, because it was rejecting lots of proposed moves. This would be because large steps would often produce either negative proposals, which are always rejected, or very large proposals, where the acceptance probability is very small. This Markov chain is rarely moving at all, so is not exploring the state space very well.\nWe can also see that when \\(\\sigma = 1\\) (green), the Markov chain was often accepting moves, but only making small steps. Compared to the \\(\\sigma = 15\\) case, this graph is much less “busy”, and looks more like a gentle wander through the state space rather than a rapid exploration. So although this Markov chain is moving through the state space, it is crawling through space quite slowly.\n\nWe saw here a general pattern when choosing the step size in a Gaussian random walk Metropolis algorithm:\n\nIf the step size is too large, then too many proposals are rejected. This means you often stay in the same state for a long time, and you only rarely move to explore a new state.\nIf the step size is too small, then proposals are very close to the current state. This means you often stay in the same approximate area for a long time, and you crawl through the state space very slowly.\n\nYou want to try and pick the “Goldilocks” step size – not too small, and not too big! There is no perfect recipe you can follow to pick the ideal step size – MCMC is an art as well as a science. If you are able to, it can be helpful to think about what typical values you hope to be sampling. In our example, ranges of between 0 and 30 or so are typical for \\(\\operatorname{Exp}(0.1)\\), so you want a step size that will explore such a range well without too regularly stepping outside of it. Our choice of \\(\\sigma = 15\\), being half of that \\([0, 30]\\) range seemed to do quite well. It’s also worth doing short “pilot” runs, where you try different values of \\(\\sigma\\) and examine what seems to work best.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Metropolis--Hastings in continuous space</span>"
    ]
  },
  {
    "objectID": "lectures/L21-mh-2.html#burn-in-period",
    "href": "lectures/L21-mh-2.html#burn-in-period",
    "title": "21  Metropolis–Hastings in continuous space",
    "section": "21.3 Burn-in period",
    "text": "21.3 Burn-in period\nRecall that the goal of MCMC is to sample from the stationary distribution \\(\\pi\\). It’s therefore a good idea, if it’s possible, to start from an initial value that’s “typical” of \\(\\pi\\); that is, has a large value of \\(\\pi\\) and is near other states with large values of \\(\\pi\\). That was definitely true for \\(X_1 = 0\\) in our previous example.\nBut what happens if we don’t pick such an initial state – either because we don’t know what these typical states look like, or by mistake.\n\nExample 21.2 We continue with the previous example, with step size \\(\\sigma = 2\\). What if we had started at \\(X_1 = 200\\) instead – what then would the Markov chain look like.\n\nset.seed(4)\nRWMbig &lt;- metroexp(2000, 0.1, 2, 200)\nplot(RWMbig, lwd = 2, col = \"red\", type = \"l\")\n\n\n\n\n\n\n\n\nYou can see that, at the start, it takes us a while to move away from the “bad” initial state \\(X_1 = 200\\) and to get to the “typical values” of \\([0, 35]\\) or so. In this example, it took around 1000 steps.\n\nThe ergodic theorem tells us this, eventually, for large enough \\(n\\), these early unrepresentative samples will be drowned out of our large-\\(n\\) collection. But for modest finite values of \\(n\\), these are likely to corrupt our Monte Carlo estimation procedure.\nFor that reason, it can often be useful to use a burn-in period. A burn-in period is when you run the Markov chain for a while without using the samples in estimation, and only start using samples once you have “reached the stationary distribution” – the phrase “reached equilibrium” is also sometimes used. Again, how long a burn-in period should be is art more than science – thinking about your specific problem and conducting experiments can help you decide if a burn-in period is necessary and how long it should be. If the state space and stationary distribution are easy to understand – say, \\(\\mathcal S\\) is one-dimensional and \\(\\pi\\) has a single mode – you can even run the MCMC algorithm first, then afterwards decide which unrepresentative early samples to throw away.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Metropolis--Hastings in continuous space</span>"
    ]
  },
  {
    "objectID": "problems/P4.html",
    "href": "problems/P4.html",
    "title": "Problem Sheet 4",
    "section": "",
    "text": "Full solutions are now available.\n\n\nThis is Problem Sheet 4, which covers material from Lectures 17 to 21. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Friday 29 November (one day later than the usual Thursday). If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Mondays at 1500.\nThis problem sheet is to help you practice material from the module. It is not for assessment and will not be marked.\nFull solutions should be released on Monday 2 December.\n\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.     Consider a Markov chain on the discrete state space \\(\\mathcal S = \\{1, 2, \\dots\\}\\) with transition probabilities \\(p(x, x+1) = p\\) for all \\(x\\), \\(p(x, 1) = 1 - p\\) for all \\(x\\), and \\(p(x, y) = 0\\) otherwise.\n\n(a)  Calculate the two-step transition probabilities \\(p^{(2)}(x, y)\\). (You might start by working out what two-step transitions are even possible.)\n\nSolution. The possible paths of length two from \\(x\\) are\n\n\\(x \\to x + 1 \\to x + 2\\): this requires two steps up, so has probability \\(p^2\\).\n\\(x \\to x+1 \\to 1\\): this requires one step up then a reset, so has probability \\(p(1-p)\\).\n\\(x \\to 1 \\to 2\\): this requires a reset then one step up, so has probability \\((1-p)p\\).\n\\(x \\to 1 \\to 1\\): this requires two resets, so has probability \\((1-p)^2\\).\n\nHence the two-step transition probabilities are \\[ \\begin{align}\np^{(2)}(x, x+2) &= p^2 \\\\\np^{(2)}(x, 2) &= (1-p)p \\\\\np^{(2)}(x, 1) &= p(1 - p) + (1-p)^2 = 1 - p\n\\end{align} \\] and \\(p(x, y) = 0\\) otherwise.\n\n\n\n(b)  Find the stationary distribution \\(\\pi\\) for the Markov chain by solving \\(\\pi(y) = \\sum_x \\pi(x) p(x,y)\\).\n\nSolution. For \\(y \\neq 1\\), the only \\(x\\) with \\(p(x, y) \\neq 0\\) is \\(x = y-1\\). So we have \\(\\pi(y) = \\pi(y-1)\\,p(y-1, y) = p\\,\\pi(y-1)\\).\nHence we have \\(\\pi(2) = p\\,\\pi(1)\\), \\(\\pi(3) = p\\,\\pi(2) = p^2 \\pi(1)\\), \\(\\pi(4) = p\\,\\pi(3) = p^3 \\pi(1)\\), and in general \\(\\pi(i) = p^{i-1}\\,\\pi(1)\\). (This equation even holds for \\(i = 1\\) itself.)\nWe also know that \\(\\pi\\) is a distribution, so must sum to 1. Hence \\[ 1 = \\sum_{i=1}^\\infty \\pi(i) = \\sum_{i=1}^\\infty p^{i-1}\\,\\pi(1) = \\pi(1) \\sum_{i=1}^\\infty p^{i-1} = \\pi(1) \\, \\frac{1}{1-p} .  \\] Hence \\(\\pi(1) = 1 - p\\), and \\(\\pi(i) = p^{i-1}(1-p)\\). This is a \\(\\operatorname{Geom}(1-p)\\) distribution.\n\n\n\n\n2.     The health of a chicken each day during a bird flu pandemic is described by a simple “healthy–sick–dead” Markov chain model. The state space is \\(\\mathcal S = \\{\\text{H}, \\text{S}, \\text{D}\\}\\). The transition probabilities are \\[ \\begin{align}\np_{\\mathrm{HH}} &= ? & p_{\\mathrm{HS}} &= 0.02 & p_{\\mathrm{HD}} &= 0.01 \\\\\np_{\\mathrm{SH}} &= 0.3 & p_{\\mathrm{SS}} &= 0.5 & p_{\\mathrm{SD}} &= 0.2 \\\\\np_{\\mathrm{DH}} &= ? & p_{\\mathrm{DS}} &= ? & p_{\\mathrm{DD}} &= 1.\n\\end{align} .\\] Fill in the three gaps (marked \\(?\\)).\nIf a chicken is healthy on day 1, what is the probability it is still alive on (a) day 2, (b) day 3; (c) day 11; (d) day 51? (You should do parts (a) and (b) by hand, but I recommend a computer for parts (c) and (d).)\n\nSolutions. Because row of a transition matrix have to add up to 1, \\(p_{\\mathrm{HH}} = 1 - 0.02 - 0.01 = 0.97\\). Because all entries also have to be non-negative, it must be that \\(p_{\\mathrm{DH}} = p_{\\mathrm{DS}} = 0\\).\n(a) One day later, on day 2, the chicken has died with probability \\(p_{\\mathrm{HD}} = 0.01\\), and is still alive with probability \\(p_{\\mathrm{HH}} + p_{\\mathrm{HS}} = 0.97 + 0.02 = 1 - p_{\\mathrm{HD}} = 0.99\\).\n(b) This requires us to calculate \\(p^{(2)}_{\\mathrm{HH}} + p^{(2)}_{\\mathrm{HS}} = 1 - p^{(2)}_{\\mathrm{HD}}\\).\nWe can either do this by summing over paths of length 2 (like in Question 1(a)) or by doing the matrix multiplication. Let’s do it the matrix multiplication way this time. We have \\[ \\mathsf P^{(2)} = \\mathsf P^2 = \\begin{pmatrix} 0.97 & 0.02 & 0.01 \\\\ 0.3 & 0.5 & 0.2 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.97 & 0.02 & 0.01 \\\\ 0.3 & 0.5 & 0.2 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.9469 & 0.0294 & 0.0237 \\\\ 0.441 & 0.256 & 0.303 \\\\ 0 & 0 & 1\\end{pmatrix}.\\]\nSo the answer is \\(0.9469 + 0.0294 = 1 - 0.0237 = 0.9763\\).\n(c) and (d) I read the transition matrix into R as follows.\n\nP &lt;- matrix(c(0.97, 0.02, 0.01, 0.3, 0.5, 0.2, 0, 0, 1), 3, 3, byrow = TRUE)\n\nI then used the matrix power function from Lecture 18.\n\nmatrixpow &lt;- function(M, n) {\n  if (n == 1) return(M)\n  else return(M %*% matrixpow(M, n - 1))\n}\n\nThe answers are the following.\n\nP11 &lt;- matrixpow(P, 11)\nP11[1, 1] + P11[1, 2]\n\n[1] 0.8354785\n\nP50 &lt;- matrixpow(P, 50)\nP50[1, 1] + P50[1, 2]\n\n[1] 0.4186196\n\n\n\n\n\n3.     Consider sampling from Poisson distribution with rate \\(\\lambda\\) using the random walk Metropolis algorithm on the integers.\n\n(a)  Calculate the acceptance probabilities for this Markov chain. What proposals are always accepted with probability 1?\n\nSolution. The Poisson distribution has PMF \\[ \\pi(x) = \\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^x}{x!} . \\]\nThe acceptance probability for “up one” is \\[ \\alpha(x, x+1) = \\min \\left\\{ \\frac{\\pi(x+1)}{\\pi(x)} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^{x+1}}{(x+1)!}}{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^x}{x!}} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{\\lambda}{x+1}, \\, 1 \\right\\} . \\] This is 1 if the step up remains less than or equal to \\(\\lambda\\).\nThe acceptance probability for “down one” is \\[ \\alpha(x, x-1) = \\min \\left\\{ \\frac{\\pi(x-1)}{\\pi(x)} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^{x-1}}{(x-1)!}}{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^x}{x!}} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{x}{\\lambda}, \\, 1 \\right\\} . \\] This is 1 if the step down is from greater than or equal to \\(\\lambda\\).\n\n\n(b)  Suggest a good initial starting point \\(X_1\\) for your Markov chain. Why did you choose this?\n\nSolution. Somewhere in the middle of the distribution would be good. I would suggest the nearest integer to \\(\\lambda\\) as a pretty good place to start, but that’s not the only sensible choice.\n\n\n\n(c)  Write some R code to run this Markov chain in the case \\(\\lambda = 4.5\\).\n\nThis is my code\n\nlambda &lt;- 4.5\n\nacceptup   &lt;- function(x, lambda) lambda / (x + 1)\nacceptdown &lt;- function(x, lambda) x / lambda\ninitial &lt;- round(lambda)\n\nn &lt;- 1e6\nMRW &lt;- rep(0, n)\nMRW[1] &lt;- initial\n\nfor (i in 1:(n - 1)) {\n  if (runif(1) &lt; 0.5) {\n    # up proposal\n    if (runif(1) &lt; acceptup(MRW[i], lambda)) MRW[i + 1] &lt;- MRW[i] + 1\n    else                                     MRW[i + 1] &lt;- MRW[i]\n  } else {\n    # down proposal\n    if (runif(1) &lt; acceptdown(MRW[i], lambda)) MRW[i + 1] &lt;- MRW[i] - 1\n    else                                       MRW[i + 1] &lt;- MRW[i]\n  }\n}\n\nLet’s check if this has worked by looking at the probabilities.\n\nobserved &lt;- table(MRW)[1:11] / n\nexpected &lt;- dpois(0:10, lambda)\nround(rbind(observed, expected), 4)\n\n              0      1      2      3      4      5      6      7      8      9\nobserved 0.0109 0.0504 0.1125 0.1682 0.1893 0.1709 0.1282 0.0823 0.0466 0.0235\nexpected 0.0111 0.0500 0.1125 0.1687 0.1898 0.1708 0.1281 0.0824 0.0463 0.0232\n             10\nobserved 0.0107\nexpected 0.0104\n\n\nThis looks like an excellent match.\n\n\n\n(d)  Using your Markov chain, obtain an MCMC estimate of \\(\\operatorname{\\mathbb E}X(X-1)\\), where \\(X \\sim \\operatorname{Po}(4.5)\\).\n\nSolution.\n\nmean(MRW * (MRW - 1))\n\n[1] 20.28441\n\n\n\n\n\n(e)  (Optional) Calculate the correct answer, and comment on the accuracy of your estimate.\n\nSolution. We have \\[ \\mathbb EX(X-1) = \\sum_{x=0}^\\infty x(x-1)\\,\\mathrm{e}^{\\lambda}\\,\\frac{\\lambda^x}{x!} =  \\lambda^2\\, \\mathrm{e}^{-\\lambda} \\sum_{x=2}^\\infty \\frac{\\lambda^{x-2}}{(x-2)!} = \\lambda^2 \\,\\mathrm{e}^{-\\lambda}\\,\\mathrm{e}^{\\lambda} = \\lambda^2 . \\] So the correct answer here is \\(\\lambda^2 = 4.5^2 = 20.25\\).\nI found my Markov chain always gets it right to the nearest integer and often gets it right to 1 decimal place (either 20.2 or 20.3). It is not quite as accurate basic Monte Carlo would be (which almost always gets 1 decimal place with a million samples, and occasionally two decimal places), but it is pretty good.\n\n\n\n\n4.     For \\(-1 &lt; \\alpha &lt; 1\\), consider the Markov chain on \\(\\mathcal S = \\mathbb R\\) given by \\(X_{i+1} = \\alpha X_i + Z_i\\), where the \\(Z_i \\sim \\operatorname{N}(0, 1)\\) are IID standard normals. (Students who have studied time series will recognise this as an AR(1) autoregressive process.)\n\n(a)  Write down the transition density \\(p(x, y)\\) for this Markov chain.\n\nSolution. Given \\(X_i = x\\), we have that \\(X_{i+1} = \\alpha x + \\operatorname{N}(0, 1) = \\operatorname{N}(\\alpha x, 1)\\). So the transition density is \\[ p(x, y) = \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-(y - \\alpha x)^2/2} . \\]\n\n\n\n(b)  Find a stationary distribution for this Markov chain.\n\nSolution. There are some long-winded ways to do this. But I would start by guessing there’s a pretty good chance the stationary distribution will be normally distributed. So let’s guess there’s a stationary distribution \\(\\operatorname{N}(\\mu, \\sigma^2)\\), and see if we can find \\(\\mu\\) and \\(\\sigma^2\\) that satisfy this. (If we can’t, then our guess was wrong, and we’ll have to go back to the drawing board.)\nIf \\(X \\sim \\operatorname{N}(\\mu, \\sigma^2)\\), then \\(\\alpha X \\sim \\operatorname{N}(\\alpha\\mu, \\alpha^2\\sigma^2)\\) and \\(\\alpha X + Z \\sim \\operatorname{N}(\\alpha\\mu, \\alpha^2\\sigma^2 + 1)\\). To have \\(X\\) with the same distribution as \\(\\alpha X + Z\\), we need \\(\\operatorname{N}(\\mu, \\sigma^2) = \\operatorname{N}(\\alpha\\mu, \\alpha^2\\sigma^2 + 1)\\).\nThis will hold true if the parameters are the same on bother sides. Looking at the mean parameters, we need \\(\\mu = \\alpha\\mu\\), which forces \\(\\mu = 0\\). Looking at the variance parameters, we need \\(\\sigma^2 = \\alpha^2 \\sigma^2 + 1\\), so \\(\\sigma^2 = 1/(1 - \\alpha^2)\\). These satisfy the equation. So a stationary distribution is \\[ \\operatorname{N}\\bigg(0, \\,\\frac{1}{1 - \\alpha^2}\\bigg) . \\]\n\n\n\n\n5.     Let \\(Y \\sim \\operatorname{N}(-3, 1)\\) and \\(Z \\sim \\operatorname{N}(3,1)\\). Let \\(X\\) be a mixture distribution that equals \\(Y\\) with probability \\(\\frac12\\) and equals \\(Z\\) with probability \\(\\frac12\\); in other words, if \\(f\\) is the PDF of \\(Y\\) and \\(g\\) is the PDF of \\(Z\\), then \\(\\pi(x) = \\frac12 f(x) + \\frac12 g(x)\\) is the PDF of \\(X\\).\n\n(a)  Draw a graph of \\(\\pi\\).\n\nSolution.\n\npdf &lt;- function(x) 0.5 * dnorm(x, -3, 1) + 0.5 * dnorm(x, 3, 1)\ncurve(pdf, from = -7, to = 7, xlim = c(-6, 6), n = 1001, col = \"blue\", lwd = 2)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\n\n\n(b)  Your intention is to sample (approximately) from \\(X\\) using the random walk Metropolis algorithm on the state space \\(\\mathcal S = \\mathbb R\\). Explain why this could be tricky, and why a good choice of the typical stepsize \\(\\sigma\\) will be particularly important.\n\nSolution. We have a “bimodal” distribution, with two humps. If we’re not carefully, our random walk might get stuck for a long time in just one of the humps, without exploring the other one, which give an output that is not representative of the full distribution. In particular, if the typical step size \\(\\sigma\\) is too small, getting from one hump to the other will require a long and unlikely trek through the “low probability zone” between the two humps.\nHence it will be vitally important to make sure that \\(\\sigma\\) is big enough that jumps between the two humps can happen reasonably often. (Although, as ever, having the typical step size \\(\\sigma\\) too big brings it’s own problems of rejecting moves that overshoot the other hump.)\n\n\n\n(c)  Write an R program that will run the random walk Metropolis algorithm with target distribution \\(\\pi\\). Experiment with different values of the typical step size \\(\\sigma\\). What did you discover, and what value of \\(\\sigma\\) did you find most appropriate?\n\nSolution.\n\ntwohump &lt;- function(n, stepsize, initial) {\n  MC &lt;- rep(0, n)\n  MC[1] &lt;- initial\n  for (i in 1:(n - 1)) {\n    prop &lt;- MC[i] + rnorm(1, 0, stepsize)\n    if (runif(1) &lt; pdf(prop) / pdf(MC[i])) MC[i + 1] &lt;- prop\n    else MC[i + 1] &lt;- MC[i]\n  }\n  return(MC)\n}\n\nI’ll start with stepsize 0.5.\n\nset.seed(6)\nhist(twohump(1e5, 0.1, 0), breaks = 50, probability = TRUE)\n\n\n\n\n\n\n\n\nThis has gone very badly – I’ve spent much more time in the left hump than the right hump. I found in general that sometimes I got lucky and got balanced humps in the histogram, but on other occasions it was even more unbalanced than the one shown above.\nPerhaps better would be to set \\(\\sigma = 6\\), since that is the gap between the two peaks.\n\nhist(twohump(1e5, 6, 0), breaks = 50)\n\n\n\n\n\n\n\n\nThat looks better – the humps are pretty balanced now (although not perfectly balanced each time).\nTo be more rigorous, we could look at the autocorrelation, specifically \\(1 + 2 \\sum_{k=0}^{\\infty} \\rho(k)\\), which we saw was an important figure in Lecture 22.\n\ntrials &lt;- c(0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000)\nresults &lt;- rep(0,12)\nfor (i in 1:12) {\n  MC &lt;- twohump(1e5, trials[i], 0)\n  results[i] &lt;- 1 + 2 * sum(acf(MC, lag.max = 1000, plot = FALSE)$acf)\n}\nplot(trials, results, xlab = \"stepsize\", ylab = \"autocorrelation calculation\", log = \"x\")\n\n\n\n\n\n\n\n\nThis suggests that stepsizes around about the size of 10 is the right order of magnitude. Let’s look a bit closer.\n\ntrials &lt;- 2*(1:15)\nresults &lt;- rep(0,15)\nfor (i in 1:15) {\n  MC &lt;- twohump(1e5, trials[i], 0)\n  results[i] &lt;- 1 + 2 * sum(acf(MC, lag.max = 1000, plot = FALSE)$acf)\n}\nplot(trials, results, xlab = \"stepsize\", ylab = \"autocorrelation calculation\")\n\n\n\n\n\n\n\n\nThis is a rather noisy picture – I’d have to do more and longer experiments to find out more. But I’d suggest something in the 4 to 10 range is probably best.\n\n\n\n(d) Estimate \\(\\mathbb EX\\) using your program. Comment on the accuracy of your estimation.\n\nThe estimate is simply\n\nmean(twohump(1e6, 6, 0))\n\n[1] -0.002129821\n\n\nThe answer should be 0. I find this is usually pretty accurate – I usually get 0.0 to one decimal place, and often 0.00 to two decimal places.\n\n\n\n\n6.     Consider the Metropolis–Hastings algorithm on the state space \\(\\mathbb R\\) with target density \\[ \\pi(x) \\propto \\sin^2(x) \\,\\exp(-|x|) . \\] Each of the following proposal methods gives a formula for the proposed next state \\(Y_{i+1}\\) given the current state \\(X_{i}\\). For each proposal method, write down the proposal density \\(r(x,y)\\) and calculate the acceptance probability \\(\\alpha(x, y)\\).\n\n(a)  \\(Y_{i+1} = X_i + Z_i\\), where \\(Z_i \\sim \\operatorname{N}(0,1)\\) are IID.\n\nSolution \\({\\displaystyle r(x, y) = \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-(y-x)^2/2}}\\).\nBecause this is symmetric, the acceptance probability is \\[ \\begin{align}\n\\alpha(x, y) = \\min \\left\\{ \\frac{\\pi(y)}{\\pi(x)},\\,1 \\right\\} &= \\min \\left\\{ \\frac{\\sin^2(y) \\,\\exp(-|y|)}{\\sin^2(x) \\,\\exp(-|x|)},\\,1 \\right\\} \\\\ &= \\min \\left\\{ \\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp(|x|-|y|),\\,1 \\right\\} .\n\\end{align} \\]\n\n\n\n(b)  \\(Y_{i+1} = X_i + Z_i\\), where \\(Z_i \\sim \\operatorname{U}[-1,2]\\) are IID.\n\nSolution \\(r(x, y) = \\frac13\\) is \\(x-1 \\leq y \\leq x+2\\).\nNow the acceptance probability. If \\(x - 1 \\leq y \\leq x + 1\\), then \\(r(x,y) = r(y,x) = \\frac13\\). In that case, \\[ \\alpha(x, y) =  \\min \\left\\{ \\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp(|x|-|y|),\\,1 \\right\\}  \\] again. On the other hand, if \\(x + 1 &lt; y \\leq x + 2\\), then \\(r(x, y) = \\frac13\\) while \\(r(y,x) = 0\\). In this case, \\(\\alpha(x, y) = 0\\).\n\n\n\n(c)  \\(Y_{i+1} \\sim \\operatorname{N}(0,1)\\), independent of \\(X_i\\).\n\nSolutions. In this case, \\(r(x, y)\\) does not depend on \\(x\\) at all. We have simply \\[ r(x, y) = \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-y^2/2} . \\] \\[ \\begin{align}\n\\alpha(x, y) &=  \\min \\left\\{ \\frac{\\mathrm{e}^{-y^2/2} \\,\\pi(y)}{\\mathrm{e}^{-x^2/2} \\, \\pi(x)} \\right\\} \\\\\n&=  \\min \\left\\{ \\frac{\\mathrm{e}^{-y^2/2}}{\\mathrm{e}^{-x^2/2}}\\,\\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp(|x|-|y|),\\,1 \\right\\} \\\\\n&= \\min \\left\\{ \\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp\\big(x^2/2 + |x|-y^2/2 - |y|\\big),\\,1 \\right\\}\n\\end{align} \\]",
    "crumbs": [
      "MCMC",
      "Problem Sheet 4"
    ]
  },
  {
    "objectID": "lectures/L22-mcmc-error.html",
    "href": "lectures/L22-mcmc-error.html",
    "title": "22  MCMC error",
    "section": "",
    "text": "22.1 Bias for MCMC\nBack in Lectures 3 and 4 we looked at the bias, variance, mean-square error and root-mean-square error for the Monte Carlo estimator \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] of \\(\\theta = \\operatorname{\\mathbb E}\\phi(X)\\) where the samples \\(X_i\\) are IID with the same distribution as \\(X\\). We saw that the estimator is unbiased, and that the variance and mean-square error are \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{MC}}\\big) = \\operatorname{MSE} \\big(\\widehat\\theta_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\Var\\big(\\phi(X)\\big) . \\]\nWe now want to find these same values where the samples \\(X_i\\) are not IID but are the output of a Markov chain whose stationary distribution is that of \\(X\\). This will be harder. We saw (under certain technical conditions that we will assume hold throughout) that the \\(X_i\\) tend to the distribution of \\(X\\) in the limit as \\(i \\to \\infty\\). But this is not the same as saying that their distribution is exactly the same as \\(X\\) (let alone are independent). So here we can get as far as \\[ \\Exg \\widehat\\theta_n^{\\mathrm{MC}} =  \\Exg \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n} \\sum_{i=1}^n \\Exg\\phi(X_i) \\] (remembering that linearity of expectation does not require independence), but then we’re a bit stuck.\nTo make progress, we will make a simplifying assumption. Suppose we picked the initial state \\(X_1\\) according to the distribution \\(\\pi\\). Then, since \\(\\pi\\) is a stationary distribution, \\(X_2\\) is distributed according to \\(\\pi\\) too. And \\(X_3\\), and \\(X_4\\), and so on. And \\(\\pi\\) itself is the distribution for \\(X\\). So, if we started from the stationary distribution – or “in equilibrium” – then we have \\[  \\Exg \\widehat\\theta_n^{\\mathrm{MC}} =  \\Exg \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n} \\sum_{i=1}^n \\Exg\\phi(X_i) = \\frac{1}{n}\\,n\\Exg\\phi(X) = \\Exg\\phi(X) ,\\] and our estimator is unbiased.\nOf course, in real life, it is highly unlikely that we are able to sample the initial state from \\(\\pi\\). After all, if we could do that, we could presumably sample all the \\(X_i\\) from \\(\\pi\\) independently, as just use the basic Monte Carlo estimator instead. However, if we have used a burn-in period of appropriate length, we hope that by the time we take the first sample “that counts”, after the burn-in period, that will be very close to the stationary distribution, and hence we will have \\[  \\Exg \\widehat\\theta_n^{\\mathrm{MC}} \\approx \\Exg\\phi(X) ,\\] and our estimator will be approximately unbiased.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>MCMC error</span>"
    ]
  },
  {
    "objectID": "lectures/L22-mcmc-error.html#variance-for-mcmc",
    "href": "lectures/L22-mcmc-error.html#variance-for-mcmc",
    "title": "22  MCMC error",
    "section": "22.2 Variance for MCMC",
    "text": "22.2 Variance for MCMC\nWhat about the variance of the MCMC estimator. Unlike the basic IID Monte Carlo case, we can no longer say \\[  \\Var \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big) =  \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\Var\\big(\\phi(X_i)\\big),\\] because the samples from a Markov chain are not independent (although we have limited their dependence structure).\nInstead, we have to include the “cross” covariance terms: \\[  \\Var \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big) =  \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n^2} \\left(\\sum_{i=1}^n \\Var\\big(\\phi(X_i)\\big) + 2 \\sum_{i &lt; j} \\Cov\\big(\\phi(X_i), \\phi(X_j)\\big) \\right) .\\]\nAgain, we grind to a halt as far as exact results are concerned. But we can again invoke our simplifying assumption that \\(X_1\\) was chosen from \\(\\pi\\), and that we are in equilibrium. Then the variance terms are all \\(\\Var(\\phi(X_i)) = \\Var(\\phi(X))\\), which I shall call \\(\\sigma^2\\). What about the covariance terms? Well, if the Markov chain is stationary, then \\(\\Cov(\\phi(X_i), \\phi(X_j))\\) only depends on how many steps apart \\(i\\) and \\(j\\) are. In equilibrium, \\(\\Cov(\\phi(X_1), \\phi(X_7))\\) is the same as \\(\\Cov(\\phi(X_2), \\phi(X_8))\\) or \\(\\Cov(\\phi(X_{101}), \\phi(X_{107}))\\): these all represent the covariance between one state chosen according to \\(\\pi\\) and the state \\(k = 7 - 1 = 6\\) steps later.\nSo here we can write \\[ \\Cov\\big(\\phi(X_i), \\phi(X_j)\\big) = \\gamma(j - i) = \\gamma(k) \\] where \\(k = j - i\\) is the number of steps between \\(i\\) and \\(j\\). Students who have studied time series will know that \\(\\gamma(k)\\) is called the autocovariance at lag \\(k\\). (The prefix “auto-” mean “self-”, and “lag” means something like a waiting time.)\n(The tempting hope that we might have \\(\\gamma(k) = 0\\) for \\(k \\geq 2\\) is not true. [EXPLAIN])\nSo now, in equilibrium, we have \\[  \\begin{align}\n\\Var \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big)\n&=  \\frac{1}{n^2} \\left(\\sum_{i=1}^n \\Var\\big(\\phi(X_i)\\big) + 2 \\sum_{i &lt; j} \\Cov\\big(\\phi(X_i), \\phi(X_j)\\big) \\right) \\\\\n&= \\frac{1}{n^2} \\left(\\sum_{i=1}^n \\sigma^2 + 2 \\sum_{i &lt; j} \\gamma(j - i) \\right) \\\\\n&=  \\frac{1}{n^2} \\left(n\\sigma^2 + 2 \\sum_{i &lt; j} \\gamma(j - i) \\right)\n\\end{align} \\]\nWhen we studied antithetic variables, we found it more convenient to work with the correlation \\[ \\operatorname{Corr}\\big(\\phi(X_i),\\phi(X_j)\\big) = \\frac{\\operatorname{Cov}\\big(\\phi(X_i),\\phi(X_j)\\big)}{\\sqrt{\\operatorname{Var}\\big(\\phi(X_i)\\big)\\operatorname{Var}\\big(\\phi(X_i)\\big)}} . \\] In equilibrium, this is \\[ \\rho(j - i) = \\operatorname{Corr}\\big(\\phi(X_i),\\phi(X_j)\\big) = \\frac{\\gamma{j-i}}{\\sqrt{\\sigma^2\\,\\sigma^2}} = \\frac{\\gamma(j-i)}{\\sigma^2}, \\] where \\(\\rho(k)\\) is called the autocorrelation at lag \\(k\\).\nSo now we have \\[  \\begin{align}\n\\Var \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big)\n&=  \\frac{1}{n^2} \\left(n\\sigma^2 + 2 \\sum_{i &lt; j} \\gamma(j - i) \\right) \\\\\n&= \\frac{1}{n}\\,\\sigma^2 + \\frac{2}{n^2} \\sum_{i &lt; j} \\rho(j - i) \\,\\sigma^2 \\\\\n&= \\frac{\\sigma^2}{n} \\left(1 + \\frac{2}{n} \\sum_{i=1}^n \\sum_{j = i+1}^n \\rho(j - i) \\right) \\\\\n&= \\frac{\\sigma^2}{n} \\left(1 + \\frac{2}{n} \\sum_{i=1}^n \\sum_{k = 1}^{n-i} \\rho(k) \\right) .\n\\end{align} \\]\nWe know from the limit theorem that, as \\(n \\to \\infty\\), a Markov chain tends to its stationary distribution regardless of what state it started from – it’s as if the Markov chain forgets where it starts from. Therefore we know that the autocorrelation \\(\\rho(k)\\) tends to 0 as \\(k \\to \\infty\\). So, provided that the number of samples \\(n\\) is large, there usually very little loss from approximating \\(\\sum_{k = 1}^{n-i} \\rho(k)\\) by \\(\\sum_{k = 1}^{\\infty} \\rho(k)\\), because the extra autocorrelations we’ve added in will all be very small.\nFinally, we get the result \\[  \\begin{align}\n\\Var \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big)\n&= \\frac{\\sigma^2}{n} \\left(1 +  \\frac{2}{n} \\sum_{i=1}^n \\sum_{k = 1}^{n-i} \\rho(k) \\right) \\\\\n&\\approx \\frac{\\sigma^2}{n} \\left(1 + \\frac{2}{n} \\sum_{i=1}^n \\sum_{k = 1}^{\\infty} \\rho(k) \\right) \\\\\n&= \\frac{\\sigma^2}{n} \\left(1 + \\frac{2}{n}\\,n \\sum_{k = 1}^{\\infty} \\rho(k) \\right) \\\\\n&= \\frac{\\sigma^2}{n} \\left(1 + 2 \\sum_{k = 1}^{\\infty} \\rho(k) \\right) .\n\\end{align} \\]\nIn conclusion, we have the following.\n\nTheorem 22.1 Let \\((X_i)\\) be a Markov chain started in its stationary distribution \\(\\pi\\), and let \\(X\\) have distribution \\(\\pi\\) also. Consider \\(\\theta = \\Exg\\phi(X)\\) and its MCMC estimator \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) .\\] Then, writing \\(\\sigma^2 = \\Var(\\phi(X))\\), we have the following:\n\n\\(\\widehat\\theta_n^{\\mathrm{MC}}\\) is unbiased, in that \\(\\Exg \\widehat\\theta_n^{\\mathrm{MC}} = \\theta\\).\nThe variance of \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) is approximately \\[ \\Var \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big) \\approx \\frac{\\sigma^2}{n} \\left(1 + 2 \\sum_{k = 1}^{\\infty} \\rho(k) \\right) . \\]\nThe mean-square error of \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) is approximately \\[ \\operatorname{MSE} \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big) \\approx \\frac{\\sigma^2}{n} \\left(1 + 2 \\sum_{k = 1}^{\\infty} \\rho(k) \\right) . \\]\nThe root-mean-square error of \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) is approximately \\[ \\operatorname{RMSE} \\big( \\widehat\\theta_n^{\\mathrm{MC}}\\big) \\approx \\frac{\\sigma}{\\sqrt{n}} \\sqrt{1 + 2 \\sum_{k = 1}^{\\infty} \\rho(k) } . \\]\n\n\nCompared to the standard Monte Carlo variance \\(\\Var (\\widehat\\theta_n^{\\mathrm{MC}}) = \\sigma^2/n\\) we have the extra term \\(2 \\sum_{k = 1}^{\\infty} \\rho(k)\\). While it would be nice for the autocorrelation to be negative, to give us an improvement (like with antithetical variables), this almost never happens with Markov chains, which almost always have positive autocorrelation. Instead, we get the best results when the autocorrelation \\(\\rho(k)\\) dies away to 0 as quickly as possible, and get poor results when the autocorrelation only decays to 0 very slowly.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>MCMC error</span>"
    ]
  },
  {
    "objectID": "lectures/L22-mcmc-error.html#example",
    "href": "lectures/L22-mcmc-error.html#example",
    "title": "22  MCMC error",
    "section": "22.3 Example",
    "text": "22.3 Example\nLast time, we used the random walk Metropolis in continuous space to sample from the \\(\\operatorname{Exp}(0.1)\\) distribution. Let’s try some different typical step sizes \\(\\sigma = 2, 15, 400\\). Let’s use these Markov chains to estimate \\(\\mathbb EX\\) (which we know is 10), and examine the error in these estimators.\nThe function we used last time was this.\n\nmetroexp &lt;- function(n, rate, sigma, initial) {\n  MC &lt;- rep(0, n)\n  accept &lt;- function(x, y) exp(-rate * (y - x))\n\n  MC[1] &lt;- initial\n  for (i in 1:(n - 1)) {\n    prop &lt;- MC[i] + rnorm(1, 0, sigma)\n    if (prop &lt; 0)  MC[i + 1] &lt;- MC[i]\n    else if (runif(1) &lt;= accept(MC[i], prop)) MC[i + 1] &lt;- prop\n    else MC[i + 1] &lt;- MC[i]\n  }\n  \n  return(MC)\n}\n\nWe can plot the autocorrelation \\(\\rho(k)\\) against the lag \\(k\\) in R using the acf() function.\n\nMC_small &lt;- metroexp(1e6, 0.1, 2, 0) \nacf(MC_small, lag.max = 100)\n\n\n\n\n\n\n\nMC_medium &lt;- metroexp(1e6, 0.1, 15, 0)\nacf(MC_medium, lag.max = 100)\n\n\n\n\n\n\n\nMC_big &lt;- metroexp(1e6, 0.1, 400, 0)\nacf(MC_big, lag.max = 100)\n\n\n\n\n\n\n\n\nRemember that our goal is for the autocorrelation to die away as quickly as possible. We see that the large step size and small step size have the autocorrelation decaying slowly, while the medium step size has the autocorrelation decaying much quicker. This matched with our earlier discussion. When \\(\\sigma\\) is too small, the Markov chain crawls around the state space too slowly; when \\(\\sigma\\) is too large, large proposal moves are usually rejected, leading the Markov chain to stay in place. We need \\(\\sigma\\) in the “Goldilocks” position where \\(\\sigma\\) is small enough that the moves proposed are not too outrageous, but \\(\\sigma\\) is big enough that we make good progress around the state space.\nThis is confirmed by looking at the relevant term \\(1 + 2\\sum_{k=1}^\\infty \\rho(k)\\) from the variance of the MCMC estimator. Well, let’s just look at the sum of the first 1000 terms. We have\n\nsum_small  &lt;- 1 + 2 * sum(acf(MC_small,  lag.max = 1000, plot = FALSE)$acf) \nsum_medium &lt;- 1 + 2 * sum(acf(MC_medium, lag.max = 1000, plot = FALSE)$acf)\nsum_big    &lt;- 1 + 2 * sum(acf(MC_big,    lag.max = 1000, plot = FALSE)$acf)\nround(c(sum_small, sum_medium, sum_big), 1)\n\n[1] 251.5  14.8 106.5\n\n\nWith the small and big step sizes, we need to take over 100 samples from the Markov chain to get the equivalent of one independent sample. But with the medium step size, we are getting the equivalent of one independent sample from roughly every 15 samples of the Markov chain.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>MCMC error</span>"
    ]
  },
  {
    "objectID": "lectures/L23-bayes.html",
    "href": "lectures/L23-bayes.html",
    "title": "23  MCMC and Bayesian statistics",
    "section": "",
    "text": "23.1 Bayesian set-up\nToday, we will complete our study of Markov chain Monte Carlo by taking a look at how MCMC can be applied to Bayesian statistic.\nWe recall the Bayesian set-up. The data is modelled by the likelihood \\(f(\\mathbf x \\mid \\theta)\\), which is the distribution (probability mass or density function) of the data \\(\\mathbf x = (x_1, x_2, \\dots, x_n)\\) given some unknown parameter \\(\\theta\\). Typically, the datapoints \\(x_i\\) are assumed to be IID, so we can write \\[ f(\\mathbf x \\mid \\theta) = \\prod_{i=1}^n f(x_i \\mid \\theta) . \\]\nOur goal in statistics is to make inferences about – that is, to learn about – the unknown parameter \\(\\theta\\).\nIn the Bayesian paradigm, we start with a prior belief about what we think the likely values of \\(\\theta\\) are before we’ve collected any data. Our prior belief is represented by a distribution \\(\\pi(\\theta)\\).\nAfter collecting the data \\(\\mathbf x\\), we then seek the posterior distribution – that is, the distribution of \\(\\theta\\) conditional on the data we have seen. This is represented by a distribution \\(\\pi(\\theta \\mid \\mathbf x)\\). It is this posterior distribution that we want to learn about.\nWe can calculate the posterior from the prior and the likelihood using Bayes’ theorem. We have \\[ \\pi(\\theta \\mid \\mathbf x) \\propto \\pi(\\theta) \\,f(\\mathbf x \\mid \\theta) = \\pi(\\theta) \\prod_{i=1}^n f(x_i \\mid \\theta) . \\] More informally, we can say \\[ \\text{posterio} \\propto \\text{prior} \\times \\text{likelihood} . \\] Let us note for future reference that this only gives the posterior \\[ \\pi(\\theta \\mid \\mathbf x) = \\frac{1}{Z}\\, \\pi(\\theta) \\,f(\\mathbf x \\mid \\theta) = \\frac{1}{Z}\\, \\pi(\\theta) \\prod_{i=1}^n f(x_i \\mid \\theta)  \\] up to proportionality, where the normalising constant is \\[ Z = \\int \\pi(\\theta)\\,f(\\mathbf x \\mid \\theta) \\, \\mathrm{d}\\theta . \\]\nIf you have seen Bayesian statistics in previous modules, you may well have seen simple cases, where the posterior turns out to have a simple form – often in the same family as the prior but with a different parameter.\nFor example, take the case where the likelihood \\(X \\sim \\operatorname{Bin}(m, \\theta)\\) is a binomial distribution with a known number of trials \\(m\\) but an unknown success probability. If the prior for \\(\\theta\\) is a Beta distribution, then it’s easy to check that the posterior for \\(\\theta\\) is another Beta distribution, just with different parameters.\nBut aside from these simple “toy examples”, in real life the posterior distribution often has a complicated form. It is often not possible to calculate the normalising constant \\(Z\\); nor, therefore, can we calculate statistics of the posterior, like its expectation or confidence intervals.\nThis is where statistical computing comes in. If we can manage to sample from the posterior distribution, then we can estimate statistics of the posterior using Monte Carlo estimation. But (again, aside from the easy toy examples), sampling from the posterior seems hard. The inverse transform method requires the normalisation constant \\(Z\\), but that is typically to difficult to calculate. Occasionally, if we’re very clever, we can think for a long time and manage to come up with an envelope rejection sampling method. But we’d like a method we know will always work.\nMCMC is that method. If we use the random walk Metropolis algorithm, all we need to do is pick our step size \\(\\sigma\\) and set it going.\nA crucial point that we haven’t mentioned yet, is that Metropolis–Hastings works perfectly fine when we don’t know a normalising constant in the distribution we are sampling from. That’s because the acceptance probability – let’s just write it for the symmetric Metropolis case – is \\[ \\begin{align}\n\\alpha(\\theta, \\theta')\n&= \\min \\left\\{ \\frac{\\pi(\\theta' \\mid \\mathbf x)}{\\pi(\\theta \\mid \\mathbf x)} , \\, 1 \\right\\} \\\\\n&= \\min \\left\\{ \\frac{\\frac{1}{Z}\\,\\pi(\\theta')\\,f(\\mathbf x \\mid \\theta')}{\\frac{1}{Z}\\,\\pi(\\theta')\\,f(\\mathbf x \\mid \\theta)} , \\, 1 \\right\\} \\\\\n&= \\min \\left\\{ \\frac{\\pi(\\theta')}{\\pi(\\theta)}\\,\\frac{f(\\mathbf x \\mid \\theta')}{f(\\mathbf x \\mid \\theta)} , \\, 1 \\right\\} ,\n\\end{align} \\] where the \\(Z\\)s in the fraction have cancelled out.",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>MCMC and Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "lectures/L23-bayes.html#example",
    "href": "lectures/L23-bayes.html#example",
    "title": "23  MCMC and Bayesian statistics",
    "section": "23.2 Example",
    "text": "23.2 Example\n\nExample 23.1 Consider a statistical model where \\(X_i\\) are IID and normally distributed with unknown mean \\(\\mu\\) and and standard deviation \\(\\sigma\\). Here our parameter \\(\\boldsymbol\\theta = (\\mu, \\sigma)\\) is two-dimensional. So the likelihood is \\[ f(\\mathbf x \\mid \\mu, \\sigma) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\\exp \\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\,\\exp \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 \\right) . \\]\nThere are some special priors for which the posterior here has a very simple distribution. For example, if the prior for \\(\\mu\\) is a normal distribution and the prior for \\(\\sigma\\) is that \\(1/\\sigma^2\\) has a Gamma distribution and these are independent, then the posterior has the same form but with different parameters. But if your beliefs about \\(\\mu\\) and \\(\\sigma\\) aren’t well represented by a distribution of this form, then it will be very difficult or impossible to get a closed-form expression for the posterior.\nLet’s suppose our prior beliefs are represented something a bit weirder. Let’s say that our prior is that \\(\\mu\\) is uniformly distributed on \\([-10, 10]\\) and that \\(\\sigma\\) is exponentially distributed with rate \\(0.3\\) and that these are independent. So our prior is \\[\\pi(\\mu, \\sigma) = \\frac{1}{20}\\,0.3\\mathrm{e}^{-0.3\\sigma} \\qquad -10 \\leq \\mu \\leq 10, \\sigma \\geq 0 . \\]\nSuppose now that we see that data \\[\\mathbf x = (3.5, 13.7, 6.4, 0.7, 10.2, -2.3, 6.6, 5.1, 9.1, 10.7) . \\] We want to update our beliefs about \\(\\mu\\) and \\(\\sigma\\) and make inferences about the posterior. Finding the posterior here “by hand” seems a hopeless task. But we can certainly simulate from it with the random walk Metropolis algorithm.\n\nprior &lt;- function(mu, sigma) dunif(mu, -10, 10) * dexp(sigma, 0.3)\naccept &lt;- function(mu, sigma, propmu, propsigma, x) {\n  if (propsigma &lt; 0) return(0) else {\n    priorratio &lt;- prior(propmu, propsigma) / prior(mu, sigma)\n    lhr &lt;- dnorm(x, propmu, propsigma) / dnorm(x, mu, sigma)\n    return(priorratio * prod(lhr))\n  }\n}\n\nx &lt;- c(3.5, 13.7, 6.4, 0.7, 10.2, -2.3, 6.6, 5.1, 9.1, 10.7)\n\nn &lt;- 1e6\n\nstepmu    &lt;- 4\nstepsigma &lt;- 2\ninitialmu    &lt;- 0\ninitialsigma &lt;- 3\n\nMCmu    &lt;- rep(0, n)\nMCsigma &lt;- rep(0, n)\nMCmu[1]    &lt;- initialmu\nMCsigma[1] &lt;- initialsigma\n\nfor (i in 1:(n - 1)) {\n  propmu    &lt;- MCmu[i]    + rnorm(1, 0, stepmu)\n  propsigma &lt;- MCsigma[i] + rnorm(1, 0, stepsigma)\n  if (runif(1) &lt; accept(MCmu[i], MCsigma[i], propmu, propsigma, x)) {\n    MCmu[i + 1]    &lt;- propmu\n    MCsigma[i + 1] &lt;- propsigma\n  } else {\n    MCmu[i + 1]    &lt;- MCmu[i]\n    MCsigma[i + 1] &lt;- MCsigma[i]\n  }\n}\n\nIf we just want “point estimators” for \\(\\mu\\) and \\(\\sigma\\), we could just take the means of the posteriors distributions. We can calculate those using MCMC; they’re simply\n\nc(mean(MCmu), mean(MCsigma))\n\n[1] 6.294983 5.072932\n\n\nCompare these to the posterior means of \\(0\\) and \\(1/0.3 = 3.33\\) respectively.\nBut the strength of Bayesian statistics is not just computing point estimates, but understanding the entire posterior distribution. We can compare our histograms of the marginal distributions of the posterior with the marginal distributions of the prior.\n\nhist(MCmu, probability = TRUE, xlim = c(-12, 12), breaks = 50)\ncurve(dunif(x, -10, 10), add = TRUE, n = 1001, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\nhist(MCsigma, probability = TRUE, breaks = 80, xlim = c(0,12))\ncurve(dexp(x, 0.3), add = TRUE, n = 1001, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nAlthough in our prior the parameters \\(\\mu\\) and \\(\\sigma\\) were independent, that might not be the case any longer in our posterior.\n\ncor(MCmu, MCsigma)\n\n[1] -0.06948407\n\n\nIt appears there’s now a slight negative correlation (if \\(\\mu\\) is smaller than we expect, \\(\\sigma\\) has to be a little bit bigger to allow the data to fit).",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>MCMC and Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "lectures/L23-bayes.html#numerical-stability",
    "href": "lectures/L23-bayes.html#numerical-stability",
    "title": "23  MCMC and Bayesian statistics",
    "section": "23.3 Numerical stability",
    "text": "23.3 Numerical stability\n\nI didn’t get to the subsection in the lecture, so let’s say that this subsection is non-examinable.\n\nIn our example, we had only 10 data points. But when you have a large number of data points \\(n\\) (as is often the case in modern “big data” applications), then \\(f(\\mathbf x \\mid \\theta) = \\prod_{i=1}^n f(\\mathbf x \\mid \\theta)\\) can be extremely small. This can prove difficult when calculating the acceptance probability, because both the numerator and denominator in \\[ \\frac{\\pi(\\theta')\\, f(\\mathbf x \\mid \\theta')}{\\pi(\\theta)\\, f(\\mathbf x \\mid \\theta)} = \\frac{\\pi(\\theta')\\prod_{i=1}^n f(x_i \\mid \\theta')}{\\pi(\\theta)\\prod_{i=1}^n f(x_i \\mid \\theta)}  \\] can be extremely small. Dividing one very small number by another can be “numerically unstable”, where small rounding errors can lead to big errors.\nIt’s usually a good idea to split the fraction into multiple fractions each of which has a more sensible size; that is \\[ \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\times \\frac{f(\\mathbf x \\mid \\theta')}{f(\\mathbf x \\mid \\theta)} = \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\times \\prod_{i=1}^n \\frac{f(x_i \\mid \\theta')}{f(x_i \\mid \\theta)} . \\]\nEven more reliable can be to work on a log scale. We know that \\(a \\times b\\) can also be written as \\(\\exp(\\log a + \\log b)\\). So it can be even better to write the fraction above as \\[ \\begin{multline}\n\\exp \\big(\\log \\pi(\\theta') - \\log \\pi(\\theta) + \\log f(\\mathbf x \\mid \\theta') - \\log f(\\mathbf x \\mid \\theta) \\big) \\\\\n= \\exp \\bigg(\\log \\pi(\\theta') - \\log \\pi(\\theta) + \\sum_{i=1}^n \\log f(x_i \\mid \\theta') - \\sum_{i=1}^n \\log f(x_i \\mid \\theta) \\bigg) .\n\\end{multline} \\]",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>MCMC and Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "lectures/L23-bayes.html#mcmc-conclusions",
    "href": "lectures/L23-bayes.html#mcmc-conclusions",
    "title": "23  MCMC and Bayesian statistics",
    "section": "23.4 MCMC conclusions",
    "text": "23.4 MCMC conclusions\nIn the second part of this module, we saw various ways to get exact IID samples from the exact distribution we want (in-built R functions, inverse transform method, Box–Muller transform, basic rejection sampling, envelope rejection sampling). In this third part of the module, we’ve only really seen one way in detail – the random walk Metropolis algorithm – and it produces non-independent samples from approximately the distribution we want. That description makes MCMC sound much worse!\nBut MCMC using the random walk Metropolis algorithm has big advantages too. todo",
    "crumbs": [
      "MCMC",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>MCMC and Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "lectures/L24-empirical.html",
    "href": "lectures/L24-empirical.html",
    "title": "24  Empirical distribution",
    "section": "",
    "text": "24.1 Introduction\nSo far in this module, we have looked at sampling from distributions where we know the precise probability density function \\(f\\) or probability mass function \\(p\\). We have then been able to discover things about that distribution by sampling from \\(f\\) or \\(p\\): either exactly with independent samples (with the inverse transform method or envelope rejection sampling, for example), or approximately with samples of restricted dependence (with the Metropolis–Hastings algorithm).\nBut statisticians deal with data, not with probability distributions. A much more common situation is that we have some data. We believe that the data has come from a distribution, but we don’t know what that distribution is. Nonetheless, we still want to find out facts about that unknown distribution.\nOne traditional way to do this would be to fit a distribution to the data. By using knowledge about the context of the data and by examining the data itself, an appropriate parametric model could be chosen, and then the parameters could be estimated from the data. Once the model has then been specified, we can find out about that model using the ideas we have looked at in this course.\nBut sometimes this is not possible or desirable. With insufficient contextual knowledge, we might not be able to choose an appropriate parametric model. Or the data might not seem to fit any of the famous parametric models. And even if we did choose and fit a model, there’s no guarantee that out choice would be correct.\nInstead, we could look for the model that makes the fewest possible assumptions about the data. This is called the empirical distribution, which we will look at today. This is the basis for a collection of statistical techniques called the bootstrap (or bootstrapping), which we will look at for the next lecture.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Empirical distribution</span>"
    ]
  },
  {
    "objectID": "lectures/L24-empirical.html#definition-and-properties",
    "href": "lectures/L24-empirical.html#definition-and-properties",
    "title": "24  Empirical distribution",
    "section": "24.2 Definition and properties",
    "text": "24.2 Definition and properties\nConsider the following dataset \\(\\mathbf x = (x_1, x_2, x_3, x_4, x_5)\\) of \\(m = 5\\) values: \\[ 2, \\ 3, \\ 3, \\ 4, \\ 6 .\\]\nWe could try to fit a distribution to this data. For example, if we knew it was recording the number of absences from a class of 20 students over five lectures, we might think a binomial \\(\\operatorname{Bin}(20, p)\\) model was appropriate, and then try to estimate the value of \\(p\\). If it was recording the number of emails received per hour, we might think a Poisson \\(\\operatorname{Po}(\\lambda)\\) model was appropriate, and try to estimate the value of \\(\\lambda\\).\nBut that involves making assumptions. What if we wanted to make no assumptions about the data – or, at least, as few assumptions as possible? Well, we can say that in our data set, one fifth of the data was the value 2, two fifths was 3 (because there were two 3s in the dataset), one fifth was 4, and one fifth was 6. So we choose the model that these come from a random variable \\(X^*\\) where \\[ \\begin{align}\n\\mathbb P(X^* = 2) &= \\tfrac15 & \\mathbb P(X^* = 3) &= \\tfrac25 \\\\\n\\mathbb P(X^* = 4) &= \\tfrac15 & \\mathbb P(X^* = 6) &= \\tfrac15 .\n\\end{align} \\] This is called the empirical distribution. (The word “empirical” refers to what you actually observed, rather than assumed.)\n\nDefinition 24.1 Consider a dataset \\(\\mathbf x = (x_1, x_2, \\dots, x_m)\\). The empirical distribution \\(X^*\\) of this data is a discrete random variable with probability mass function \\(p^*(x) = \\mathbb P(X^* = x)\\), where \\[ p^*(x) = \\frac{1}{m} \\,\\big| \\{j : x_j = x \\} \\big| = \\frac{1}{m} \\sum_{j=1}^m \\mathbb{I}_{\\{x\\}}(x_j) . \\]\n\nIn other words, \\(p^*(x)\\) is simply the proportion of the dataset that took the value \\(x\\).\nWe’re not suggesting that the distribution \\(X^*\\) is necessarily the “true” distribution the data actually came from. Rather, we feel that by reducing any of our own assumptions that we make on the data, we are giving the data the best opportunity to “speak for itself”, rather than imposing our own views and opinions on the data.\nOn that point, consider this dataset of \\(m = 6\\) datapoints: \\[ 0.580, \\ 3.219, \\ 3.433, \\ 4.913, \\ 18.784, \\ 28.133. \\] It certainly looks like these came from a continuous distribution. But the empirical distribution \\(X^*\\) is still discrete – it takes each of those values with probability \\(\\tfrac16\\). The claim is not the the empirical distribution is likely to be “correct”; rather, the claim is that by minimising the assumptions we make, we aren’t polluting the data any further.\nLet’s think further about this random variable \\(X^*\\), the empirical distribution of a dataset \\(\\mathbf x\\).\nFirst, taking a single sample from the random variable \\(X^*\\) is equivalent to picking one of the datapoints at random. The probability we pick a value \\(x\\) is simply the proportion of the datapoints that take the value \\(x\\).\nTaking multiple IID samples from \\(X^*\\) is the same as sampling from the dataset with replacement – since, to be independent samples, it has to be possible to pick the same datapoint twice.\nSecond, since \\(X^*\\) is a random variable, we can do calculations with it just as we would any other random variable.\nFor example, we can calculate its expectation. This is \\[ \\begin{align}\n\\mathbb E_*X^* &= \\sum_x x\\,p^*(x) \\\\\n&=\\sum_x x\\, \\frac{1}{m} \\sum_{j=1}^m \\mathbb{I}_{\\{x\\}}(x_j)  \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m \\sum_x x\\, \\mathbb{I}_{\\{x\\}}(x_j) .\n\\end{align} \\] Now, if we think about the term \\(x\\, \\mathbb{I}_{\\{x\\}}(x_j)\\) inside the sum over \\(x\\) here, the indicator will equal 0 for every term in the sum except the term \\(x = x_j\\), when the term will equal \\(x \\times 1 = x_j\\). Hence, we have \\[ \\mathbb E_*X^* = \\frac{1}{m} \\sum_{j=1}^m x_j . \\] But this is just the sample mean of the data set. The empirical expectation is the sample mean, \\(\\mathbb E_*X^* = \\overline x\\).\nYou probably noticed the notation \\(\\mathbb E_*\\) here. We use this notation when we want to emphasise we are taken the expectation over the empirical distribution, taking the data as fixed. This isn’t really needed here – we know our data is the fixed observations \\(\\mathbf x\\). But later on, we will model the data itself as being samples from a random variable. There we will want to distinguish between taking an expectation \\(\\mathbb E\\) over the randomness in the samples themselves and taking an expectation \\(\\mathbb E_*\\) over the empirical distribution while treating the samples as fixed.\nWe can also calculate the variance similarly. Since \\(\\mathbb E_*X^* = \\overline x\\), we have \\(\\operatorname{Var}_*(X^*) = \\mathbb E_*(X^* - \\overline x)^2\\). By the same argument as for the expectation, we have \\[ \\begin{align}\n\\operatorname{Var}_*(X^*) &= \\sum_x \\big(x - \\overline x\\big)^2 \\, p^*(x)  \\\\\n&= \\sum_x \\big(x - \\overline x\\big)^2 \\,\\frac{1}{m} \\sum_{j=1}^m \\mathbb{I}_{\\{x\\}}(x_j) \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m \\sum_x \\big(x - \\overline x\\big)^2 \\,\\mathbb{I}_{\\{x\\}}(x_j) \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m \\big(x_j - \\overline x\\big)^2 .\n\\end{align} \\] This is almost the sample variance – it just has \\(\\frac{1}{m}\\) in front instead of the usual \\(\\frac{1}{m-1}\\). This won’t make much difference when \\(m\\) is large.\nThird, its often more mathematically pleasant to work with is the empirical cumulative distribution function \\(F^*\\). This is \\(F^*(x) = \\mathbb P(X^* \\leq x)\\). We can calculate the empirical CDF as \\[ \\begin{align}\nF^*(x) = \\sum_{y \\leq x} p^*(y) &= \\sum_{y \\leq x} \\frac{1}{m}\\, \\big| \\{j : x_j = y \\} \\big| \\\\\n&= \\frac{1}{m}\\sum_{y \\leq x}\\big| \\{j : x_j = y \\} \\big| \\\\\n&= \\frac{1}{m}\\, \\big| \\{j : x_j \\leq x \\} \\big| .\n\\end{align} \\] So this is just the proportion of the datapoints that are less than or equal to \\(x_j\\). This tends to be more mathematically convenient, because the CDF works equally well for discrete and continuous random variables, so this is flexible to the fact that the empirical distribution is always discrete while the true distribution may be continuous.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Empirical distribution</span>"
    ]
  },
  {
    "objectID": "lectures/L24-empirical.html#empirical-distributions-in-r",
    "href": "lectures/L24-empirical.html#empirical-distributions-in-r",
    "title": "24  Empirical distribution",
    "section": "24.3 Empirical distributions in R",
    "text": "24.3 Empirical distributions in R\n\nExample 24.1 The heights (in cm) of 20 student’s surveyed in one of Dr Voss’s modules is as follows:\n\nheights &lt;- c(\n  180, 182, 182, 181, 164, 180, 154, 153, 177, 190,\n  182, 175, 167, 168, 185, 153, 172, 177, 176, 170\n)\nm &lt;- length(heights)\n\nIn R, the table() function tells us how many outcomes we had of each value. We can then find the empirical PMF \\(p^*\\) by dividing this by \\(m\\).\n\ntable(heights)\n\nheights\n153 154 164 167 168 170 172 175 176 177 180 181 182 185 190 \n  2   1   1   1   1   1   1   1   1   2   2   1   3   1   1 \n\ntable(heights) / m\n\nheights\n 153  154  164  167  168  170  172  175  176  177  180  181  182  185  190 \n0.10 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.10 0.10 0.05 0.15 0.05 0.05 \n\nplot(table(heights) / m, lwd = 3, col = \"blue\", ylab = \"p*\")\n\n\n\n\n\n\n\n\nWe can sample from the empirical distribution \\(X^*\\). Recall that we said taking IID samples from \\(X^*\\) is equivalent to sampling from \\(\\mathbf x\\) with replacement. Here is a sample of \\(n = 30\\) heights. (Note that this is more than the \\(m = 20\\) datapoints we had – but this is no problem if we are sampling with replacement.)\n\nsample(heights, 30, replace = TRUE)\n\n [1] 177 182 170 153 164 180 170 181 182 182 182 167 185 167 190 177 176 167 153\n[20] 182 180 170 168 180 175 153 153 170 180 170\n\n\nMake sure you use replace = TRUE to ensure you are sampling with replacement.\nThe expectation and variance of the empirical distribution are as follows.\n\nheights_mean &lt;- mean(heights)\nheights_var &lt;- sum((heights - heights_mean)^2) / m\nc(heights_mean, heights_var)\n\n[1] 173.40 109.64\n\n\nWe can form the empirical CDF in R with the ecdf() function.\n\nFstar &lt;- ecdf(heights)\nplot(Fstar, main = \"Empirical CDF of heights\", xlab = \"height (cm)\")\n\n\n\n\n\n\n\n\nThe object produced by ecdf() is a function. So we can find, for example, the empirical CDF at 172, \\(F^*(172)\\), which is the proportion of a dataset with heights less than or equal to 172 cm.\n\nFstar(172.333)\n\n[1] 0.4\n\n\n\nOne more thing on the empirical distribution \\(X^*\\). If we pick an index \\(J\\) uniformly from \\(\\{1, 2, \\dots, m\\}\\), then \\(x_J = X^*\\). That is, we can think of \\(X^*\\) as picking one of the datapoints uniformly at random. (This way of looking at \\(X^*\\) is used a lot in the book of Voss.)",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Empirical distribution</span>"
    ]
  },
  {
    "objectID": "lectures/L25-plugin.html",
    "href": "lectures/L25-plugin.html",
    "title": "25  Plug-in estimation & Bootstrap I",
    "section": "",
    "text": "25.1 The “plug-in” principle\nWe have one thing we didn’t get to last time, which is “plug-in estimation”.\nSuppose now that \\(\\mathbf X = (X_1, X_2, \\dots, X_m)\\) is an IID sample from a distribution \\(X\\) that is either unknown or too difficult to work with directly. How can we find out things about this distribution \\(X\\)?\nWell, we could form the empirical distribution \\(X^*\\) from these samples, and work with that instead.\nWe have to be a bit careful here, because there two levels of randomness here.\nWe will write \\(\\mathbb E\\), \\(\\mathbb P\\), \\(\\operatorname{Var}\\) and so on for the first type of randomness – that is, randomness coming from the random variable \\(X\\). We will write \\(\\mathbb E_*\\), \\(\\mathbb P_*\\), \\(\\operatorname{Var}_*\\) and so on for randomness coming from the empirical random variable \\(X^*\\) treating the samples \\(\\mathbf X\\) as fixed. So, for example, the expectation \\(\\mathbb E_*(\\phi(X^*))\\) is really shorthand for the conditional expectation \\(\\mathbb E_*(\\phi(X^*) \\mid \\mathbf X)\\).\nOne way to estimate something about the random variable \\(X\\) is to take the formula involving \\(X\\), then keep that same formula, but replace the true random variable \\(X\\) with the empirical random variable \\(X^*\\). This is called the plug-in principle, and such an estimator is a plug-in estimator – the idea is that we simply “plug \\(X^*\\) in” to the existing formula.\nThis is easier to see if we take some examples.\nSuppose we wanted to estimate the expectation \\(\\mathbb EX\\) of the true distribution \\(X\\). To estimate this, we instead plug in the empirical distribution \\(X^*\\) in place of \\(X\\) and the empirical expectation \\(\\mathbb E_*\\) in place of the expectation over the random samples \\(\\mathbb E\\). So our estimator is instead \\(\\mathbb E_*X^*\\). We saw last time that \\(\\mathbb E_*X^*\\) is the sample mean \\[\\overline X = \\frac{1}{m} \\sum_{j=1}^m X_j .\\] So the plug-in estimator for the expectation \\(\\mathbb EX\\) is the sample mean \\(\\overline X\\).\nSuppose we wanted to estimate the variance \\(\\operatorname{Var}(X)\\) of the true distribution. Again, we plug in \\(X^*\\), to instead find \\(\\operatorname{Var}_*(X^*)\\), which we saw last time is \\[ \\operatorname{Var}_*(X^*) = \\frac{1}{m} \\sum_{j = 1}^m \\big(X_j - \\overline X\\big)^2 , \\] which is very similar to the sample variance of \\(\\mathbf X\\).\nSuppose wanted to estimate \\(\\operatorname{\\mathbb E}\\phi(X)\\) for some function \\(\\phi\\). The plug-in estimator for this is \\[ \\begin{align}\n\\operatorname{\\mathbb{E}}_* \\phi(X^*) &= \\sum_x \\phi(x)\\,p^*(x) \\\\\n&=\\sum_x \\phi(x)\\, \\frac{1}{m} \\sum_{j=1}^m \\mathbb{I}_{\\{x\\}}(X_j)  \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m \\sum_x \\phi(x)\\, \\mathbb{I}_{\\{x\\}}(X_j) \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m \\phi(X_j) ,\n\\end{align} \\] by the same logic we used for the expectation and variance last time. This is the Monte Carlo estimator from the beginning of this module – we have a sample \\(X_1, X_2, \\dots, X_m\\) and we have form the Monte Carlo estimator \\(\\operatorname{\\mathbb E}\\phi(X)\\). This shows there are deep connections between Monte Carlo estimation and the the empirical distribution and plug-in estimation.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Plug-in estimation & Bootstrap I</span>"
    ]
  },
  {
    "objectID": "lectures/L25-plugin.html#the-plug-in-principle",
    "href": "lectures/L25-plugin.html#the-plug-in-principle",
    "title": "25  Plug-in estimation & Bootstrap I",
    "section": "",
    "text": "First, there is the fact that the samples \\(\\mathbf X = (X_1, X_2, \\dots, X_m)\\) are random IID samples from \\(X\\).\nOnce we have the samples \\(\\mathbf X\\), that fixes the empirical PMF \\(p^*\\). Then \\(X^*\\) is itself a random variable, with PMF \\(p^*\\).",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Plug-in estimation & Bootstrap I</span>"
    ]
  },
  {
    "objectID": "lectures/L25-plugin.html#the-bootstrap-set-up",
    "href": "lectures/L25-plugin.html#the-bootstrap-set-up",
    "title": "25  Plug-in estimation & Bootstrap I",
    "section": "25.2 The bootstrap set-up",
    "text": "25.2 The bootstrap set-up\nOK, we’re now moving on from the empirical distribution to a slightly different but related topic: the bootstrap.\nSuppose a statistician is interested in a statistic \\(T = T(X_1, X_2, \\dots, X_n)\\) of \\(n\\) IID samples from a random variable \\(X\\). For example, this might be:\n\nSuppose I pick a basketball squad of 12 players at random; what is their average height? Here, \\(X\\) is the distribution of basketball players’ heights, \\(n = 12\\), and the statistic is \\[ T = T(X_1, X_2, \\dots, X_{12}) = \\frac{1}{12} \\sum_{i=1}^{12} X_i . \\]\nSuppose I visit The Edit Room cafe 5 times; what’s the longest queue I have to deal with. Here, \\(X\\) is the distribution of queue lengths at The Edit Room, \\(n = 5\\), and the statistic is \\[ T = T(X_1, X_2, X_3, X_4, X_5) = \\max\\{X_1, X_2, X_3, X_4, X_5\\} . \\]\nSuppose a supermarket distributor buys 1001 beef steaks; what is the median weight of the steaks? Here \\(X\\) is the distribution of weights of steaks, \\(n = 1001\\), and the statistic is \\[ T = T(X_1, X_2, \\dots, X_{1000}) = \\operatorname{median} (X_1, X_2, \\dots, X_{1001}) . \\]\n\nThe statistician is likely to be interested in properties of this statistic. For example, three of the most important things the statistician is likely to want to know are:\n\nThe expectation \\(\\mathbb ET = \\mathbb ET(X_1, \\dots, X_n)\\) of the statistic.\nThe variance \\(\\operatorname{Var}(T) = \\operatorname{Var}(T(X_1, \\dots, X_n))\\) of the statistic – or related concepts like the standard deviation.\nA prediction interval \\([U,V]\\) for the statistic, such that \\(\\mathbb P(T \\in [U,V]) = 1-\\alpha\\).\n\nNow, if the statistician knew the true distribution \\(X\\), and if it were simple enough to work with, then she could calculate the true values of these. But suppose the distribution is unknown (or too complicated to work with). Instead, the statistician just has \\(m\\) samples \\(\\mathbf X = (X_1, X_2, \\dots, X_m)\\). You could think of these as data measurements that are modelled as coming from the distribution \\(X\\), or you could think of them as output from a computer program that can sample from \\(X\\) exactly.\nNote that there’s two numbers here: \\(n\\) is the number of samples required to calculate the statistic \\(T = T(X_1, X_2, \\dots, X_n)\\) once, and \\(m\\) is the total number of samples we have available. The most common situation is “\\(m\\) is somewhat bigger than \\(n\\), although not vastly bigger”, but the mathematical definitions are valid for any \\(n\\) and \\(m\\).\nThe bootstrap method is the following idea:\n\nTake \\(n\\) samples from the empirical distribution \\(X^*\\) of \\(\\mathbf X\\). This is equivalent to sampling \\(n\\) of the values \\(X_1, X_2, \\dots, X_m\\) with replacement. Let’s call these samples \\(X^*_1, X^*_2, \\dots, X^*_n\\). Evaluate the statistic with these samples \\[ T^* = T(X_1^*, X_2^*, \\dots, X_n^*) . \\]\nRepeat step 1 many times; let’s say \\(B\\) times. Keep taking \\(n\\) of the samples with replacement and evaluating the statistic. We now have \\(B\\) versions of that statistic \\(T^*_1, T^*_2, \\dots, T^*_B\\).\nUse these \\(B\\) versions of the statistic to get a bootstrap estimate of the expectation, variance, or prediction interval. To estimate the expectation of the statistic \\(\\mathbb ET = \\mathbb ET(X_1, \\dots, X_n)\\), use the sample mean of the evaluated statistics \\(\\overline{T^*} = \\frac{1}{B} \\sum_{k=1}^B T^*_B\\). To estimate the variance \\(\\operatorname{Var}(T)\\) use the sample variance \\[ \\sum{1}{B-1} \\sum_{k=1}^B \\big(T^*_k - \\overline{T^*}\\big) .\\] We’ll come back to the prediction interval next time.\n\nThe bootstrap concept was discovered by the American statistician Bradley Efron in a hugely influential paper “Bootstrap methods: another look at the jackknife” in 1979. The name “bootstrap” comes from the phrase “to pull yourself up by your bootstraps”, which roughly means to make progress without any outside help, in a way that might initially seem impossible – similarly, the bootstrap manages to estimate properties of a statistic by just reusing the same set of samples over and over again. (The “jackknife” in the title of Efron’s paper was and earlier, simpler, less powerful idea along similar lines, named after the multipurpose tool the jackknife.)",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Plug-in estimation & Bootstrap I</span>"
    ]
  },
  {
    "objectID": "lectures/L25-plugin.html#bootstrap-for-expectation-and-variance",
    "href": "lectures/L25-plugin.html#bootstrap-for-expectation-and-variance",
    "title": "25  Plug-in estimation & Bootstrap I",
    "section": "25.3 Bootstrap for expectation and variance",
    "text": "25.3 Bootstrap for expectation and variance\n\nExample 25.1 Let’s take the cafe example above. The statistic in question is \\[ T = T(X_1, X_2, X_3, X_4, X_5) = \\min\\{X_1, X_2, X_3, X_4, X_5\\} . \\] A researcher wants to estimate the expectation of this statistic.\nThe researcher visits The Edit Room at 30 random occasion and notes the following data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQueue length\n0\n1\n2\n3\n4\n5\n7\n11\nTotal\n\n\n\n\nNumber of occasions\n11\n5\n7\n3\n1\n1\n1\n1\n30\n\n\n\n \nWe start by taking 5 samples from the empirical distribution – that is, we choose 5 of the datapoints uniformly at random with replacement. Let’s say these are \\((0, 1, 4, 4, 5)\\). (It turns out we sampled the value 4 twice, even though it only occured once – that does happen sometimes when we’re sampling with replacement.) The value of the statistic for this sample is \\[ T_1^* = T(0, 1, 4, 4, 5) = \\max \\{0, 1, 4, 4, 5\\} = 5 .\\]\nWe keep doing this many times – we pick five samples with replacement, and calculate their maximum.\n\nqueues &lt;- c(rep(0, 11), rep(1, 5), rep(2, 7), rep(3, 3), 4, 5, 7, 11)\n\nboots &lt;- 1000\nmaxes &lt;- rep(0, boots)\nfor (k in 1:boots) {\n  minisample &lt;- sample(queues, 5, replace = TRUE)\n  maxes[k] &lt;- max(minisample)\n}\n\nThis gives us 1000 realisations of the test statistic. We can use these to look at the distribution of the test statistic:\n\ndist &lt;- table(maxes) / boots\ndist\n\nmaxes\n    0     1     2     3     4     5     7    11 \n0.005 0.035 0.219 0.248 0.108 0.103 0.131 0.151 \n\nplot(dist)\n\n\n\n\n\n\n\n\nWe can also look at particular figures of interest. For example, the expectation of \\(T\\) should be close to the sample mean of our \\(T^*\\)s, and the variance of \\(T\\) should be close to the sample variance of our \\(T^*s\\).\n\nc(mean(maxes), var(maxes))\n\n[1] 4.742000 9.659095\n\n\n\nNext time, we’ll look into bootstrap methods in more detail.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Plug-in estimation & Bootstrap I</span>"
    ]
  },
  {
    "objectID": "lectures/L26-bootstrap-error.html",
    "href": "lectures/L26-bootstrap-error.html",
    "title": "26  Bootstrap II",
    "section": "",
    "text": "26.1 Bootstrap with a prediction interval\nRecall where we had got to last time. We are interested in a statistic \\(T = T(X_1, \\dots, X_n)\\), where \\(X_1, \\dots, X_n\\) are IID copies of a random variable \\(X\\). We want to find out about the distribution of \\(T\\). But all we have to work with is \\(X_1, \\dots, X_m\\), where are\\(m\\) IID samples from \\(X\\).\nSo the bootstrap procedure is to repeatedly choose \\(n\\) samples \\(X^*_1, \\dots, X^*_n\\) from the empirical distribution \\(X^*\\) – or, equivalently, choose \\(n\\) of the values \\(X_1, \\dots, X_m\\) with replacement, and calculate the statistic \\(T^*_k = T(X^*_1, \\dots, X^*_n)\\). The distribution of the \\(T^*_k\\) should be similar to the distribution of \\(T\\).\nIn particular, to estimate \\(\\mathbb ET\\) we can use the sample mean of the \\(T^*_1, \\dots, T^*_B\\), and to estimate \\(\\operatorname{Var}(T)\\), we can use the sample variance \\(S^2\\) of the \\(T^*_k\\).\nWhat if we want to estimate a prediction interval – that is, an interval \\([U, V]\\) such that \\(\\mathbb P(U \\leq T \\leq V) \\approx 1- \\alpha\\)?\nThere is a lazy way to do this, which is to hope that the \\(T^*_k\\) are approximately normally distributed. With \\(\\overline{T^*}\\) as the sample mean and \\(S^2\\) as the sample variance of our observed statistics, we could take \\[ \\left[\\overline{T^*} - z_{\\alpha/2}\\,S,\\, \\overline{T^*} + z_{\\alpha/2}\\,S\\right] .\\] But we can do better than this by taking the actual distribution of the \\(T^*_k\\), which might not be normal, into account.\nInstead, we can take the sample quantiles of the \\(T^*_k\\). That is, put \\(T^*_1, \\dots, T^*_B\\) in increasing order. Go \\(\\alpha/2\\) of the way up the list to get the lower-\\(\\alpha/2\\) sample quantile, and \\(1 - \\alpha/2\\) of the way up the list to get the upper-\\(\\alpha/2\\) sample quantile. These two values can be our prediction interval.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Bootstrap II</span>"
    ]
  },
  {
    "objectID": "lectures/L26-bootstrap-error.html#bootstrap-with-a-prediction-interval",
    "href": "lectures/L26-bootstrap-error.html#bootstrap-with-a-prediction-interval",
    "title": "26  Bootstrap II",
    "section": "",
    "text": "Example 26.1 Let’s go back to The Edit Room cafe queues from last time. The statistic in question was the maximum queue length from 5 visits. The data and our bootstrap samples were these:\n\nqueues &lt;- c(rep(0, 11), rep(1, 5), rep(2, 7), rep(3, 3), 4, 5, 7, 11)\n\nboots &lt;- 1000\nmaxes &lt;- rep(0, boots)\nfor (k in 1:boots) {\n  minisample &lt;- sample(queues, 5, replace = TRUE)\n  maxes[k] &lt;- max(minisample)\n}\n\nWe saw that our estimates for the expectation and the variance of the statistic were the following:\n\nmax_mean &lt;- mean(maxes)\nmax_var &lt;- var(maxes)\nc(max_mean, max_var)\n\n[1] 4.90900 9.93065\n\n\nA lazy 80% prediction interval under a normal assumption would be the following:\n\nmax_mean + qnorm(c(0.1, 0.9)) * sqrt(max_var)\n\n[1] 0.8704551 8.9475449\n\n\nBut that seems a bit silly – our queue length isn’t going to be a real number with seven decimal places. Better is to use the actual quantiles of the statistics we evaluated.\n\nquantile(maxes, c(0.1, 0.9))\n\n10% 90% \n  2  11 \n\n\nI usually get the interval \\([2, 11]\\) from this data. this much better reflects the actual data. It also takes into account the “skew” of the data (lots of values of 0, where there was no queue, but no negative values, of course), to give a tighter lower boundary than the crude and inaccurate normal approximation.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Bootstrap II</span>"
    ]
  },
  {
    "objectID": "lectures/L26-bootstrap-error.html#bootstrap-for-statistical-inference",
    "href": "lectures/L26-bootstrap-error.html#bootstrap-for-statistical-inference",
    "title": "26  Bootstrap II",
    "section": "26.2 Bootstrap for statistical inference",
    "text": "26.2 Bootstrap for statistical inference\nSo we’ve seen two ideas here about what to do when we only have samples from a distribution (for example, some data measurements).\n\nThe first idea was that to estimate something about the distribution, use the plug-in estimator.\nThe second ideas was the to find out about properties of a statistic, use bootstrap sampling.\n\nBut an estimator \\(\\widehat\\theta\\) is just a special type of statistic – one that is hoped to be close to some true parameter \\(\\theta\\). So we can combine these two ideas together. Suppose we have \\(m\\) samples \\(X_1, X_2, \\dots, X_m\\).\n\nTo estimate a parameter \\(\\theta\\), use the plug-in estimator.\nTo find out things about that parameter – for example, its bias, variance, mean-square error, or a confidence interval – use bootstrap sampling.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Bootstrap II</span>"
    ]
  },
  {
    "objectID": "lectures/L26-bootstrap-error.html#bootstrap-estimation-of-bias",
    "href": "lectures/L26-bootstrap-error.html#bootstrap-estimation-of-bias",
    "title": "26  Bootstrap II",
    "section": "26.3 Bootstrap estimation of bias",
    "text": "26.3 Bootstrap estimation of bias\n\nExample 26.2 Let’s talk through an example. Let’s suppose we want to estimate the average number of hours sleep people have per night. That is, we want to know \\(\\theta = \\mathbb EX\\), where \\(X\\) is the random distribution of sleep times for the entire population.\nWe have data on \\(m = 1991\\) people thanks to a survey by the US Centers for Disease Control and Prevention. (Credit: Scott, Data Science in R.) We can read this into R as follows.\n\nsleep &lt;- read.csv(\"https://bookdown.org/jgscott/DSGI/data/NHANES_sleep.csv\")$SleepHrsNight\nm &lt;- length(sleep)\n\nOur estimate for the average sleep time \\(\\theta = \\mathbb EX\\) will be the plug-in estimator \\(\\theta^* \\mathbb E_*X^*\\), which, as we have discussed before, is the sample mean \\(\\theta^* = \\overline{X}\\).\n\nestimate &lt;- mean(sleep)\nestimate\n\n[1] 6.878955\n\n\nThis is a bit under 7 hours.\nBut we should check whether our estimator is likely to be biased or not. How can we estimate the bias (in order to check it’s close to 0)? Well, the bias is, of course, \\[ \\operatorname{bias}(\\theta^*) = \\mathbb E(\\theta^*) - \\theta . \\] How will we estimate each of those terms. Well, we’ve already estimated the second term \\(\\theta\\) by \\(\\theta^* = 6.8790\\). And the first term, we have just discussed how to estimate that using bootstrapping.\nHere our estimator/statistic is using all \\(m\\) pieces of data so, in our previous notation, \\(n\\) and \\(m\\) are the same: we will take \\(n = 1991\\) samples from our \\(m = 1991\\) pieces of data. Note that these are sampling with replacement, so we’re not just getting the same thing every time.\n\nset.seed(3)\nboots &lt;- 1e5\nbootests &lt;- rep(0, boots)\nfor (k in 1:boots) {\n  resample &lt;- sample(sleep, m, replace = TRUE)\n  bootests[k] &lt;- mean(resample)\n}\nest_exp &lt;- mean(bootests)\nest_exp\n\n[1] 6.878924\n\n\nThis typically comes out as very close to the original estimate. With the seed set as 3 for reproducibility, we get 6.8795. So our estimate of the bias is \\[ \\widehat{\\operatorname{bias}}(\\theta^*) = \\widehat{\\mathbb E(\\theta^*)} - \\theta^* = 6.8795 - 6.8790 = 0.0005 .\\]\n\nNote here that we are the bias \\[ \\operatorname{bias}(\\theta^*) = \\mathbb E(\\theta^*) - \\theta  \\] is estimated by \\[ \\widehat{\\operatorname{bias}}(\\theta) = \\widehat{\\mathbb E(\\theta^*)} - \\theta^*\\] In the first, true, expression, \\(\\theta^*\\) is playing the role of the estimator in the first term on the right-hand side; but in the second, estimated, expression, \\(\\theta^*\\) is playing the role of (the estimate of) the true value in the second term on the right-hand side.\nOn this occasion our estimator turned out to be unbiased – or at least extremely close to unbiased.\nBut had we found some bias, it is often preferable to improve our estimator by removing that bias. Our refined, or “debiased”, estimator would be \\[ \\theta^* - \\widehat{\\operatorname{bias}}(\\theta) = \\theta^* - \\Big(\\widehat{\\mathbb E(\\theta^*)} - \\theta^*\\Big) = 2\\theta^* - \\widehat{\\mathbb E(\\theta^*)} . \\]",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Bootstrap II</span>"
    ]
  },
  {
    "objectID": "lectures/L26-bootstrap-error.html#bootstrap-estimation-of-variance-and-intervals",
    "href": "lectures/L26-bootstrap-error.html#bootstrap-estimation-of-variance-and-intervals",
    "title": "26  Bootstrap II",
    "section": "26.4 Bootstrap estimation of variance and intervals",
    "text": "26.4 Bootstrap estimation of variance and intervals\nWhat about the error in our estimator? We measure that through the variance, because the variance equals the mean-square error for an unbiased estimator. (For a biased estimator, the MSE is the bias-square plus the variance – remember s@thm-MSE-bias a long time ago). Again, we can use the bootstrap.\n\nExample 26.3 We continue the sleep example.\n\nvar(bootests)\n\n[1] 0.0008780489\n\n\nThis is 0.09, which is a pretty good estimate for the variance.\nHowever, this sample variance is calculated as \\[ S^2 = \\frac{1}{B-1} \\sum_{k=1}^B \\big(T^*_k - \\overline{T^*}\\big)^2 . \\] But it’s a bit strange to be looking at squared-difference from \\(\\overline T^*\\), a Monte Carlo-like estimate of \\(\\theta\\), when we have the plug-in estimate \\(\\theta^*\\) available, which we would think is better. So preferable to use is \\[ \\frac{1}{B-1} \\sum_{k=1}^B \\big(T^*_k - \\theta^*\\big)^2 . \\] Here, this is the following\n\n(1 / (boots - 1)) * sum((bootests - estimate)^2)\n\n[1] 0.0008780498\n\n\nIn this case, with plenty of samples and well-behaved data, this is identical to 6 decimal places. But on other occasions, with fewer samples and weirder data, it could be an improvement.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Bootstrap II</span>"
    ]
  },
  {
    "objectID": "lectures/L27-bootstrap-ci.html",
    "href": "lectures/L27-bootstrap-ci.html",
    "title": "27  Bootstrap III",
    "section": "",
    "text": "27.1 Bootstrap summary\nA reminder where we have got to with the bootstrap.\nTo estimate a parameter \\(\\theta\\) from random samples \\(X_1, X_2, \\dots, X_m\\), we use the plug-in estimator \\(\\theta^*\\).\nWe can also get bootstrap statistics \\(T^*_k\\) by sampling \\(m\\) of the datapoints with replacement and using those resampled points to re-calculate the estimator.\nThe idea of bootstrapping is that the variability of \\(\\theta^*\\) around \\(\\theta\\) can be approximated by the variability of the \\(T^*_k\\) around \\(\\theta^*\\).\nWe saw that we can estimate the bias by \\[ \\widehat{\\operatorname{bias}}(\\theta^*) = \\overline{T^*} - \\theta^* .\\] If the bias appears to be significant, it can be appropriate to “de-bias” the plug-in estimator by instead using \\[ \\theta^* - \\widehat{\\operatorname{bias}}(\\theta^*) = \\theta^* - \\big(\\overline{T^*} - \\theta^*\\big) = 2\\theta^* -\\overline{T^*} . \\]\nWe saw that we can estimate the variance by \\[ S^2 = \\frac{1}{B-1} \\sum_{k=1}^B \\big(T^*_k - \\overline{T^*}\\big)^2 , \\] although it can be more appropriate to centre the approximation at the plug-in estimator, and use \\[ \\frac{1}{B-1} \\sum_{k=1}^B \\big(T^*_k - \\theta^*\\big)^2  \\] instead.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Bootstrap III</span>"
    ]
  },
  {
    "objectID": "lectures/L27-bootstrap-ci.html#bootstrap-summary",
    "href": "lectures/L27-bootstrap-ci.html#bootstrap-summary",
    "title": "27  Bootstrap III",
    "section": "",
    "text": "The real-world: Treating the samples \\(X_1, X_2, \\dots, X_m\\) as random, there is variability of our estimator \\(\\theta^*\\) around the true value \\(\\theta\\).\nThe bootstrap world: Treating the samples \\(X_1, X_2, \\dots, X_m\\) as fixed, there is variability of the bootstrap statistics \\(T^*_k\\) around the plug-in estimate \\(\\theta^*\\).",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Bootstrap III</span>"
    ]
  },
  {
    "objectID": "lectures/L27-bootstrap-ci.html#bootstrap-confidence-intervals",
    "href": "lectures/L27-bootstrap-ci.html#bootstrap-confidence-intervals",
    "title": "27  Bootstrap III",
    "section": "27.2 Bootstrap confidence intervals",
    "text": "27.2 Bootstrap confidence intervals\nThe one thing left is to look at confidence intervals. We saw that we can create a \\((1-\\alpha)\\)-prediction interval \\((T_\\mathrm{L}, T_\\mathrm{U})\\) for a statistic by taking \\(T_\\mathrm{L}\\) to be the lower \\(\\alpha/2\\)-quantile – let’s call it \\(T^*_\\mathrm{L}\\) – of the \\(T^*_k\\)’s and taking \\(T_\\mathrm{U}\\) to be the upper \\(\\alpha/2\\)-quantile \\(T^*_\\mathrm{U}\\) of the \\(T^*_k\\)’s.\nHowever, it can again be more appropriate to anchor these to the plug-in estimator. We have \\[ \\begin{align}\n1 - \\alpha &= \\mathbb P\\big(T^*_\\mathrm{L} \\leq \\overline{T^*} \\leq T^*_\\mathrm{U} \\big) \\\\\n&= \\mathbb P\\big(T^*_\\mathrm{L} - \\theta^* \\leq \\overline{T^*}-\\theta^* \\leq T^*_\\mathrm{U} -\\theta^*\\big) \\\\\n&= \\mathbb P\\big(\\theta^* - T^*_\\mathrm{L}   \\geq \\theta^* - \\overline{T^*} \\geq  \\theta^* - T^*_\\mathrm{U}\\big) \\\\\n&= \\mathbb P\\big(\\theta^* + \\overline{T^*} - T^*_\\mathrm{L}   \\geq \\theta^*  \\geq  \\theta^* +  \\overline{T^*}- T^*_\\mathrm{U}\\big) \\\\\n&= \\mathbb P\\big(\\theta^* +  \\overline{T^*}- T^*_\\mathrm{U}  \\leq \\theta^*  \\leq \\theta^* + \\overline{T^*} - T^*_\\mathrm{L} \\big) .\n\\end{align} \\] Replacing \\(\\overline T^*\\) by the plug-in estimator \\(\\theta^*\\) therefore suggests we should take the lower limit to be \\(\\theta^* + \\theta^* - T^*_\\mathrm{U} = 2\\theta^* - T^*_\\mathrm{U}\\) and the upper limit to be \\(\\theta^* + \\theta^* - T^*_\\mathrm{L} = 2\\theta^* - T^*_\\mathrm{L}\\).\n\nExample 27.1 We return to the sleep data example of last time. We seek a 95% confidence interval.\n\nsleep &lt;- read.csv(\"https://bookdown.org/jgscott/DSGI/data/NHANES_sleep.csv\")$SleepHrsNight\nm &lt;- length(sleep)\n\nestimate &lt;- mean(sleep)\n\nboots &lt;- 1e5\nbootests &lt;- rep(0, boots)\nfor (k in 1:boots) {\n  resample &lt;- sample(sleep, m, replace = TRUE)\n  bootests[k] &lt;- mean(resample)\n}\n\nTlower &lt;- quantile(bootests, 0.025)\nTupper &lt;- quantile(bootests, 0.975)\n\nc(Tlower, Tupper)\n\n    2.5%    97.5% \n6.821195 6.936715 \n\n2 * estimate - c(Tupper, Tlower)\n\n   97.5%     2.5% \n6.821195 6.936715 \n\n\nAgain, our data here was very well behaved, so anchoring to the plug-in estimator, using the second method, did not make much difference. But this can be important with a biased estimator or heavily skewed data.\n\nResearchers have looked into lots of other ways to build bootstrap confidence intervals, but we won’t go into those here.",
    "crumbs": [
      "Bootstrap",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Bootstrap III</span>"
    ]
  },
  {
    "objectID": "problems/P5.html",
    "href": "problems/P5.html",
    "title": "Problem Sheet 5",
    "section": "",
    "text": "This is Problem Sheet 5, which covers material from Lectures 22 to 27. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Thursday 11 December. If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Mondays at 1500.\nThis problem sheet is to help you practice material from the module. It is not for assessment and will not be marked.\nFull solutions should be released on Friday 12 December.\nRemember that Thursday 12 December at 1400 is also the deadline for the module coursework\n\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.     The Gamma distribution \\(X \\sim \\Gamma(m, \\lambda)\\) has PDF \\[ f(x) = \\frac{\\lambda^m}{(m-1)!} \\,x^{m-1}\\,\\mathrm{e}^{-\\lambda x} . \\] [Correction: There was a typo in this PDF earlier.] (This PDF can be evaluated with the dgamma() function in R.) The time between eruptions of a volcano, measured in years, is modelled as \\(X \\sim \\Gamma(5, \\theta)\\), IID over eruptions, where \\(\\theta\\) is unknown. A scientist beliefs about \\(\\theta\\) are represented in a prior distribution \\(\\theta \\sim \\operatorname{Exp}(6)\\).\nThe following times between eruptions are available in the historical record:\n       56.55,  2.57, 29.97, 10.27,  4.32, 17.91, 51.98,  7.06, 11.40, 54.80\n            Sample from the posterior distribution for \\(\\theta\\), using the random walk Metropolis algorithm. Explain how you chose the stepsize for your algorithm and how you checked it was a good choice.\nDraw a histogram of the posterior distribution, and comment on how it differs from the prior.\n\nSolution. The posterior distribution is \\[ \\pi(\\theta \\mid \\mathbf x) \\propto 6\\mathrm{e}^{-6\\theta} \\times \\prod_{i=1}^{10} x_i^{4} \\, \\mathrm{e}^{-\\theta x_i} . \\]\nThe code for the Metropolis random walk algorithm will be the following.\n\nx &lt;- c(56.55,  2.57, 29.97, 10.27,  4.32,\n       17.91, 51.98,  7.06, 11.40, 54.80)\n\nprior &lt;- function(theta) dexp(theta, 6)\nlike  &lt;- function(theta) prod(dgamma(x, 5, theta))\npost  &lt;- function(theta) prior(theta) * like(theta)\n\naccept &lt;- function(curr, prop) post(prop) / post(curr)\ninitial &lt;- 1\nn &lt;- 1e5\nsigma &lt;- 0.25\n\nMRW &lt;- rep(0, n)\nMRW[1] &lt;- initial\nfor (i in 1:(n - 1)) {\n  prop &lt;- MRW[i] + rnorm(1, 0, sigma)\n  if (prop &lt; 0)                             MRW[i + 1] &lt;- MRW[i]\n  else if (runif(1) &lt; accept(MRW[i], prop)) MRW[i + 1] &lt;- prop\n  else                                      MRW[i + 1] &lt;- MRW[i]\n}\n\nhist(MRW[-(1:99)], probability = TRUE, xlim = c(0, 0.5))\ncurve(prior, add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nA little experimentation suggested \\(\\sigma = 0.2\\) worked about right – \\(\\sigma = 0.5\\) rejected too many changes. With my initial start point of 1, I found a short burn-in period was necessary, so my histogram starts with the 100th step of the random walk. (I could have just started from 0.2 instead, I suppose.)\nWe see that while the prior though any number between 0 and \\(\\tfrac12\\), maybe even more, was plausible for \\(\\theta\\), we can see the prior has become concentrated in the interval \\([0.11, 0.32]\\) or so, with values around 0.2 the most common. The posterior variance is much smaller than the prior, so we have gained certainty from the results.\n\n\n\n2.     [2019 exam, Question 4]\n\n(a)   Introducing any notation you use, state the Metropolis–Hastings algorithm for discrete state space, and explain the purpose of this algorithm.\n\nSolution. The Metropolis–Hastings algorithm is a method to sample from a distribution \\(\\pi\\) on a discrete space \\(\\mathcal S\\) by setting up a Markov chain \\((X_i)\\) on \\(\\mathcal S\\) with stationary distribution \\(\\pi\\).\nSuppose the Markov chain is at \\(X_i = x\\). The Metropolis–Hastings algorithm proposes a move to \\(y\\) with probability \\(r(x, y)\\), where \\(\\mathsf R = (r(x, y))\\) is the transition matrix for an irreducible Markov chain on \\(\\mathcal S\\). This move is accepted with probability \\[ \\alpha(x, y) = \\min \\left\\{ \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)},\\, 1\\right\\} , \\] meaning that \\(X_{i+1} = y\\), or rejected otherwise, meaning that \\(X_{i+1} = X_i = x\\).\nAfter being run for a large number of steps (possibly with a burn-in period), the values \\(X_i\\) should be approximately distributed like \\(\\pi\\).\n\n\n\n(b)   Using the Metropolis–Hastings algorithm, find a Markov chain with state space \\(\\mathbb N = \\{1, 2, 3, \\dots\\}\\) that has stationary distribution \\(\\mathbb P(X_n = x) = 2^{-x}\\) for \\(x \\in \\mathbb N\\).\n\nSolution. Although this isn’t the only way, I will use \\(\\mathsf R\\) to be the transition matrix of the simple symmetric random walk on \\(\\mathbf Z\\). Because this is symmetric, the acceptance probabilities are \\[ \\begin{align}\n\\alpha(x, x+1) &= \\min \\left\\{ \\frac{\\pi(x+1)}{\\pi(x)},\\, 1 \\right\\} = \\min\\big\\{\\tfrac12,\\, 1\\big\\} = \\tfrac 12 \\\\\n\\alpha(x, x-1) &= \\min \\left\\{ \\frac{\\pi(x-1)}{\\pi(x)},\\, 1 \\right\\} = \\min\\{2,\\, 1\\} = 1,\n\\end{align} \\] except that \\(\\alpha(1, 0) = 1\\).\nI’ll choose the starting point \\(X = 1\\). From here, the Markov chain repeatedly proposes a move up 1 with probability \\(\\tfrac12\\), which is accepted with probability \\(\\tfrac12\\), or a move down 1 with probability \\(\\tfrac12\\), which is always accepted – except for a move from 1 down to 0 which is always rejected.\n\n\n\n(c)   In a Bayesian setting, assume we have observations \\(Z_1, X_2, \\dots, Z_n\\) from an \\(\\operatorname{N}(0, \\sigma^2)\\) distribution, where the prior distribution for \\(\\sigma^2\\) is \\(\\operatorname{Exp}(1)\\). Using the Metropolis–Hastings algorithm, describe in detail a method for sampling from the posterior distribution of \\(\\sigma^2\\).\n\nSolution The posterior distribution is given by \\[ \\begin{align}\n\\pi(\\sigma^2 \\mid \\mathbf Z) &\\propto \\pi(\\sigma^2) \\prod_{i=1}^n f(z_i \\mid \\sigma^2) \\\\\n  &= \\mathrm{e}^{-\\sigma^2} \\prod_{i=1}^n \\frac{1}{\\sqrt{\\sigma^2}} \\,\\exp\\left(- \\frac{Z_i^2}{2\\sigma^2} \\right) \\\\\n  &=  \\mathrm{e}^{-\\sigma^2} \\,\\sigma^{-n/2} \\,\\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n Z_i^2 \\right) .\n\\end{align} \\]\nI will choose as the Markov chain a symmetric Gaussian random walk on \\(\\mathbb R\\), meaning that the Markov chain is \\(X_{i+1} = X_i + \\mathrm{N}(0, s^2)\\). (The special case is known as the random walk Metropolis algorithm in continuous space.) Because this is symmetric, the acceptance probability is \\[ \\alpha(x, y) = \\min \\left\\{ \\frac{ \\pi(x \\mid \\mathbf Z)}{\\pi(y \\mid \\mathbf Z)},\\,1\\right\\} , \\] for \\(y \\geq 0\\), where the constant of proportionality in the \\(\\pi\\)s cancel out, of \\(\\alpha(x, y) = 0\\) for \\(y &lt; 0\\).\nI would suggest starting from \\(X_1\\) between \\(1\\) (the prior variance) and the sample variance of the \\(Z_i\\) (the posterior variance in the large sample limit \\(n \\to \\infty\\)). I would suggest choosing the step size \\(s\\) by experiment, with smaller \\(s\\) for large \\(n\\), and vice versa.\n\n\n\n(d)   Let the set \\(A \\subset \\mathbb R^2\\) be given by \\[\nA = \\big([0,3]\\times[0,1]\\big) \\cup \\big([0,1]\\times [4,5]\\big)\n\\cup \\big([4,5]\\times[0,1]\\big) \\cup \\big([4,5] \\times [3,5]\\big) .\n\\] Consider the Metropolis–Hastings algorithm on \\(\\mathbb R^2\\) with target density \\[ \\pi(\\mathbf x) = \\begin{cases} \\frac17 & \\text{if }\\mathbf x \\in A \\\\\n0 & \\text{otherwise} \\end{cases} \\] for all \\(x \\in \\mathbb R^2\\), and where the proposals \\(\\mathbf Y_{i+1}\\) are chosen uniformly distributed on a disk of radius \\(r\\) around the previous state – that is, \\[ Y_{j+1} \\sim \\operatorname{U} \\Big(\\big\\{ \\mathbf y \\in \\mathbb R^2 : |\\mathbf y - \\mathbf X_i | \\leq r \\big\\}\\Big) . \\] Assume the algorithm starts at \\(\\mathbf X_1 = (0.5, 0.5)\\).\ni.      For every \\(r &gt; 0\\), determine the stationary distribution of the resulting Markov chain.\nii.    For what values of \\(r\\) does the algorithm work correctly?\n\nSolution. To see what’s going on where, we need to draw a picture of the set \\(A\\).\nThe question is: for what \\(r\\) will it be possible to jump between the different blocks. We start from \\((0.5, 0.5)\\) which is in the bottom-left block. To jump from bottom-left to bottom-right, we need \\(r &gt; 1\\). To jump from bottom-right to top-right, we need \\(r &gt; 2\\). To jump from bottom-left to top-left, we need \\(r &gt; 3\\).\n(i) The distribution is uniform, so as long as the Markov chain can reach an area, the stationary distribution will be uniform there. So the stationary distribution is uniform on the following sets:\n\n\\(0 &lt; r \\leq 1\\): \\(\\big([0,3]\\times[0,1]\\big)\\)\n\\(1 &lt; r \\leq 2\\): \\(\\big([0,3]\\times[0,1]\\big) \\cup \\big([4,5]\\times[0,1]\\big)\\)\n\\(2 &lt; r \\leq 3\\): \\(\\big([0,3]\\times[0,1]\\big) \\cup \\big([4,5]\\times[0,1]\\big) \\cup \\big([4,5] \\times [3,5]\\big)\\)\n\\(r &gt; 3\\): \\(A\\)\n\n(If \\(r\\) is only just above the boundary, convergence to the stationary distribution may be extremely slow.)\n(ii) The stationary distribution is the desired distribution for \\(r &gt; 3\\), which is the values for which the algorithm technically “works”, as \\(n \\to \\infty\\). To work “well”, \\(r\\) will need to be comfortably bigger than 3 (so that movement to and from the top-left is reasonably common) while not being so huge that most steps are to ridiculously far away.\n\n\n\n\n3.     [2017 exam, Question 5]\n\n(a)   Give the definition of a Markov chain with state space \\(\\mathcal S\\).\nLet \\((Z_i)\\) be a sequence of IID random variables whose distribution is known. Consider the stochastic processes \\((U_i)\\), \\((V_i)\\), \\((W_i)\\) defined by \\[ \\begin{align}\nU_i &= Z_1 + Z_2 + \\cdots + Z_i \\\\\nV_i &= \\frac{1}{i} (Z_1 + Z_2 + \\cdots + Z_i) \\\\\nW_i &= \\max \\{Z_j : 1 \\leq j \\leq i-1\\} + Z_i\n\\end{align} \\] for \\(i = 1, 2, \\dots\\). Which of these processes are Markov chains? Justify your answers.\n\n\n(b)   Defining any notation you use, state the random walk Metropolis algorithm. What is the purpose of this algorithm?\n\n\n(c)   Use the Metropolis–Hastings algorithm to define a Markov chain \\((X_n)\\) on \\(\\mathcal S = \\{0, 1, 2, \\dots \\}\\) whose stationary distribution is the Poisson distribution \\(\\pi(x) = \\mathrm{e}^{-\\lambda} \\lambda^x/x!\\).\n\n\n(d)   Let \\((X_i)\\) be a Markov chain with values in \\(\\mathbb R\\) and stationary distribution \\(\\pi\\). Consider the estimator \\[\\widehat{\\theta}_n^{\\mathrm{MCMC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i)  \\] for \\(\\theta = \\operatorname{\\mathbb E}(\\phi(X))\\) for \\(X\\) having PDF \\(\\pi\\). Assuming \\(X_1\\) has PDF \\(\\pi\\), derive the result \\[ \\operatorname{Var} \\big(\\widehat{\\theta}_n^{\\mathrm{MCMC}}\\big) \\approx \\frac{\\operatorname{Var}(\\phi(X))}{n} \\left(1 + 2 \\sum_{i=2}^\\infty \\operatorname{Corr}\\big(\\phi(X_1),\\phi(X_i)\\big) \\right) . \\]\n\n\n4.     Let \\(\\mathbf X = (X_1, \\dots, X_n)\\) be IID samples from a random variable \\(X\\) with cumulative distribution function \\(F\\). Let \\(F^*\\) be the empirical cumulative distribution function of the samples \\(\\mathbf X\\).\n\n(a)   For fixed \\(x\\), show that \\(\\operatorname{\\mathbb E}F^*(x) = F(x)\\).\n\n\n(a) For fixed \\(x\\), show that \\(\\operatorname{\\mathbb E}F^*(x) = F(x)\\).\n\nSolution. We have \\[ F^*(x) = \\frac{1}{n} \\sum_{i=1}^n\\mathbb{I}_{(-\\infty, x]}(X_i) . \\] So \\[ \\begin{multline}\n\\operatorname{\\mathbb E}F^*(x) = \\mathbb E \\left( \\frac{1}{n} \\sum_{i=1}^n\\mathbb{I}_{(-\\infty, x]}(X_i) \\right) = \\frac{1}{n} \\sum_{i=1}^n \\operatorname{\\mathbb E} \\mathbb{I}_{(-\\infty, x]}(X_i) \\\\ = \\frac{1}{n}\\,\\sum_{i=1}^n \\mathbb P\\big(X_i \\in (\\infty, x]\\big) = \\frac{1}{n}\\,n\\,\\mathbb P\\big(X_i \\in (\\infty, x]\\big) = F(x)\n\\end{multline} \\]\n\n\n\n(b) For fixed \\(x\\), show that \\(\\operatorname{Var}\\big(F^*(x)\\big) = \\displaystyle\\frac{1}{n}F(x)\\big(1 - F(x)\\big)\\).\n\nSolution. With the same notation as part (a), \\[ \\begin{multline}\n\\operatorname{Var}\\big(F^*(x)\\big) = \\operatorname{Var} \\left( \\frac{1}{n} \\sum_{i=1}^n\\mathbb{I}_{(-\\infty, x]}(X_i) \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}\\big(\\mathbb{I}_{(-\\infty, x]}(X_i)\\big) \\\\ = \\frac{1}{n^2}\\,\\sum_{i=1}^n F(x) \\big(1 - F(x)\\big) = \\frac{1}{n^2}\\,n\\,F(x) \\big(1 - F(x)\\big) = \\frac{1}{n} F(x) \\big(1 - F(x)\\big)\n\\end{multline} \\]\n\n\n\n(c) For fixed \\(x\\) and \\(y\\) with \\(y &gt; x\\), find \\(\\operatorname{Cov}\\big(F^*(y), F^*(x)\\big)\\). (c) For fixed \\(x\\) and \\(y\\) with \\(x \\leq y\\), find \\(\\operatorname{Cov}\\big(F^*(y), F^*(x)\\big)\\).\n\nSolution. Using the same notation again, \\[ \\begin{align}\n\\operatorname{Cov}\\big(F(x), F(y)\\big)\n&= \\operatorname{Cov} \\left( \\frac1n\\sum_{i=1}^n \\mathbb{I}_{(-\\infty, x]}(X_i) ,  \\frac1n\\sum_{i=1}^n \\mathbb{I}_{(-\\infty, y]}(X_i) \\right) \\\\\n&= \\frac{1}{n^2} \\left( \\sum_{i=1}^n \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_i)\\big) + 2 \\sum_{i&lt;j} \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_j)\\big) \\right) \\\\\n&= \\frac{1}{n^2} \\left(n \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_i)\\big) + 0\\right) \\\\\n&= \\frac{1}{n} \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_i)\\big)\n\\end{align} .\\]\nSo we need to find that covariance. In particular, we need \\[\\operatorname{\\mathbb E} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i) \\mathbb{I}_{(-\\infty, y]}(X_i)\\big) = F(x),\\] since the produce of indicator function is 1 if and only if both indicators are 1, which is if \\(X_i \\leq x\\) and \\(X_i \\leq y\\). Since \\(x\\) is the smaller, this is \\(F(x)\\).\nHence we have \\[ \\operatorname{Cov}\\big(F^*(y), F^*(x)\\big) = \\frac{1}{n} \\big(F(x) - F(x)F(y)\\big) = \\frac{1}{n}\\,F(x) \\big(1 - F(y)\\big) . \\]\n\n\n\n@@ -201,13 +235,52 @@ $$ (_n^{}) \n\n(a) Explain why the plug-in estimator is \\(\\theta^* = \\max \\{X_j : j = 1, \\dots, 24\\}\\), and find the \\(\\theta^*\\) for this data.\n\nSolution. This is basically just the definition of the plug-in estimator. (Not sure why I set that as part of the question…) The maximum value of \\(X_i\\) here is\n\nsamples &lt;- c(12.40,  2.99, 14.79,  3.59, 24.92,  4.11,\n             22.68, 28.30,  0.90,  8.84, 24.17, 31.31,\n             12.97, 12.43, 20.34, 14.75, 20.61, 10.93,\n             17.59, 30.88, 28.13,  0.83, 17.43, 20.78)\nplugin &lt;- max(samples)\nplugin\n\n[1] 31.31\n\n\nSo \\(\\theta^* = 31.31\\).\n\n\n\n(b) Use the bootstrap to estimate the bias of the plug-in estimator.\n\nSolution. The idea is to pick 24 samples with replacement and look at how the maximum of those differs from the overall maximum 31.31.\n\nboots &lt;- 1e5\nm &lt;- length(samples)\nbootests &lt;- rep(0, boots)\nfor (k in 1:boots) {\n  resample &lt;- sample(samples, m, replace = TRUE)\n  bootests[k] &lt;- max(resample)\n}\nbias &lt;- mean(bootests) - plugin\nbias\n\n[1] -0.5307819\n\n\n\n\n\n(c) Improve the plug-in estimator by using your estimation of the bias to (approximately) “de-bias” the plug-in estimate.\n\nSolution. We want to subtract the bias from the plugin estimator to debias it. In maths, this is \\(\\theta^* - (\\overline{T^*} - \\theta^*) = 2\\theta^* - \\overline{T^*}\\). In code, this is\n\nplugin - bias\n\n[1] 31.84078",
    "crumbs": [
      "Bootstrap",
      "Problem Sheet 5"
    ]
  },
  {
    "objectID": "coursework/coursework.html",
    "href": "coursework/coursework.html",
    "title": "Computational coursework",
    "section": "",
    "text": "About the coursework\nThere is one piece of computational coursework for MATH5835M Statistical Computing. This coursework is worth 20% of the module mark.\nThis page contains all the administrative and organisational information about the coursework.\nA coursework sheet contains the tasks you must carry out. The coursework sheet is available in here in HTML format or here in R Markdown format. The HTML page can be opened in a standard web browser. The R Markdown version (which I personally prefer) should be downloaded then opened in RStudio; I recommend using the “Visual” editor – see the button in the top left of the editing window.\nThe are two deadlines for this work.\nThe Gradescope submission for the final report will open after 1400 on Friday 6 December (to avoid drafts for optional feedback getting mixed up with final reports). The Gradescope submission for optional feedback on draft work will be open from the computer practical sessions in week 9.\nYour report must be entirely your own work. You must not work together with others, and you must not use generative AI – the coursework is RED for use of AI on the University’s traffic light system, meaning the use of generative AI is forbidden. You must comply with the University’s rules on academic integrity.\nIf you encounter extraordinary unforeseeable personal circumstances that make it impossible for you to submit your coursework on time, you can apply for an deadline extension through the usual mitigating circumstances procedure. (Do not contact me about this – I cannot unilaterally offer deadline extensions.)",
    "crumbs": [
      "Computational coursework"
    ]
  },
  {
    "objectID": "coursework/coursework.html#about-the-coursework",
    "href": "coursework/coursework.html#about-the-coursework",
    "title": "Computational coursework",
    "section": "",
    "text": "There is the main submission deadline which is the penultimate day of term, Thursday 12 December at 1400. You must submit your work. Work that is submitted after this deadline will receive a penalty of 5% for every day or part-day late. Work that is more than 14 days late will not be marked and will receive 0. Work will be submitted electronically through Gradescope via the Minerva page for the module.\nOptionally, you may also wish to get some brief informal feedback on your draft work before the main deadline. If you wish to get feedback on your draft work, you should submit that draft by Friday 6 December at 1400. I will return a small amount of general feedback about your work by Monday 9 December. I will not mark your draft, and the feedback will be a brief sentence or too that may be of a little help when completing the final version of your work. The quality of your draft submission will have no effect (either positive or negative) on the mark for your main submission, and there is no penalty if you choose not to take advantage of this offer. No extensions can be offered on the deadline for this informal feedback.",
    "crumbs": [
      "Computational coursework"
    ]
  },
  {
    "objectID": "coursework/coursework.html#about-your-report",
    "href": "coursework/coursework.html#about-your-report",
    "title": "Computational coursework",
    "section": "About your report",
    "text": "About your report\nYour task is to write a short report in response to the tasks on the coursework sheet. Your report should include all the important R code you use and any important plots you draw. If I can’t work out what R code you ran to get your results, I cannot award you marks for those results. Only include the code and figures relevant to your final and best solution – I don’t need or want to see earlier attempts or errors along the way, unless they genuinely help explain your final choices.\nAll computational work must be done in R – no marks will be given for code written in Python or any other language. It is strongly recommended that you draw figures in R (with the possible exception of rough illustrative sketches, if you find them useful), but I do not insist on this.\nI call your output a “report” because your work should be explained in detail, in full English-language sentences, with your solutions discussed and justified. The code and figures you include should be an integral part of your report, not just copy-pasted in at random, and should be fully explained within the text. However, other parts of a more formal “report”, such as an introduction, literature review, conclusion, references, etc, are not required here.\nThere is no page limit for the report, although I expect that good reports will typically be around 5–7 pages. If your report is 4 pages or less, make sure you have completed all the tasks and fully explained your work. If your report has 8 pages or more, make sure you are not wandering from the point or including too many unnecessary figures.\nYour report should be prepared electronically, but can be submitted in any sensible format – I recommend PDF (whether produced by LaTeX, R Markdown, Microsoft Word, or any other way), but HTML or Word document are fine too.",
    "crumbs": [
      "Computational coursework"
    ]
  },
  {
    "objectID": "coursework/coursework.html#computer-practical-and-other-help",
    "href": "coursework/coursework.html#computer-practical-and-other-help",
    "title": "Computational coursework",
    "section": "Computer practical and other help",
    "text": "Computer practical and other help\nThere will be a computer practical in Week 9 where I will introduce the practical coursework in more detail. This is a good place to ask any questions you have about the coursework.\nPlaces where you can get help with the coursework include:\n\nIn the computer practical session.\nIn my office hours – Mondays 1500–1600 in my office, 9.10n in the “Maths Research Deck” area on the 9th floor of the EC Stoner building at staircase 1. (It is unlikely I will have time to discuss computational work at the beginning or end of a lecture, although if your question is extremely short you can try.)\nBy submitting your draft work for optional feedback, as discussed above.",
    "crumbs": [
      "Computational coursework"
    ]
  },
  {
    "objectID": "coursework/coursework.html#r-and-rstudio-on-university-computers",
    "href": "coursework/coursework.html#r-and-rstudio-on-university-computers",
    "title": "Computational coursework",
    "section": "R and RStudio on University computers",
    "text": "R and RStudio on University computers\nA quick reminder on how to access R and RStudio on University computers.\n\nOpen the AppsAnywhere portal This should be a link on the desktop. If invited to run any software, accept.\nLoad the language R Search on AppsAnywhere for “R for Windows” (or similar) and launch it. This will (silently) load the language R. It will also open an inferior RStudio-like program called “RGui” – you can close it.\n\n3.Launch RStudio Search on AppsAnywhere for “RStudio” (or similar) and launch it.\nYou can also use the Posit Cloud to access R and RStudio online.",
    "crumbs": [
      "Computational coursework"
    ]
  },
  {
    "objectID": "problems/solutions.html",
    "href": "problems/solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Problem Sheet 1",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "problems/solutions.html#P1-sols",
    "href": "problems/solutions.html#P1-sols",
    "title": "Solutions",
    "section": "",
    "text": "\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.      Let \\(X\\) be uniform on \\([-1,2]\\).\n\n(a)   By hand, calculate the exact value of \\(\\Ex X^4\\).\n\nSolution.\n\\[\\int_{-1}^2 x^4\\,\\frac{1}{2-(-1)}\\,\\mathrm{d}x = \\tfrac13 \\Big[\\tfrac15x^5\\Big]_{-1}^2 = \\tfrac13\\Big(\\tfrac{32}{5}-\\big(-\\tfrac15\\big)\\Big) = \\tfrac{33}{15} = \\tfrac{11}{5} = 2.2\\]\n\n\n\n(b)   Using R, calculate a Monte Carlo estimate for \\(\\Ex X^4\\).\n\nSolution. I used the R code\n\nn &lt;- 1e6\nsamples &lt;- runif(n, -1, 2)\nmean(samples^4)\n\n[1] 2.202239\n\n\n\n\n\n\n2.      Let \\(X\\) and \\(Y\\) both be standard normal distributions. Compute a Monte Carlo estimate of \\(\\Exg \\max\\{X,Y\\}\\). (You may wish to investigate R’s pmax() function.)\n\nSolution. By looking at ?pmax (or maybe searching on Google) I discovered that pmax() gives the “parallel maxima” of two (or more vectors). That is the first element is the maximum of all the first elements of the vectors; the second element is the maximum of all the second elements of the vectors; and so on.\nSo I used the R code\n\nn &lt;- 1e6\nxsamples &lt;- rnorm(n)\nysamples &lt;- rnorm(n)\nmean(pmax(xsamples, ysamples))\n\n[1] 0.5648906\n\n\n\n\n\n3.      You are trapped alone on an island. All you have with you is a tin can (radius \\(r\\)) and a cardboard box (side lengths \\(2r \\times 2r\\)) that it fits snugly inside. You put the can inside the box [left picture].\nWhen it starts raining, each raindrop that falls in the cardboard box might fall into the tin can [middle picture], or might fall into the corners of the box outside the can [right picture].\n\n\n(a)   Using R, simulate rainfall into the box. You may take units such that \\(r = 1\\). Estimate the probability \\(\\theta\\) that a uniformly-random raindrop that falls in the cardboard box falls into the tin can also.\n\nSolution. I set things up so that the box is \\([-1, 1]^2\\), centered at the origin. This means that the inside of the can is the set of points is those \\((x,y)\\) such that \\(x^2 + y^2 \\leq 1\\).\n\nn &lt;- 1e6\nrain_x &lt;- runif(n, -1, 1)\nrain_y &lt;- runif(n, -1, 1)\nin_box &lt;- function(x, y) x^2 + y^2 &lt;= 1\nmean(in_box(rain_x, rain_y))\n\n[1] 0.784906\n\n\n\n\n\n(b)   Calculate exactly the probability \\(\\theta\\).\n\nSolution. The area of the box is \\(2r \\times 2r = 4r^2\\). The area of the can is \\(\\pi r^2\\). So the probability a raindrop landing in the box lands in the can is \\[ \\frac{\\text{area of can}}{\\text{area of box}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\approx 0.785. \\]\n\n\n\n(c)   You realise that, despite being trapped all alone on the island, you now have a way of approximating the value of \\(\\pi\\). If you want to calculate \\(\\pi\\) to 6 decimal places, roughly how many raindrops do you need to fall into the box?\n\nSolution. The phrase “to 6 decimal places” isn’t a precise mathematical one. I’m going to interpret this as getting the root-mean-square error below \\(10^{-6}\\). If you interpret it slightly differently that’s fine – for example, getting the width of a 95% confidence interval below \\(10^{-6}\\) could be another, slightly stricter, criterion.\nOne could work this out by hand. Since the variance of a Bernoulli random variable is \\(p(1-p)\\), the mean-square error of our estimator is \\[ \\frac{\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})}{n} \\approx \\frac{0.169}{n} . \\] So we need \\[n = \\frac{0.169}{(10^{-6})^2} \\approx 169 \\text{ billion} . \\]\nThat said, if we are trapped on our desert island, maybe we don’t know what \\(\\frac{\\pi}{4}(1 - \\frac{\\pi}{4})\\) is. In that case we could do this using the can and the box. Our estimate of the variance is\n\nvar_est &lt;- var(in_box(rain_x, rain_y))\nvar_est / (1e-6)^2\n\n[1] 168828739992\n\n\nWe will probably spend a long time waiting for that much rain!\n\n\n\n\n4.      Let \\(h(x) = 1/(x + 0.1)\\). We wish to estimate \\(\\int_0^5 h(x) \\, \\mathrm{d}x\\) using a Monte Carlo method.\n\n(a)   Estimate the integral using \\(X\\) uniform on \\([0,5]\\).\n\nSolution.\n\nn &lt;- 1e6\nintegrand &lt;- function(x) 1 / (x + 0.1)\nsamples1 &lt;- runif(n, 0, 5)\nmean(5 * integrand(samples1))\n\n[1] 3.93898\n\n\n(In fact, the true answer is \\(\\log(5.1) - \\log(0.1) = 3.932\\), so it looks like this is working correctly.)\n\n\n\n(b)   Can you come up with a choice of \\(X\\) that improves on the estimate from (a)?\n\nSolution. Let’s look at a graph of the integrand \\(h\\).\n\n\nCode for drawing this graph\ncurve(\n  integrand, n = 1001, from = 0, to = 5,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(0,5)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nWe see that we get a much bigger contribution to the integral from values near 0. So a random variable that picks values nearer to 0 more often might have a chance of giving a more accurate result.\n\nI decided to try an exponential distribution with rate 1, which should sample the smaller values of \\(x\\) more often.\n\npdf &lt;- function(x) dexp(x, 1)\nphi &lt;- function(x) (integrand(x) / pdf(x)) * (x &lt;= 5)\n\nsamples2 &lt;- rexp(n, 1)\nmean(phi(samples2))\n\n[1] 3.93244\n\n\n(I had to include x &lt;= 5 in the expression for \\(\\phi\\), because my exponential distribution will sometimes take samples above 5, but they should count as 0 in an estimate for the integral between 0 and 5.)\nTo see whether or not this was an improvement, I estimated the mean-square error.\n\nvar(5 * integrand(samples1)) / n\n\n[1] 3.387878e-05\n\nvar(phi(samples2)) / n\n\n[1] 6.072919e-06\n\n\nI found that I had reduced the mean-square error by roughly a factor of 5. \n\n\n\n\n5.      When calculating a Monte Carlo estimate \\(\\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\\), one might wish to first generate the \\(n\\) samples \\((x_1, x_2, \\dots, x_n)\\) and store them, and only then, after all samples are generated, finally calculate the estimate. However, when \\(n\\) is extremely large, storing all \\(n\\) samples uses up a lot of space in a computer’s memory. Describe (in words, in R code, or in a mixture of the two) how the Monte Carlo estimate could be produced using much less memory.\n\nSolution. The idea is to keep a “running total” of the \\(\\phi(x_i)\\)s. Then we only have to store that running total, not all the samples. Once this has been done \\(n\\) times, then divide by \\(n\\) to get the estimate.\nIn R code, this might be something like\n\nn &lt;- 1e6\n\ntotal &lt;- 0\nfor (i in 1:n) {\n  sample &lt;- # sampling code for 1 sample\n  total &lt;- total + phi(sample)\n}\n\nMCest &lt;- total / n\n\n\n\n\n6.      Show that the indicator functions \\(\\mathbb I_A(X)\\) and \\(\\mathbb I_B(X)\\) have correlation 0 if and only if the events \\(\\{X \\in A\\}\\) and \\(\\{X \\in B\\}\\) are independent.\n\nSolution. Recall that two random variables \\(U\\), \\(V\\) have correlation 0 if and only if their covariance \\(\\Cov(U,V) = \\Ex UV - (\\Ex U)(\\Ex V)\\) is 0 too.\nWe know that \\(\\Exg\\Ind_A(X) = \\mathbb P(X \\in A)\\) and \\(\\Exg \\Ind_B(Y) = \\mathbb P(X \\in B)\\). What about \\(\\Exg \\Ind_A(X) \\Ind_B(X)\\)? Well, \\(\\Ind_A(x) \\Ind_B(x)\\) is 1 if and only if both indicator functions equal 1, which is if and only if both \\(x \\in A\\) and \\(x \\in B\\). So \\(\\Exg \\Ind_A(X) \\Ind_B(X) = \\mathbb P(X \\in A \\text{ and } X \\in B)\\).\nSo the covariance is \\[ \\Cov \\big(\\Ind_A(X), \\Ind_B(X) \\big) = \\mathbb P(X \\in A \\text{ and } X \\in B) - \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B) . \\] If this is 0, then \\(\\mathbb P(X \\in A \\text{ and } X \\in B) = \\mathbb P(X \\in A)\\, \\mathbb P(X \\in B)\\), which is precisely the definition of those two events being independent.\n\n\n\n7.      Let \\(X\\) be an exponential distribution with rate 1.\n\n(a)   Estimate \\(\\mathbb EX^{2.1}\\) using the standard Monte Carlo method.\n\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 1)\nmean(samples^2.1)\n\n[1] 2.19999\n\n\n\n\n\n(b)   Estimate \\(\\mathbb EX^{2.1}\\) using \\(X^2\\) as a control variate. (You may recall that if \\(Y\\) is exponential with rate \\(\\lambda\\) then \\(\\mathbb EY^2 = 2/\\lambda^2\\).)\n\nSolution. We have \\(\\Ex X^2 = 2\\). So, re-using the same sample as before (you don’t have to do this – you could take new samples), our R code is as follows.\n\nmean(samples^2.1 - samples^2) + 2\n\n[1] 2.197787\n\n\n\n\n\n(c)   Which method is better?\n\nSolution. The better answer is the one with the smaller mean-square error.\nFor the basic method,\n\nvar(samples^2.1) / n\n\n[1] 2.775811e-05\n\n\nFor the control variate method,\n\nvar(samples^2.1 - samples^2) / n\n\n[1] 6.743602e-07\n\n\nSo the control variates method is much, much better.\n\n\n\n\n8.      Let \\(Z\\) be a standard normal distribution. A statistician has been investigating Monte Carlo estimates of \\(\\mathbb EZ^k\\) for different positive integers values of \\(k\\). Her colleague suggests using \\(Z' = -Z\\) as an antithetic variable. Without running any R code, explain whether or not this is a good idea (a) when \\(k\\) is even; (b) when \\(k\\) is odd.\n\nSolution.\n(a) When \\(k\\) is even, we have \\(Z^k = (-Z)^k\\). So the antithetic variables method just repeats each sample twice. This is obviously no benefit at all, and just wastes time. Indeed, we have perfect positive correlation \\(\\rho = +1\\), which is the “worst-case scenario”.\n(b) When \\(k\\) is odd, we have \\(Z^k = -(-Z)^k\\). In this case we know that \\(\\Ex Z^k = 0\\), because the results for positive \\(z\\) exactly balance out those for negative \\(z\\), so no Monte Carlo sampling is necessary. If our statistician has somehow forgotten that, though, she will get a pleasant surprise! After just two samples, she will get the estimate \\[\\frac{1}{2} \\big(Z_1^k + (-Z_1)^k \\big) = \\frac{1}{2} (Z_1^k - Z_1^k) = 0 ,\\] Thereby getting the result exactly right. Indeed, we have perfect negative correlation \\(\\rho = -1\\), which is the “best-case scenario”.",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "problems/solutions.html#P2-sols",
    "href": "problems/solutions.html#P2-sols",
    "title": "Solutions",
    "section": "Problem Sheet 2",
    "text": "Problem Sheet 2\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.      [2018 exam, Question 4]\n\n(a)   Suppose it is desired to estimate the value of an integral \\[ \\theta = \\int_0^1 h(x)\\,\\mathrm{d}x \\] by Monte Carlo integration\n\n        i.   By splitting \\(h(x) = \\phi(x)f(x)\\), where \\(f\\) is a probability density function, describe how the Monte Carlo method of estimating \\(\\theta\\) works.\n\nSolution. As suggested, we have \\[ \\theta = \\int_0^1 h(x)\\,\\mathrm{d}x = \\int_0^1 \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\Exg \\phi(X) ,\\] where \\(X\\) is a random variable with PDF \\(f\\). To estimate this, we sample \\(X_1, \\dots, X_n\\) from \\(X\\), and use \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) . \\]\n\n\n\n        ii.  Let \\(\\widehat\\theta_n\\) be the Monte Carlo estimate based on \\(n\\) simulations. Find the expectation and variance of \\(\\widehat\\theta_n\\), as a function of \\(n\\).\n\nSolution. As in lectures, we have \\[ \\Exg \\theta_n^{\\mathrm{MC}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n}\\,n\\Exg\\phi(X) = \\Exg\\phi(X) \\] and \\[ \\Var\\big((\\theta_n^{\\mathrm{MC}}\\big) = \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n^2}\\,n\\Var\\big(\\phi(X)\\big) = \\frac{1}{n}\\Var\\big(\\phi(X)\\big) . \\]\n\n\n\n        iii.  What guidelines can be given for the choice of \\(f\\) in practice?\n\nSolution. First, \\(f\\) (or equivalently \\(X\\)) should be easy to sample from. Second, we want to minimise \\(\\Var(\\phi(X))\\), so should pick \\(f\\) approximately proportional to \\(h\\), so that \\(\\phi\\) is roughly constant, and therefore has low variance.\nIn the absence of better options, \\(X\\) being uniform on \\([0, 1]\\) (so \\(f(x) = 1\\) on this interval) is often not a bad choice.\n\n\n\n\n(b)  Consider evaluation the integral \\[ \\int_0^1 x^2\\,\\mathrm{d}x \\] by Monte Carlo integration using \\(f(x) = \\Ind_{[0,1]}(x)\\). Write down the Monte Carlo estimator \\(\\widehat\\theta_n\\).\n\nSolution. Since \\(f(x) = 1\\) on this interval, we must take \\(\\phi(x) = x^2\\). Thus the estimator is \\[\\widehat\\theta_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2 ,\\] where \\(X_i \\sim \\operatorname{U}[0,1]\\) are independent.\n\n          Explain how antithetic variables can be used in this situation, and justify briefly why their use here is guaranteed to improve efficiency.\n\nSolution. Antithetic variables attempt to reduce the variance in Monte Carlo estimation by using pairs of variables \\((X_i, X'_i)\\) that both have the same distribution as \\(X\\), but where \\(\\phi(X)\\) and \\(\\phi(X')\\) are negatively correlated.\nIn this situation, if \\(X_i \\sim \\operatorname{U}[0,1]\\), then \\(X'_i = 1 - X_i\\) has this same distribution. The corresponding antithetic variable estimator is \\[\\widehat\\theta_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(X_i^2 + (1 - X_i)^2\\big) .\\]\nAn anitithetic variables estimator always decreases the variance of a Monte Carlo estimator if the correlation (or, equivalently, the covariance) between \\(\\phi(X)\\) and \\(\\phi(X')\\) is negative. We saw in lectures that, if \\(X \\sim \\operatorname{U}[0,1]\\) and \\(\\phi\\) is monotonically increasing, then \\(\\phi(X)\\) and \\(\\phi(1 - X)\\) have negative correlation. Since \\(x^2\\) is increasing on \\([0,1]\\), that is the case here.\n\n          For \\(U \\sim \\operatorname{U}[0,1]\\), use the results \\[\\Ex U^2 = \\tfrac13 \\qquad \\Ex U^4 = \\tfrac15 \\qquad \\Ex U^2(1-U)^2 = \\tfrac{1}{30} \\] to find the correlation between \\(U^2\\) and \\((1-U)^2\\). Hence, or otherwise, confirm that using antithetic variables reduces the variance of the Monte Carlo estimator by a factor of 8.\n\nSolution. We need to find the correlation between \\(U^2\\) and \\((1 - U)^2\\), where \\(U \\sim \\operatorname{U}[0,1]\\). The covariance is \\[ \\operatorname{Cov}\\big(U^2, (1-U)^2\\big)\n= \\Ex U^2(1-U)^2 - \\big(\\Ex U^2\\big) \\big(\\Ex (1-U)^2\\big)\n= \\tfrac{1}{30} - \\Big(\\tfrac{1}{3}\\Big)^2 = -\\tfrac{7}{90} .\\] The variance are both \\[ \\Var(U^2) = \\Ex U^4 - \\big(\\Ex U^2\\big)^2 = \\tfrac15 - \\Big(\\tfrac{1}{3}\\Big)^2 = \\tfrac{4}{45} . \\] Hence, the correlation is \\[ \\rho = \\operatorname{Corr}\\big(U^2, (1-U)^2\\big) = \\frac{-\\frac{7}{90}}{\\frac{4}{45}} = -\\tfrac78 . \\]\nThe variance of the standard Monte Carlo estimator is \\(\\frac{1}{n}\\Var(\\phi(X))\\), while the variance of the antithetic variables estimator is \\(\\frac{1+\\rho}{n}\\Var(\\phi(X))\\). So the variance changes by a factor of \\(1 + \\rho\\), which here is \\(1 - \\frac{7}{8} = \\frac{1}{8}\\). So the variance reduces by a factor of 8, as claimed.\n\n          (You may use without proof any results about the variance of antithetic variable estimates, but you should clearly state any results you are using.)\n\n\n\n2.     Let \\(X \\sim \\operatorname{N}(0,1)\\). Consider importance sampling estimation for the probability \\(\\theta = \\mathbb P(3 \\leq X \\leq 4)\\) using samples \\(Y_i\\) from the following sample distributions: (i) \\(Y \\sim \\operatorname{N}(1,1)\\); (ii) \\(Y \\sim \\operatorname{N}(2,1)\\); (iii) \\(Y \\sim \\operatorname{N}(3.5,1)\\); (iv) \\(Y \\sim 3 + \\operatorname{Exp}(1)\\).\nEach of these four distributions gives rise to a different importance sampling method. Our aim is to compare the resulting estimates.\n\n(a)  For each of the four methods, estimate the variance \\(\\Var\\big(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\big)\\). Which of these four methods gives the best results?\n\nSolution. I used the following R code\n\nn &lt;- 1e5\nphi   &lt;- function(x) (x &gt;= 3) & (x &lt;= 4)\npdf_x &lt;- function(x) dnorm(x, 0, 1)\n\npdf_y1 &lt;- function(x) dnorm(x, 1, 1)\nsamples_y1 &lt;- rnorm(n, 1, 1)\nvar1 &lt;- var((pdf_x(samples_y1) / pdf_y1(samples_y1)) * phi(samples_y1))\n\npdf_y2 &lt;- function(x) dnorm(x, 2, 1)\nsamples_y2 &lt;- rnorm(n, 2, 1)\nvar2 &lt;- var((pdf_x(samples_y2) / pdf_y2(samples_y2)) * phi(samples_y2))\n\npdf_y3 &lt;- function(x) dnorm(x, 3.5, 1)\nsamples_y3 &lt;- rnorm(n, 3.5, 1)\nvar3 &lt;- var((pdf_x(samples_y3) / pdf_y3(samples_y3)) * phi(samples_y3))\n\npdf_y4 &lt;- function(x) dexp(x - 3, 1)\nsamples_y4 &lt;- 3 + rexp(n, 1)\nvar4 &lt;- var((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4))\n\nsignif(c(var1, var2, var3, var4), 3)\n\n[1] 8.65e-05 1.40e-05 6.68e-06 1.92e-06\n\n\n(For the fourth PDF, we used that the PDF of \\(3 + Z\\) is \\(f_Z(z-3)\\).)\nWe see that the fourth method \\(3 + \\operatorname{Exp}(1)\\) is the most accurate.\n\n\n\n(b)  Determine a good estimate for \\(\\mathbb P(3 \\leq X \\leq 4)\\), and discuss the accuracy of your estimate.\n\nSolution. We’ll use the fourth method, and boost the number of samples to one million.\n\nn &lt;- 1e6\nISest &lt;- mean((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4))\nIS_MSE &lt;- var((pdf_x(samples_y4) / pdf_y4(samples_y4)) * phi(samples_y4)) / n\nc(ISest, sqrt(IS_MSE))\n\n[1] 1.310251e-03 1.387297e-06\n\n\nSo our estimate is \\(\\widehat\\theta = 0.00131\\). Since the RMSE is three orders of magnitude less than the estimate, so the estimate is probably accurate to a couple of significant figures.\n\n\n\n(c)  For each of the four methods, approximate how many samples from \\(Y\\) are required to reduce the root-mean-square error of the estimate of \\(\\mathbb P(3 \\leq X \\leq 4)\\) to 1%?\n\nSolution. To get the error to 1% means an absolute error of roughly \\(\\epsilon = 0.01\\widehat\\theta\\). Then we know that the required number of samples is \\[ n = \\frac{\\Var\\big(\\frac{f(Y)}{g(Y)}\\phi(Y)\\big)}{\\epsilon^2} . \\]\n\neps &lt;- 0.01 * ISest\nround(c(var1, var2, var3, var4) / eps^2)\n\n[1] 503778  81273  38901  11211\n\n\n\n\n\n\n3.     [2017 exam, Question 3]\n\n(a)  Defining any notation you use, write down the basic Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) and the importance sampling estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) for an expectation of the form \\(\\theta = \\Exg \\phi(X)\\), where \\(X\\) is a random variable with probability density function \\(f\\).\nWhat is the advantage of importance sampling over the standard Monte Carlo method?\n\nSolution. The basic Monte Carlo estimator is \\[ \\widehat\\theta_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n\\phi(X_i) , \\] where the \\(X_i\\) are independent random samples from \\(X\\).\nThe basic importance sampling estimator is \\[ \\widehat\\theta_n^{\\mathrm{IS}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) , \\] where the \\(Y_i\\) are independent random samples from a random variable \\(Y\\) with probability density function \\(g\\). We must have \\(g(y) &gt; 0\\) whenever \\(f(y) &gt; 0\\).\nThe main advantage of the importance sampling estimator is that can reduce the variance of the estimator by oversampling the most important areas of \\(y\\), but then downweighting those samples. Another advantage is that importance sampling can be used when it is difficult to sample from \\(X\\).\n\n\n\n(b)  Prove that both the basic Monte Carlo and importance sampling estimates from part (a) are unbiased.\n\nSolution. For the standard Monte Carlo estimator, \\[ \\Exg \\theta_n^{\\mathrm{MC}} = \\Ex \\left(\\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\right) = \\frac{1}{n}\\,n\\Exg\\phi(X) = \\Exg\\phi(X) = \\theta .\\]\nFor the importance sampling estimator, first note that \\[ \\mathbb E \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\int_{-\\infty}^{+\\infty} \\frac{f(y)}{g(y)}\\,\\phi(y)\\,g(y)\\,\\mathrm{d}y = \\int_{-\\infty}^{+\\infty} \\phi(y) \\,f(y) \\, \\mathrm{d}y = \\Exg\\phi(X)  , \\] since \\(f\\) is the PDF of \\(X\\). Hence \\[ \\Exg \\widehat\\theta_n^{\\mathrm{IS}} = \\Exg \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) \\right) = \\frac{1}{n}\\,n\\,\\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) = \\Exg \\phi(X) = \\theta. \\]\nHence, both estimators are unbiased.\n\n\n\n(c)  Show that the variance of the importance sampling estimator is given by \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\frac{1}{n}\\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y - \\frac{1}{n}\\big(\\Exg \\phi(X)\\big)^2. \\]\n\nSolution. First, note that \\[\\begin{align} \\Var \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right)\n&= \\Ex \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right)^2 - \\big(\\Exg \\phi(X)\\big)^2 \\\\\n&= \\int_{-\\infty}^{+\\infty} \\left(\\frac{f(y)}{g(y)}\\,\\phi(y)\\right)^2 g(y)\\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2\\\\\n&= \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2}{g(y)^2}\\,\\phi(y)^2 \\,g(y)\\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2\\\\\n&= \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2}{g(y)}\\,\\phi(y)^2 \\,\\mathrm{d}y  - \\big(\\Exg \\phi(X)\\big)^2 \\end{align} \\] Second, we have \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\Var \\left(\\frac{1}{n} \\sum_{i=1}^n \\frac{f(Y_i)}{g(Y_i)} \\,\\phi(Y_i) \\right) = \\frac{1}{n} \\Var \\left(\\frac{f(Y)}{g(Y)}\\,\\phi(Y)\\right) . \\] Putting these together proves the result.\n\n\n\n(d)  Let \\(X \\sim \\operatorname{N}(0,2)\\) and \\(a \\in \\mathbb R\\). We want to estimate \\(\\theta = \\Ex \\big(\\sqrt{2}\\exp(-(X-a)^2/4)\\big)\\), using importance sampling with samples \\(Y \\sim \\operatorname{N}(\\mu, 1)\\) for some \\(\\mu \\in \\mathbb R\\). Using the result from part (c), or otherwise, show that in the case the variance of the importance sampling estimator \\(\\widehat\\theta_n^{\\mathrm{IS}}\\) is given by \\[ \\Var\\big(\\widehat\\theta_n^{\\mathrm{IS}}\\big) = \\frac{\\exp(\\mu^2 - a\\mu) - \\theta^2}{n} . \\] [Note: This equation has changed since an earlier version of the question.]\n\nSolution. It’s clear, using part (c), that it will suffice to show that \\[ \\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y = \\exp(\\mu^2 - a\\mu) . \\] Here, we have \\[ \\begin{align}\nf(y) &= \\frac{1}{\\sqrt{4\\pi}} \\exp\\big(-\\tfrac14 y^2 \\big) \\\\\ng(y) &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-\\mu)^2 \\big) \\\\\n\\phi(y) &= \\sqrt{2} \\exp\\big(-\\tfrac14 (y-a)^2 \\big) .\n\\end{align} \\] Therefore, by a long and painful algebra slog, we have \\[ \\begin{align}\n\\frac{f(y)^2 \\phi(y)^2}{g(y)} &= \\frac{\\frac{1}{4\\pi} \\exp\\big(-\\tfrac12 y^2 \\big)\\times 2\\exp\\big(-\\tfrac12 (y-a)^2 \\big)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\tfrac12 (y-\\mu)^2 \\big)} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y^2 + (y-a)^2 - (y-\\mu)^2 \\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y^2 - 2(a - \\mu)y + a^2 + \\mu^2\\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big((y - (a - \\mu))^2 - (a- \\mu)^2 + a^2 - \\mu^2\\big)\\Big) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big((y - (a - \\mu))^2 + 2a\\mu - 2\\mu^2 \\big)\\Big) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\exp(\\mu^2 + a\\mu),\n\\end{align} \\] where we ‘completed the square’ on the fourth line. Thus \\[ \\begin{align}\n\\int_{-\\infty}^{+\\infty} \\frac{f(y)^2 \\phi(y)^2}{g(y)}\\,\\mathrm{d}y\n&= \\int_{-\\infty}^{+\\infty}  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\exp(\\mu^2 + 2a) \\,\\mathrm{d}y\\\\\n&= \\exp(\\mu^2 + 2a) \\int_{-\\infty}^{+\\infty}  \\frac{1}{\\sqrt{2\\pi}} \\exp \\Big(-\\tfrac12 \\big(y - (a - \\mu)\\big)^2\\Big)\\,\\mathrm{d}y \\\\\n&= \\exp(\\mu^2 + a\\mu) ,\n\\end{align} \\] since the big integral on the right is the integral of the PDF of a normal \\(\\operatorname{N}(a-\\mu, 1)\\) distribution, so equals 1.\nHence, we have proved the result.\n\n          For fixed \\(n\\) and \\(a\\), find the value of \\(\\mu\\) for which the importance sampling estimator has the smallest mean-square error. Comment on the result.\n\nSolution. Minimising this expression is equivalent to minimising \\(\\mu^2 + a\\mu\\). By differentiating with respect to \\(\\mu\\) (or otherwise), we see that this is at \\(a = \\tfrac12 \\mu\\).\n\n\n\n\n4.     (Answer the following question “by hand”, without using R. You may check your answer with R, if you wish.)\n\n(a)  Consider the LCG with modulus \\(m = 2^4 = 16\\), multiplier \\(a = 5\\), and increment \\(a = 8\\). What is the period of this LCG when started from the seed (i) \\(x_1 = 1\\); (ii) \\(x_1 = 2\\)?\n\nSolution. For (i), we have \\[ \\begin{align}\nx_1 &= 1 \\\\\nx_2 &= (5 \\times 1 + 8) \\bmod 16 = 13 \\bmod 16 = 13 \\\\\nx_3 &= (5 \\times 13 + 8) \\bmod 16 = 73 \\bmod 16 = 9 \\\\\nx_4 &= (5 \\times 9 + 8) \\bmod 16 = 53 \\bmod 16 = 5 \\\\\nx_5 &= (5 \\times 5 + 8) \\bmod 16 = 33 \\bmod 16 = 1 .\n\\end{align}\\] Here, \\(x_5\\) is a repeat of \\(x_1\\), so the period is \\(5-1=4\\).\nFor (ii), we have \\[ \\begin{align}\nx_1 &= 2 \\\\\nx_2 &= (5 \\times 2 + 8) \\bmod 16 = 18 \\bmod 16 = 2 .\n\\end{align}\\] This is an immediate repeat, so the period is 2.\n\n\n\n(b) Consider the LCG with modulus \\(m = 2^4 = 16\\), multiplier \\(a = 2\\), and increment \\(c = 4\\). Start from the seed \\(x_1 = 3\\). (i) When do we first see a repeat output? (ii) What is the period?\n\nSolution. We have \\[ \\begin{align}\nx_1 &= 3 \\\\\nx_2 &= (2 \\times 3 + 4) \\bmod 16 = 10 \\bmod 16 = 10 \\\\\nx_3 &= (2 \\times 10 + 4) \\bmod 16 = 24 \\bmod 16 = 8 \\\\\nx_4 &= (2 \\times 8 + 4) \\bmod 16 = 20 \\bmod 16 = 4 \\\\\nx_5 &= (2 \\times 4 + 4) \\bmod 16 = 12 \\bmod 16 = 12 \\\\\nx_6 &= (2 \\times 12 + 4) \\bmod 16 = 28 \\bmod 16 = 12 .\n\\end{align}\\]\n(i) The first repeat is \\(x_6\\).\n(ii) Since 12 is a fixed point of this LCG, the remainder of the sequence is 12 forever, with period 1.\n\n\n\n\n5.     Consider the following LCGs with modulus \\(m = 2^8 = 256\\):\n (i) \\(a = 31\\), \\(c = 47\\);\n (ii) \\(a = 21\\), \\(c = 47\\);\n (iii) \\(a = 129\\), \\(c = 47\\).\n\n(a)  Without using a computer, work out which of these LCGs have a full period of 256.\n\nSolution.\n (i) Here, \\(a\\) is \\(3 \\bmod 4\\), not \\(1 \\bmod 4\\), so this does not have full period of 256.\n (ii) Here, \\(c\\) is odd and \\(a\\) is \\(1 \\bmod 4\\), so this does have full period of 256.\n (iii) Here, \\(c\\) is odd and \\(a\\) is \\(1 \\bmod 4\\), so this does have full period of 256.\n\n\n\n(b)  Which of these LCGs would make good pseudorandom number generators?\n\nSolution. We will check the random appearance of the outputs using R.\n\nlcg &lt;- function(n, modulus, mult, incr, seed) {\n  samples &lt;- rep(0, n)\n  samples[1] &lt;- seed\n  for (i in 1:(n - 1)) {\n    samples[i + 1] &lt;- (mult * samples[i] + incr) %% modulus\n  }\n  return(samples)\n}\n\n (i) This does not have full period, so is unlikely to be good PRNG. Let’s check\n\nm &lt;- 2^8\nseed &lt;- 1\nplot(lcg(m, m, 31, 47, seed))\n\n\n\n\n\n\n\n\nThe picture confirms that this does not look random, and in fact has very short period of 16.\n (ii) This does have full period, so is a candidate for a good PRNG if the output looks random.\n\nplot(lcg(m, m, 21, 47, seed))\n\n\n\n\n\n\n\n\nThis mostly looks random, but there does appear to be a sort of thick diagonal line in the picture going from bottom left to rop right. I’d might be happy to use this for casual statistical work – the lack of randomness does not seem super-serious – but I would avoid this for cryptographic purposes, for example.\n (iii) This does have full period, so is a candidate for a good PRNG if the output looks random.\n\nplot(lcg(m, m, 129, 47, seed))\n\n\n\n\n\n\n\n\nEven though this has full period, there is a very clear non-random pattern. This is not appropriate for a PRNG.\n\n\n\n\n6.     Consider an LCG with modulus \\(m = 2^{10} = 1024\\), multiplier \\(a = 125\\), and increment \\(c = 5\\). Using R:\n\n(a) Generate 1000 outputs in \\(\\{0, 1, \\dots, 1023\\}\\) from this LCG, starting with the seed \\(x_1 = 1\\).\n[This question originally said 200 outputs, not 1000. It’s fine if you answered that version, but the conclusions to the problem are less interesting that way.]\n\nSolution. Using the same lcg() function from Question 5, we have\n\nn &lt;- 1000\nm &lt;- 2^{10}\nseed &lt;- 1\nsamples1 &lt;- lcg(n, m, 125, 5, seed)\n\n\n\n\n(b)  Convert these to 1000 outputs to pseudorandom uniform samples in \\([0, 1]\\).\n\nTo do this, we simply divide the samples by \\(m\\).\n\nsamples1 &lt;- samples1 / m\n\n\n\n\n(c)  Using these samples, obtain a Monte Carlo estimate for \\(\\mathbb E\\cos(U)\\), where \\(U \\sim \\operatorname{U}[0,1]\\).\n\nSolution.\n\nmean(cos(samples1))\n\n[1] 0.8418218\n\n\nThis is very close to the correct answer \\(\\sin 1 = 0.8414\\)\n\n\n\n(d)  What is the root-mean-square error of your estimate?\n\nSolution.\n\nsqrt(var(cos(samples1)) / n)\n\n[1] 0.004392217\n\n\n\n\n\n(e)  Repeat parts (a) to (d) for the LCG with the same \\(m\\), but now with multiplier \\(a = 127\\) and increment \\(c = 4\\).\n\nSolution.\n\nsamples2 &lt;- lcg(n, m, 127, 4, seed)\nsamples2 &lt;- samples2 / m\nmean(cos(samples2))\n\n[1] 0.868257\n\nsqrt(var(cos(samples2)) / n)\n\n[1] 0.003899394\n\n\nThis does not seem to be quite accurate to the answer to \\(\\sin 1 = 0.841\\), and the reported RMSE to too small to account for the error.\nHowever, the problem is that this LCG is not actually a uniform (pseudo)random number generator – it has period 8.\n\nplot(samples2)\n\n\n\n\n\n\n\n\nThus the estimator is just keeping using the same 8 points over and over again. So this is actually estimating \\(\\mathbb E(\\cos Y)\\), where \\(Y\\) is uniform on the 8 points actually visited by the LCG. So while the correct answer is \\(\\mathbb EU = \\sin 1 = 0.841\\), this is in fact estimating \\[ \\begin{multline}\n\\frac{1}{8} \\bigg(\\cos \\frac{1}{2^{10}} + \\cos \\frac{131}{2^{10}} + \\cos \\frac{257}{2^{10}} + \\cos \\frac{899}{2^{10}} \\\\\n+ \\cos \\frac{513}{2^{10}} + \\cos \\frac{643}{2^{10}} + \\cos \\frac{769}{2^{10}} + \\cos \\frac{387}{2^{10}}\\bigg) = 0.868\n\\end{multline} \\] (where the numerators are the 8 values visited by the LCG), which is not the correct answer.",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "problems/solutions.html#P3-sols",
    "href": "problems/solutions.html#P3-sols",
    "title": "Solutions",
    "section": "Problem Sheet 3",
    "text": "Problem Sheet 3\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.     Consider a discrete random variable that takes values \\(1, 2, 3, 4.5\\) with probabilities \\(0.2, 0.2, 0.5, 0.1\\) respectively. Write some R code that will sample from this distribution. (Your code may use the runif() function, but may not use the sample() function.) Check that a large sample from your code really does have the correct distribution.\n\nSolution. There are various ways to do this. With only four outcomes, you can just write a lot of this “by hand”, in a way that wouldn’t be practical if the range were very large, but it pretty easy to write – that’s what I did.\n\nrq1 &lt;- function(n) {\n  x &lt;- c(1, 2, 3, 4.5)\n  prob &lt;- c(0.2, 0.2, 0.5, 0.1)\n  cumprob &lt;- cumsum(prob)\n  unif &lt;- runif(n)\n  ifelse(unif &lt;= cumprob[1], x[1],\n  ifelse(unif &lt;= cumprob[2], x[2],\n  ifelse(unif &lt;= cumprob[3], x[3], x[4])))\n}\n\n                Let’s test its accuracy by drawing a bar plot of a large sample.\n\nn &lt;- 1e6\nsamples &lt;- rq1(n)\nplot(table(samples) / n)\n\n\n\n\n\n\n\n\n                Looks good to me.\n\n\n\n2.     The geometric distribution \\(X \\sim \\operatorname{Geom}(p)\\) represents the number of trials until the first success, where each trial succeeds independently with probability \\(p\\). The probability mass function of \\(X\\) is \\[ p(x) = (1-p)^{x-1}p \\qquad x = 1, 2, \\dots. \\]\n\n(a)  Show that the cumulative distribution function \\(F\\) of \\(X\\) is given by \\[ F(x) = 1 - (1-p)^x \\qquad x = 1, 2, \\dots .\\]\n\nSolution. I can think of two ways to do this. The first way is to just sum the probabilities, using the formula \\[ \\sum_{z=0}^{x-1} a^z = \\frac{1 - a^x}{1-a} \\] for the sum of a geometric progression. We have \\[ F(x) = \\sum_{y=1}^x (1-p)^{y-1} p = p \\sum_{z=0}^{x-1} (1-p)^z = p \\frac{1 - (1-p)^{x}}{1 - (1-p)} = 1 - (1-p)^x , \\] where in the second equality where shifted the index of the sum with \\(z = y-1\\).\nAlternatively (and, in my opinion, better) is to think about what the geometric distribution means. The geometric probability \\(p(x)\\) is the probability the first success occurs on the \\(x\\)th trial. So the complement of the CDF, \\(1 - F(x) = \\mathbb P(X &gt; x)\\) is the probability the first success happens after the \\(x\\)th trial, which is if and only if the first \\(x\\) trials are all failures. This is \\[ 1 - F(x) = \\mathbb P(X &gt; x) = (1-p)^x \\] because each of those \\(x\\) trials fails with probability \\(1 - p\\). This gives the answer.\n\n\n\n(b)  Write down a function – either in mathematical notation or in R code – that will transform a standard uniform random variable \\(U\\) into a geometric distribution. Try to make your function as simple as possible.\n\nSolution. The “dividing lines” between the segments come at the values \\(u\\) where \\(u = 1- (1-p)^x\\). Actually, since we’re trying to make the function simple, we could use \\(v = 1-u\\) (since one minus a standard uniform is still standard uniform), with dividing lines at \\(v = (1-p)^x\\), which corresponds to \\(x = \\log v / \\log(1-p)\\). So the segment corresponding to a uniformly distributed \\(v\\) will correspond to this value rounded up to the next integer.\nSo we can take \\(U \\sim \\operatorname{U}[0,1]\\), and put \\[ X = \\left\\lceil \\frac{\\log U}{\\log(1-p)} \\right\\rceil . \\]\nLet’s check this with R code\n\nrgeom2 &lt;- function(n, p) {\n  unif &lt;- runif(n)\n  ceiling(log(unif) / log(1 - p))\n}\n\nn &lt;- 1e6\np &lt;- 1/3\nsamples &lt;- rgeom2(n, p)\nplot(table(samples) / n, xlim = c(0, 10))\n\n\n\n\n\n\n\n\nWe could alternatively check the probabilities exactly.\n\ntrue &lt;- (1 - p)^{1:8 - 1} * p\nemp &lt;- table(samples)[1:8] / n\nround(rbind(true, emp), 4)\n\n          1      2      3      4      5      6      7      8\ntrue 0.3333 0.2222 0.1481 0.0988 0.0658 0.0439 0.0293 0.0195\nemp  0.3331 0.2224 0.1484 0.0983 0.0659 0.0438 0.0292 0.0196\n\n\nLooks good again.\n\n\n\n\n3.     Consider a Cauchy random variable \\(X\\) with probability density function \\[ f(x) = \\frac{1}{\\pi(1 + x^2)} .\\]\n\n(a)  Show that the cumulative distribution function of \\(X\\) is \\[ F(x) = \\frac12 + \\frac{1}{\\pi}\\arctan x \\]\n\nSolution. The CDF is \\[ \\begin{multline}\nF(x) = \\int_{-\\infty}^x f(y)\\,\\mathrm{d}y\n  = \\frac{1}{\\pi} \\int_{-\\infty}^x \\frac{1}{1+y^2}\\,\\mathrm{d}y\n  = \\frac{1}{\\pi} \\big[\\arctan y\\big]_{-\\infty}^x \\\\ = \\frac{1}{\\pi} \\bigg( \\arctan x - \\Big(-\\frac{\\pi}{2}\\Big)\\bigg) = \\frac{1}{\\pi}\\arctan x + \\frac12,\n\\end{multline}\\] since \\(\\lim_{y\\to-\\infty} \\arctan y = -\\frac{\\pi}{2}\\).\n\n\n\n(b)  Write down a function that will transform a standard uniform random variable \\(U\\) into a Cauchy distribution.\n\nSolution. We use the inverse transform method: write \\(U = F(X)\\) and invert. Here, we have \\[ U = \\frac12 + \\frac{1}{\\pi}\\arctan X .\\] Inverting gives \\[X = \\tan \\bigg(\\pi \\Big(U - \\frac12\\Big) \\bigg) .\\]\nThis can be interpreted as \\(X = \\tan\\Theta\\) where \\(\\Theta\\) is uniform between \\(-\\frac{\\pi}{2}\\) and \\(\\frac{\\pi}{2}\\).\n\n\n\n(c)  Using your answer to part (b), draw a histogram of samples from the Cauchy distribution in R.\n\nSolution. My function for generating samples is the following:\n\nrcauchy2 &lt;- function(n) {\n  unif &lt;- runif(n)\n  tan(pi * (unif - 1/2))\n}\n\nThe Cauchy distribution is a very heavy-tailed distribution. So to make the histogram look sensible, I’m going to throw away the occasional very large or very small sample. If I didn’t do this, the histogram would probably just look like a single spike at 0. (You of course shouldn’t do this when performing Monte Carlo estimation – those rare very large or very small samples can be extremely important in calculations!)\n\nn &lt;- 1e6\nsamples &lt;- rcauchy2(n)\n\nsamples_rest &lt;- samples[abs(samples) &lt; 12]\nhist(samples_rest, probability = TRUE, ylim = c(0, 0.33), breaks = 48)\ncurve(1 / (pi * (1 + x^2)), add = TRUE, n = 1001, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n4.     Let \\(F\\) be a cumulative distribution function and \\(F^{-1}\\) its inverse.\n\n(a)  Prove that \\(F^{-1}\\) is a non-decreasing function.\n\nSolution. We recall the definition \\(F^{-1}(u) = \\min \\{x : F(x) \\geq u\\}\\). As \\(u\\) increases, the set \\(A_u = \\{x : F(x) \\geq u \\}\\) gets smaller – more specifically, for \\(u \\leq v\\), we have \\(A_u \\subseteq A_v\\). Hence the minimum of the set \\(A_u\\) cannot be larger than the minimum of the set \\(A_v\\). Hence \\(F^{-1}(u) = \\min A_u \\leq \\min A_v = F^{-1}(v)\\), as required.\n\n\n\n(b)  Show that \\(X = F^{-1}(U)\\) and \\(X' = F^{-1}(1-U)\\) have negative (or, rather, non-positive) correlation. You may use any results from the module, provided you state them clearly.\n\nSolutions. The relevant result here is Theorem 7.2. This said that if \\(\\phi\\) is an non-decreasing function, then \\(\\phi(U)\\) and \\(\\phi(1-U)\\) have covariance – and therefore correlation – less than or equal to 0. Here, we use \\(F^{-1}\\) as the function \\(\\phi\\).\n\n\n\n\n5.     Let \\(X \\sim \\operatorname{Beta}(3, 2)\\) be a Beta distribution with PDF \\[ f(x) = 12 x^2(1-x) \\qquad 0 \\leq x \\leq 1 . \\] [Note: An earlier version of this question wrongly had the constant at the front as \\(\\frac{1}{12}\\) instead of \\(12\\).]\nShow how you could sample from \\(X\\) using envelope rejection sampling and an optimised value of the constant \\(c\\).\n\nSolution. The obvious suggestion here is to take \\(Y\\) uniformly distributed on \\([0, 1]\\), so \\(g(x) = 1\\) (although you don’t have to choose that).\nTo find the optimal value of \\(c\\), we consider the maximum of \\(f(x)\\). We can find that by differentiating \\[ f'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} (12x^2 - 12x^3) = (24x - 36x^2) , \\] so the maximum is at \\(x = \\frac23\\), where \\(f(x) = \\frac{16}{9}\\). Therefore, we take \\(c = \\frac{16}{9}\\).\nThus, our algorithm is to sample from a standard uniform, and then to accept with probability \\[ \\alpha(x) = \\frac{12x^2(1-x)}{\\frac{16}{9}} = \\frac{27}{4}\\,x^2(1-x) . \\]\n\n\n\n6.     Consider sampling from the half-normal distribution \\[ f(x) = \\sqrt{\\frac{2}{\\pi}} \\exp\\big(-\\tfrac12 x^2\\big) \\qquad x \\geq 0 \\] using envelope rejection sampling with an \\(\\operatorname{Exp}(\\lambda)\\) proposal distribution \\[g(x) = \\lambda \\mathrm{e}^{-\\lambda x} \\qquad x \\geq 0 .\\] You wish to design your envelope rejection sampling algorithm so that the acceptance probability is as high as possible.\n\n(a)  For fixed \\(\\lambda\\), show that the optimal value of \\(c\\) is \\[c = \\sqrt{\\frac{2}{\\pi}}\\,\\frac{\\exp(\\frac12\\lambda^2)}{\\lambda}.\\]\n\nSolution. This is very similar to Example 15.2 in Lecture 15. We have \\[ \\frac{f(x)}{g(x)} = \\frac{\\sqrt\\frac{2}{\\pi} \\exp(\\tfrac12 x^2)}{\\lambda \\exp(-\\lambda x)} = \\sqrt\\frac{2}{\\pi} \\,\\frac{\\exp \\big(-\\tfrac12 x^2 + \\lambda x\\big)}{\\lambda} . \\] We want to pick \\(c\\) to be the maximum value of this. The maximum occurs where \\(-\\frac12x^2 + \\lambda x\\) is maximised. By differentiating this and setting equal to 0, we get \\(-x + \\lambda = 0\\), so \\(x = \\lambda\\) and \\[ c =  \\sqrt\\frac{2}{\\pi} \\,\\frac{\\exp \\big(-\\tfrac12 \\lambda^2 + \\lambda^2\\big)}{\\lambda} =   \\sqrt\\frac{2}{\\pi} \\,\\frac{\\exp \\big(\\tfrac12 \\lambda^2\\big)}{\\lambda}, \\] as required.\n\n\n\n(b)  Show that the optimal value of \\(\\lambda\\) is \\(\\lambda = 1\\).\n\nSolution. Again, our goal is to choose \\(\\lambda\\) get \\(c\\) as small as possible, since \\(1/c\\) is the acceptance probability. Differentiating the expression from part (a) with respect to \\(\\lambda\\) gives \\[ \\frac{\\mathrm d}{\\mathrm d\\lambda} \\,c = \\sqrt\\frac{2}{\\pi} \\frac{\\exp \\big(\\tfrac12 \\lambda^2\\big)(\\lambda^2 - 1)}{\\lambda^2}.\\] This is 0 when \\(\\lambda^2 = 1\\), and since \\(\\lambda \\geq 0\\), the only solution is \\(\\lambda = 1\\). (This can be easily checked to be a minimum by looking at a sketch of \\(c\\) against \\(\\lambda\\), or differentiating twice, or just by thinking about the behaviour of \\(c\\) as a function of \\(\\lambda\\).)\n\n\n\n\n7.      [2016 exam, Question 1]  In this question, we consider generating samples from the distribution with probability density function \\[ f_a(x) = \\frac{1}{Z} \\,\\frac{1}{a(\\cos x + 1) + x^2} \\qquad x \\geq 1,\\] where \\(a\\) is a parameter and \\[Z = \\int_{1}^{\\infty} \\frac{1}{a(\\cos x + 1) + x^2}\\,\\mathrm{d}x \\] is a normalising constant.\n\n(a)  Introducing any notation you use, state the inverse transform method for random number generation.\n\nSolution. Fix a cumulative distribution function (CDF) \\(F(x) = \\mathbb P(X \\leq x)\\). The inverse CDF is defined to be \\(F^{-1}(u) = \\max \\{x : F(x) \\geq u\\}\\). We then have that \\(X = F^{-1}(U)\\) has CDF \\(F\\). We use this typically by writing \\(U = F(X)\\) and inverting to make \\(X\\) the subject.\n\n\n\n(b)  For \\(a = 0\\), explain how the inverse transform method can be used to generate samples with PDF \\(f_0\\).\n\nFor \\(a = 0\\), the PDF is \\[ f_0(x) = \\frac{1}{Z} \\,\\frac{1}{x^2} \\qquad x \\geq 1 .\\]\nWe should start by finding the normalising constant \\(Z\\): it’s \\[ Z = \\int_1^\\infty \\frac{1}{x^2}\\,\\mathrm{d}x = \\big[-x^{-1}\\big]_1^\\infty = 0 -(-1) = 1 , \\] so the PDF is simply \\(f_0(x) = 1/x^2\\).\nNext, we want to find the CDF \\(F\\): it’s \\[ F(x) = \\int_1^x f_0(y)\\,\\mathrm{d}y = \\int_1^x \\frac{1}{y^2}\\,\\mathrm{d}y = \\big[-y^{-1}\\big]_1^x = 1 - \\frac{1}{x} . \\]\nFinally, to perform the inverse transform method, we write \\(U = F(X)\\) and invert. We have \\(U = 1 - 1/x\\), and so \\(X = 1/(1-U)\\).\n\n\n\n(c)  Introducing any notation you use, state the envelope rejection sampling method for random number generation.\n\n\n(d)  For \\(a &gt; 0\\), explain how the envelope rejection sampling method can be used to generate samples with PDF \\(f_a\\).\n\n\n(e)  How does the efficiency of your method in part (d) change as the value of \\(a\\) increases? Justify your answer.",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "problems/solutions.html#P4-sols",
    "href": "problems/solutions.html#P4-sols",
    "title": "Solutions",
    "section": "Problem Sheet 4",
    "text": "Problem Sheet 4\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.     Consider a Markov chain on the discrete state space \\(\\mathcal S = \\{1, 2, \\dots\\}\\) with transition probabilities \\(p(x, x+1) = p\\) for all \\(x\\), \\(p(x, 1) = 1 - p\\) for all \\(x\\), and \\(p(x, y) = 0\\) otherwise.\n\n(a)  Calculate the two-step transition probabilities \\(p^{(2)}(x, y)\\). (You might start by working out what two-step transitions are even possible.)\n\nSolution. The possible paths of length two from \\(x\\) are\n\n\\(x \\to x + 1 \\to x + 2\\): this requires two steps up, so has probability \\(p^2\\).\n\\(x \\to x+1 \\to 1\\): this requires one step up then a reset, so has probability \\(p(1-p)\\).\n\\(x \\to 1 \\to 2\\): this requires a reset then one step up, so has probability \\((1-p)p\\).\n\\(x \\to 1 \\to 1\\): this requires two resets, so has probability \\((1-p)^2\\).\n\nHence the two-step transition probabilities are \\[ \\begin{align}\np^{(2)}(x, x+2) &= p^2 \\\\\np^{(2)}(x, 2) &= (1-p)p \\\\\np^{(2)}(x, 1) &= p(1 - p) + (1-p)^2 = 1 - p\n\\end{align} \\] and \\(p(x, y) = 0\\) otherwise.\n\n\n\n(b)  Find the stationary distribution \\(\\pi\\) for the Markov chain by solving \\(\\pi(y) = \\sum_x \\pi(x) p(x,y)\\).\n\nSolution. For \\(y \\neq 1\\), the only \\(x\\) with \\(p(x, y) \\neq 0\\) is \\(x = y-1\\). So we have \\(\\pi(y) = \\pi(y-1)\\,p(y-1, y) = p\\,\\pi(y-1)\\).\nHence we have \\(\\pi(2) = p\\,\\pi(1)\\), \\(\\pi(3) = p\\,\\pi(2) = p^2 \\pi(1)\\), \\(\\pi(4) = p\\,\\pi(3) = p^3 \\pi(1)\\), and in general \\(\\pi(i) = p^{i-1}\\,\\pi(1)\\). (This equation even holds for \\(i = 1\\) itself.)\nWe also know that \\(\\pi\\) is a distribution, so must sum to 1. Hence \\[ 1 = \\sum_{i=1}^\\infty \\pi(i) = \\sum_{i=1}^\\infty p^{i-1}\\,\\pi(1) = \\pi(1) \\sum_{i=1}^\\infty p^{i-1} = \\pi(1) \\, \\frac{1}{1-p} .  \\] Hence \\(\\pi(1) = 1 - p\\), and \\(\\pi(i) = p^{i-1}(1-p)\\). This is a \\(\\operatorname{Geom}(1-p)\\) distribution.\n\n\n\n\n2.     The health of a chicken each day during a bird flu pandemic is described by a simple “healthy–sick–dead” Markov chain model. The state space is \\(\\mathcal S = \\{\\text{H}, \\text{S}, \\text{D}\\}\\). The transition probabilities are \\[ \\begin{align}\np_{\\mathrm{HH}} &= ? & p_{\\mathrm{HS}} &= 0.02 & p_{\\mathrm{HD}} &= 0.01 \\\\\np_{\\mathrm{SH}} &= 0.3 & p_{\\mathrm{SS}} &= 0.5 & p_{\\mathrm{SD}} &= 0.2 \\\\\np_{\\mathrm{DH}} &= ? & p_{\\mathrm{DS}} &= ? & p_{\\mathrm{DD}} &= 1.\n\\end{align} .\\] Fill in the three gaps (marked \\(?\\)).\nIf a chicken is healthy on day 1, what is the probability it is still alive on (a) day 2, (b) day 3; (c) day 11; (d) day 51? (You should do parts (a) and (b) by hand, but I recommend a computer for parts (c) and (d).)\n\nSolutions. Because row of a transition matrix have to add up to 1, \\(p_{\\mathrm{HH}} = 1 - 0.02 - 0.01 = 0.97\\). Because all entries also have to be non-negative, it must be that \\(p_{\\mathrm{DH}} = p_{\\mathrm{DS}} = 0\\).\n(a) One day later, on day 2, the chicken has died with probability \\(p_{\\mathrm{HD}} = 0.01\\), and is still alive with probability \\(p_{\\mathrm{HH}} + p_{\\mathrm{HS}} = 0.97 + 0.02 = 1 - p_{\\mathrm{HD}} = 0.99\\).\n(b) This requires us to calculate \\(p^{(2)}_{\\mathrm{HH}} + p^{(2)}_{\\mathrm{HS}} = 1 - p^{(2)}_{\\mathrm{HD}}\\).\nWe can either do this by summing over paths of length 2 (like in Question 1(a)) or by doing the matrix multiplication. Let’s do it the matrix multiplication way this time. We have \\[ \\mathsf P^{(2)} = \\mathsf P^2 = \\begin{pmatrix} 0.97 & 0.02 & 0.01 \\\\ 0.3 & 0.5 & 0.2 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.97 & 0.02 & 0.01 \\\\ 0.3 & 0.5 & 0.2 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.9469 & 0.0294 & 0.0237 \\\\ 0.441 & 0.256 & 0.303 \\\\ 0 & 0 & 1\\end{pmatrix}.\\]\nSo the answer is \\(0.9469 + 0.0294 = 1 - 0.0237 = 0.9763\\).\n(c) and (d) I read the transition matrix into R as follows.\n\nP &lt;- matrix(c(0.97, 0.02, 0.01, 0.3, 0.5, 0.2, 0, 0, 1), 3, 3, byrow = TRUE)\n\nI then used the matrix power function from Lecture 18.\n\nmatrixpow &lt;- function(M, n) {\n  if (n == 1) return(M)\n  else return(M %*% matrixpow(M, n - 1))\n}\n\nThe answers are the following.\n\nP11 &lt;- matrixpow(P, 11)\nP11[1, 1] + P11[1, 2]\n\n[1] 0.8354785\n\nP50 &lt;- matrixpow(P, 50)\nP50[1, 1] + P50[1, 2]\n\n[1] 0.4186196\n\n\n\n\n\n3.     Consider sampling from Poisson distribution with rate \\(\\lambda\\) using the random walk Metropolis algorithm on the integers.\n\n(a)  Calculate the acceptance probabilities for this Markov chain. What proposals are always accepted with probability 1?\n\nSolution. The Poisson distribution has PMF \\[ \\pi(x) = \\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^x}{x!} . \\]\nThe acceptance probability for “up one” is \\[ \\alpha(x, x+1) = \\min \\left\\{ \\frac{\\pi(x+1)}{\\pi(x)} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^{x+1}}{(x+1)!}}{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^x}{x!}} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{\\lambda}{x+1}, \\, 1 \\right\\} . \\] This is 1 if the step up remains less than or equal to \\(\\lambda\\).\nThe acceptance probability for “down one” is \\[ \\alpha(x, x-1) = \\min \\left\\{ \\frac{\\pi(x-1)}{\\pi(x)} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^{x-1}}{(x-1)!}}{\\mathrm{e}^{-\\lambda} \\,\\frac{\\lambda^x}{x!}} , \\, 1 \\right\\}\n= \\min \\left\\{ \\frac{x}{\\lambda}, \\, 1 \\right\\} . \\] This is 1 if the step down is from greater than or equal to \\(\\lambda\\).\n\n\n(b)  Suggest a good initial starting point \\(X_1\\) for your Markov chain. Why did you choose this?\n\nSolution. Somewhere in the middle of the distribution would be good. I would suggest the nearest integer to \\(\\lambda\\) as a pretty good place to start, but that’s not the only sensible choice.\n\n\n\n(c)  Write some R code to run this Markov chain in the case \\(\\lambda = 4.5\\).\n\nThis is my code\n\nlambda &lt;- 4.5\n\nacceptup   &lt;- function(x, lambda) lambda / (x + 1)\nacceptdown &lt;- function(x, lambda) x / lambda\ninitial &lt;- round(lambda)\n\nn &lt;- 1e6\nMRW &lt;- rep(0, n)\nMRW[1] &lt;- initial\n\nfor (i in 1:(n - 1)) {\n  if (runif(1) &lt; 0.5) {\n    # up proposal\n    if (runif(1) &lt; acceptup(MRW[i], lambda)) MRW[i + 1] &lt;- MRW[i] + 1\n    else                                     MRW[i + 1] &lt;- MRW[i]\n  } else {\n    # down proposal\n    if (runif(1) &lt; acceptdown(MRW[i], lambda)) MRW[i + 1] &lt;- MRW[i] - 1\n    else                                       MRW[i + 1] &lt;- MRW[i]\n  }\n}\n\nLet’s check if this has worked by looking at the probabilities.\n\nobserved &lt;- table(MRW)[1:11] / n\nexpected &lt;- dpois(0:10, lambda)\nround(rbind(observed, expected), 4)\n\n              0      1      2      3      4      5      6      7      8      9\nobserved 0.0109 0.0499 0.1122 0.1678 0.1897 0.1710 0.1295 0.0829 0.0462 0.0232\nexpected 0.0111 0.0500 0.1125 0.1687 0.1898 0.1708 0.1281 0.0824 0.0463 0.0232\n             10\nobserved 0.0105\nexpected 0.0104\n\n\nThis looks like an excellent match.\n\n\n\n(d)  Using your Markov chain, obtain an MCMC estimate of \\(\\operatorname{\\mathbb E}X(X-1)\\), where \\(X \\sim \\operatorname{Po}(4.5)\\).\n\nSolution.\n\nmean(MRW * (MRW - 1))\n\n[1] 20.25965\n\n\n\n\n\n(e)  (Optional) Calculate the correct answer, and comment on the accuracy of your estimate.\n\nSolution. We have \\[ \\mathbb EX(X-1) = \\sum_{x=0}^\\infty x(x-1)\\,\\mathrm{e}^{\\lambda}\\,\\frac{\\lambda^x}{x!} =  \\lambda^2\\, \\mathrm{e}^{-\\lambda} \\sum_{x=2}^\\infty \\frac{\\lambda^{x-2}}{(x-2)!} = \\lambda^2 \\,\\mathrm{e}^{-\\lambda}\\,\\mathrm{e}^{\\lambda} = \\lambda^2 . \\] So the correct answer here is \\(\\lambda^2 = 4.5^2 = 20.25\\).\nI found my Markov chain always gets it right to the nearest integer and often gets it right to 1 decimal place (either 20.2 or 20.3). It is not quite as accurate basic Monte Carlo would be (which almost always gets 1 decimal place with a million samples, and occasionally two decimal places), but it is pretty good.\n\n\n\n\n4.     For \\(-1 &lt; \\alpha &lt; 1\\), consider the Markov chain on \\(\\mathcal S = \\mathbb R\\) given by \\(X_{i+1} = \\alpha X_i + Z_i\\), where the \\(Z_i \\sim \\operatorname{N}(0, 1)\\) are IID standard normals. (Students who have studied time series will recognise this as an AR(1) autoregressive process.)\n\n(a)  Write down the transition density \\(p(x, y)\\) for this Markov chain.\n\nSolution. Given \\(X_i = x\\), we have that \\(X_{i+1} = \\alpha x + \\operatorname{N}(0, 1) = \\operatorname{N}(\\alpha x, 1)\\). So the transition density is \\[ p(x, y) = \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-(y - \\alpha x)^2/2} . \\]\n\n\n\n(b)  Find a stationary distribution for this Markov chain.\n\nSolution. There are some long-winded ways to do this. But I would start by guessing there’s a pretty good chance the stationary distribution will be normally distributed. So let’s guess there’s a stationary distribution \\(\\operatorname{N}(\\mu, \\sigma^2)\\), and see if we can find \\(\\mu\\) and \\(\\sigma^2\\) that satisfy this. (If we can’t, then our guess was wrong, and we’ll have to go back to the drawing board.)\nIf \\(X \\sim \\operatorname{N}(\\mu, \\sigma^2)\\), then \\(\\alpha X \\sim \\operatorname{N}(\\alpha\\mu, \\alpha^2\\sigma^2)\\) and \\(\\alpha X + Z \\sim \\operatorname{N}(\\alpha\\mu, \\alpha^2\\sigma^2 + 1)\\). To have \\(X\\) with the same distribution as \\(\\alpha X + Z\\), we need \\(\\operatorname{N}(\\mu, \\sigma^2) = \\operatorname{N}(\\alpha\\mu, \\alpha^2\\sigma^2 + 1)\\).\nThis will hold true if the parameters are the same on bother sides. Looking at the mean parameters, we need \\(\\mu = \\alpha\\mu\\), which forces \\(\\mu = 0\\). Looking at the variance parameters, we need \\(\\sigma^2 = \\alpha^2 \\sigma^2 + 1\\), so \\(\\sigma^2 = 1/(1 - \\alpha^2)\\). These satisfy the equation. So a stationary distribution is \\[ \\operatorname{N}\\bigg(0, \\,\\frac{1}{1 - \\alpha^2}\\bigg) . \\]\n\n\n\n\n5.     Let \\(Y \\sim \\operatorname{N}(-3, 1)\\) and \\(Z \\sim \\operatorname{N}(3,1)\\). Let \\(X\\) be a mixture distribution that equals \\(Y\\) with probability \\(\\frac12\\) and equals \\(Z\\) with probability \\(\\frac12\\); in other words, if \\(f\\) is the PDF of \\(Y\\) and \\(g\\) is the PDF of \\(Z\\), then \\(\\pi(x) = \\frac12 f(x) + \\frac12 g(x)\\) is the PDF of \\(X\\).\n\n(a)  Draw a graph of \\(\\pi\\).\n\nSolution.\n\npdf &lt;- function(x) 0.5 * dnorm(x, -3, 1) + 0.5 * dnorm(x, 3, 1)\ncurve(pdf, from = -7, to = 7, xlim = c(-6, 6), n = 1001, col = \"blue\", lwd = 2)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\n\n\n(b)  Your intention is to sample (approximately) from \\(X\\) using the random walk Metropolis algorithm on the state space \\(\\mathcal S = \\mathbb R\\). Explain why this could be tricky, and why a good choice of the typical stepsize \\(\\sigma\\) will be particularly important.\n\nSolution. We have a “bimodal” distribution, with two humps. If we’re not carefully, our random walk might get stuck for a long time in just one of the humps, without exploring the other one, which give an output that is not representative of the full distribution. In particular, if the typical step size \\(\\sigma\\) is too small, getting from one hump to the other will require a long and unlikely trek through the “low probability zone” between the two humps.\nHence it will be vitally important to make sure that \\(\\sigma\\) is big enough that jumps between the two humps can happen reasonably often. (Although, as ever, having the typical step size \\(\\sigma\\) too big brings it’s own problems of rejecting moves that overshoot the other hump.)\n\n\n\n(c)  Write an R program that will run the random walk Metropolis algorithm with target distribution \\(\\pi\\). Experiment with different values of the typical step size \\(\\sigma\\). What did you discover, and what value of \\(\\sigma\\) did you find most appropriate?\n\nSolution.\n\ntwohump &lt;- function(n, stepsize, initial) {\n  MC &lt;- rep(0, n)\n  MC[1] &lt;- initial\n  for (i in 1:(n - 1)) {\n    prop &lt;- MC[i] + rnorm(1, 0, stepsize)\n    if (runif(1) &lt; pdf(prop) / pdf(MC[i])) MC[i + 1] &lt;- prop\n    else MC[i + 1] &lt;- MC[i]\n  }\n  return(MC)\n}\n\nI’ll start with stepsize 0.5.\n\nset.seed(6)\nhist(twohump(1e5, 0.1, 0), breaks = 50, probability = TRUE)\n\n\n\n\n\n\n\n\nThis has gone very badly – I’ve spent much more time in the left hump than the right hump. I found in general that sometimes I got lucky and got balanced humps in the histogram, but on other occasions it was even more unbalanced than the one shown above.\nPerhaps better would be to set \\(\\sigma = 6\\), since that is the gap between the two peaks.\n\nhist(twohump(1e5, 6, 0), breaks = 50)\n\n\n\n\n\n\n\n\nThat looks better – the humps are pretty balanced now (although not perfectly balanced each time).\nTo be more rigorous, we could look at the autocorrelation, specifically \\(1 + 2 \\sum_{k=0}^{\\infty} \\rho(k)\\), which we saw was an important figure in Lecture 22.\n\ntrials &lt;- c(0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000)\nresults &lt;- rep(0,12)\nfor (i in 1:12) {\n  MC &lt;- twohump(1e5, trials[i], 0)\n  results[i] &lt;- 1 + 2 * sum(acf(MC, lag.max = 1000, plot = FALSE)$acf)\n}\nplot(trials, results, xlab = \"stepsize\", ylab = \"autocorrelation calculation\", log = \"x\")\n\n\n\n\n\n\n\n\nThis suggests that stepsizes around about the size of 10 is the right order of magnitude. Let’s look a bit closer.\n\ntrials &lt;- 2*(1:15)\nresults &lt;- rep(0,15)\nfor (i in 1:15) {\n  MC &lt;- twohump(1e5, trials[i], 0)\n  results[i] &lt;- 1 + 2 * sum(acf(MC, lag.max = 1000, plot = FALSE)$acf)\n}\nplot(trials, results, xlab = \"stepsize\", ylab = \"autocorrelation calculation\")\n\n\n\n\n\n\n\n\nThis is a rather noisy picture – I’d have to do more and longer experiments to find out more. But I’d suggest something in the 4 to 10 range is probably best.\n\n\n\n(d) Estimate \\(\\mathbb EX\\) using your program. Comment on the accuracy of your estimation.\n\nThe estimate is simply\n\nmean(twohump(1e6, 6, 0))\n\n[1] -0.002129821\n\n\nThe answer should be 0. I find this is usually pretty accurate – I usually get 0.0 to one decimal place, and often 0.00 to two decimal places.\n\n\n\n\n6.     Consider the Metropolis–Hastings algorithm on the state space \\(\\mathbb R\\) with target density \\[ \\pi(x) \\propto \\sin^2(x) \\,\\exp(-|x|) . \\] Each of the following proposal methods gives a formula for the proposed next state \\(Y_{i+1}\\) given the current state \\(X_{i}\\). For each proposal method, write down the proposal density \\(r(x,y)\\) and calculate the acceptance probability \\(\\alpha(x, y)\\).\n\n(a)  \\(Y_{i+1} = X_i + Z_i\\), where \\(Z_i \\sim \\operatorname{N}(0,1)\\) are IID.\n\nSolution \\({\\displaystyle r(x, y) = \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-(y-x)^2/2}}\\).\nBecause this is symmetric, the acceptance probability is \\[ \\begin{align}\n\\alpha(x, y) = \\min \\left\\{ \\frac{\\pi(y)}{\\pi(x)},\\,1 \\right\\} &= \\min \\left\\{ \\frac{\\sin^2(y) \\,\\exp(-|y|)}{\\sin^2(x) \\,\\exp(-|x|)},\\,1 \\right\\} \\\\ &= \\min \\left\\{ \\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp(|x|-|y|),\\,1 \\right\\} .\n\\end{align} \\]\n\n\n\n(b)  \\(Y_{i+1} = X_i + Z_i\\), where \\(Z_i \\sim \\operatorname{U}[-1,2]\\) are IID.\n\nSolution \\(r(x, y) = \\frac13\\) is \\(x-1 \\leq y \\leq x+2\\).\nNow the acceptance probability. If \\(x - 1 \\leq y \\leq x + 1\\), then \\(r(x,y) = r(y,x) = \\frac13\\). In that case, \\[ \\alpha(x, y) =  \\min \\left\\{ \\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp(|x|-|y|),\\,1 \\right\\}  \\] again. On the other hand, if \\(x + 1 &lt; y \\leq x + 2\\), then \\(r(x, y) = \\frac13\\) while \\(r(y,x) = 0\\). In this case, \\(\\alpha(x, y) = 0\\).\n\n\n\n(c)  \\(Y_{i+1} \\sim \\operatorname{N}(0,1)\\), independent of \\(X_i\\).\n\nSolutions. In this case, \\(r(x, y)\\) does not depend on \\(x\\) at all. We have simply \\[ r(x, y) = \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-y^2/2} . \\] \\[ \\begin{align}\n\\alpha(x, y) &=  \\min \\left\\{ \\frac{\\mathrm{e}^{-y^2/2} \\,\\pi(y)}{\\mathrm{e}^{-x^2/2} \\, \\pi(x)} \\right\\} \\\\\n&=  \\min \\left\\{ \\frac{\\mathrm{e}^{-y^2/2}}{\\mathrm{e}^{-x^2/2}}\\,\\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp(|x|-|y|),\\,1 \\right\\} \\\\\n&= \\min \\left\\{ \\frac{\\sin^2(y)}{\\sin^2(x)} \\,\\exp\\big(x^2/2 + |x|-y^2/2 - |y|\\big),\\,1 \\right\\}\n\\end{align} \\]",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "problems/solutions.html#P5-sols",
    "href": "problems/solutions.html#P5-sols",
    "title": "Solutions",
    "section": "Problem Sheet 5",
    "text": "Problem Sheet 5\n\n\\[\\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}}\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}}\\]\n\n\n1.     The Gamma distribution \\(X \\sim \\Gamma(m, \\lambda)\\) has PDF \\[ f(x) = \\frac{\\lambda^m}{(m-1)!} \\,x^{m-1}\\,\\mathrm{e}^{-\\lambda x} . \\] [Correction: There was a typo in this PDF earlier.] (This PDF can be evaluated with the dgamma() function in R.) The time between eruptions of a volcano, measured in years, is modelled as \\(X \\sim \\Gamma(5, \\theta)\\), IID over eruptions, where \\(\\theta\\) is unknown. A scientist beliefs about \\(\\theta\\) are represented in a prior distribution \\(\\theta \\sim \\operatorname{Exp}(6)\\).\nThe following times between eruptions are available in the historical record:\n       56.55,  2.57, 29.97, 10.27,  4.32, 17.91, 51.98,  7.06, 11.40, 54.80\n            Sample from the posterior distribution for \\(\\theta\\), using the random walk Metropolis algorithm. Explain how you chose the stepsize for your algorithm and how you checked it was a good choice.\nDraw a histogram of the posterior distribution, and comment on how it differs from the prior.\n\nSolution. The posterior distribution is \\[ \\pi(\\theta \\mid \\mathbf x) \\propto 6\\mathrm{e}^{-6\\theta} \\times \\prod_{i=1}^{10} x_i^{4} \\, \\mathrm{e}^{-\\theta x_i} . \\]\nThe code for the Metropolis random walk algorithm will be the following.\n\nx &lt;- c(56.55,  2.57, 29.97, 10.27,  4.32,\n       17.91, 51.98,  7.06, 11.40, 54.80)\n\nprior &lt;- function(theta) dexp(theta, 6)\nlike  &lt;- function(theta) prod(dgamma(x, 5, theta))\npost  &lt;- function(theta) prior(theta) * like(theta)\n\naccept &lt;- function(curr, prop) post(prop) / post(curr)\ninitial &lt;- 1\nn &lt;- 1e5\nsigma &lt;- 0.25\n\nMRW &lt;- rep(0, n)\nMRW[1] &lt;- initial\nfor (i in 1:(n - 1)) {\n  prop &lt;- MRW[i] + rnorm(1, 0, sigma)\n  if (prop &lt; 0)                             MRW[i + 1] &lt;- MRW[i]\n  else if (runif(1) &lt; accept(MRW[i], prop)) MRW[i + 1] &lt;- prop\n  else                                      MRW[i + 1] &lt;- MRW[i]\n}\n\nhist(MRW[-(1:99)], probability = TRUE, xlim = c(0, 0.5))\ncurve(prior, add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nA little experimentation suggested \\(\\sigma = 0.2\\) worked about right – \\(\\sigma = 0.5\\) rejected too many changes. With my initial start point of 1, I found a short burn-in period was necessary, so my histogram starts with the 100th step of the random walk. (I could have just started from 0.2 instead, I suppose.)\nWe see that while the prior though any number between 0 and \\(\\tfrac12\\), maybe even more, was plausible for \\(\\theta\\), we can see the prior has become concentrated in the interval \\([0.11, 0.32]\\) or so, with values around 0.2 the most common. The posterior variance is much smaller than the prior, so we have gained certainty from the results.\n\n\n\n2.     [2019 exam, Question 4]\n\n(a)   Introducing any notation you use, state the Metropolis–Hastings algorithm for discrete state space, and explain the purpose of this algorithm.\n\nSolution. The Metropolis–Hastings algorithm is a method to sample from a distribution \\(\\pi\\) on a discrete space \\(\\mathcal S\\) by setting up a Markov chain \\((X_i)\\) on \\(\\mathcal S\\) with stationary distribution \\(\\pi\\).\nSuppose the Markov chain is at \\(X_i = x\\). The Metropolis–Hastings algorithm proposes a move to \\(y\\) with probability \\(r(x, y)\\), where \\(\\mathsf R = (r(x, y))\\) is the transition matrix for an irreducible Markov chain on \\(\\mathcal S\\). This move is accepted with probability \\[ \\alpha(x, y) = \\min \\left\\{ \\frac{\\pi(y)\\,r(y,x)}{\\pi(x)\\,r(x,y)},\\, 1\\right\\} , \\] meaning that \\(X_{i+1} = y\\), or rejected otherwise, meaning that \\(X_{i+1} = X_i = x\\).\nAfter being run for a large number of steps (possibly with a burn-in period), the values \\(X_i\\) should be approximately distributed like \\(\\pi\\).\n\n\n\n(b)   Using the Metropolis–Hastings algorithm, find a Markov chain with state space \\(\\mathbb N = \\{1, 2, 3, \\dots\\}\\) that has stationary distribution \\(\\mathbb P(X_n = x) = 2^{-x}\\) for \\(x \\in \\mathbb N\\).\n\nSolution. Although this isn’t the only way, I will use \\(\\mathsf R\\) to be the transition matrix of the simple symmetric random walk on \\(\\mathbf Z\\). Because this is symmetric, the acceptance probabilities are \\[ \\begin{align}\n\\alpha(x, x+1) &= \\min \\left\\{ \\frac{\\pi(x+1)}{\\pi(x)},\\, 1 \\right\\} = \\min\\big\\{\\tfrac12,\\, 1\\big\\} = \\tfrac 12 \\\\\n\\alpha(x, x-1) &= \\min \\left\\{ \\frac{\\pi(x-1)}{\\pi(x)},\\, 1 \\right\\} = \\min\\{2,\\, 1\\} = 1,\n\\end{align} \\] except that \\(\\alpha(1, 0) = 1\\).\nI’ll choose the starting point \\(X = 1\\). From here, the Markov chain repeatedly proposes a move up 1 with probability \\(\\tfrac12\\), which is accepted with probability \\(\\tfrac12\\), or a move down 1 with probability \\(\\tfrac12\\), which is always accepted – except for a move from 1 down to 0 which is always rejected.\n\n\n\n(c)   In a Bayesian setting, assume we have observations \\(Z_1, X_2, \\dots, Z_n\\) from an \\(\\operatorname{N}(0, \\sigma^2)\\) distribution, where the prior distribution for \\(\\sigma^2\\) is \\(\\operatorname{Exp}(1)\\). Using the Metropolis–Hastings algorithm, describe in detail a method for sampling from the posterior distribution of \\(\\sigma^2\\).\n\nSolution The posterior distribution is given by \\[ \\begin{align}\n\\pi(\\sigma^2 \\mid \\mathbf Z) &\\propto \\pi(\\sigma^2) \\prod_{i=1}^n f(z_i \\mid \\sigma^2) \\\\\n  &= \\mathrm{e}^{-\\sigma^2} \\prod_{i=1}^n \\frac{1}{\\sqrt{\\sigma^2}} \\,\\exp\\left(- \\frac{Z_i^2}{2\\sigma^2} \\right) \\\\\n  &=  \\mathrm{e}^{-\\sigma^2} \\,\\sigma^{-n/2} \\,\\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n Z_i^2 \\right) .\n\\end{align} \\]\nI will choose as the Markov chain a symmetric Gaussian random walk on \\(\\mathbb R\\), meaning that the Markov chain is \\(X_{i+1} = X_i + \\mathrm{N}(0, s^2)\\). (The special case is known as the random walk Metropolis algorithm in continuous space.) Because this is symmetric, the acceptance probability is \\[ \\alpha(x, y) = \\min \\left\\{ \\frac{ \\pi(x \\mid \\mathbf Z)}{\\pi(y \\mid \\mathbf Z)},\\,1\\right\\} , \\] for \\(y \\geq 0\\), where the constant of proportionality in the \\(\\pi\\)s cancel out, of \\(\\alpha(x, y) = 0\\) for \\(y &lt; 0\\).\nI would suggest starting from \\(X_1\\) between \\(1\\) (the prior variance) and the sample variance of the \\(Z_i\\) (the posterior variance in the large sample limit \\(n \\to \\infty\\)). I would suggest choosing the step size \\(s\\) by experiment, with smaller \\(s\\) for large \\(n\\), and vice versa.\n\n\n\n(d)   Let the set \\(A \\subset \\mathbb R^2\\) be given by \\[\nA = \\big([0,3]\\times[0,1]\\big) \\cup \\big([0,1]\\times [4,5]\\big)\n\\cup \\big([4,5]\\times[0,1]\\big) \\cup \\big([4,5] \\times [3,5]\\big) .\n\\] Consider the Metropolis–Hastings algorithm on \\(\\mathbb R^2\\) with target density \\[ \\pi(\\mathbf x) = \\begin{cases} \\frac17 & \\text{if }\\mathbf x \\in A \\\\\n0 & \\text{otherwise} \\end{cases} \\] for all \\(x \\in \\mathbb R^2\\), and where the proposals \\(\\mathbf Y_{i+1}\\) are chosen uniformly distributed on a disk of radius \\(r\\) around the previous state – that is, \\[ Y_{j+1} \\sim \\operatorname{U} \\Big(\\big\\{ \\mathbf y \\in \\mathbb R^2 : |\\mathbf y - \\mathbf X_i | \\leq r \\big\\}\\Big) . \\] Assume the algorithm starts at \\(\\mathbf X_1 = (0.5, 0.5)\\).\ni.      For every \\(r &gt; 0\\), determine the stationary distribution of the resulting Markov chain.\nii.    For what values of \\(r\\) does the algorithm work correctly?\n\nSolution. To see what’s going on where, we need to draw a picture of the set \\(A\\).\nThe question is: for what \\(r\\) will it be possible to jump between the different blocks. We start from \\((0.5, 0.5)\\) which is in the bottom-left block. To jump from bottom-left to bottom-right, we need \\(r &gt; 1\\). To jump from bottom-right to top-right, we need \\(r &gt; 2\\). To jump from bottom-left to top-left, we need \\(r &gt; 3\\).\n(i) The distribution is uniform, so as long as the Markov chain can reach an area, the stationary distribution will be uniform there. So the stationary distribution is uniform on the following sets:\n\n\\(0 &lt; r \\leq 1\\): \\(\\big([0,3]\\times[0,1]\\big)\\)\n\\(1 &lt; r \\leq 2\\): \\(\\big([0,3]\\times[0,1]\\big) \\cup \\big([4,5]\\times[0,1]\\big)\\)\n\\(2 &lt; r \\leq 3\\): \\(\\big([0,3]\\times[0,1]\\big) \\cup \\big([4,5]\\times[0,1]\\big) \\cup \\big([4,5] \\times [3,5]\\big)\\)\n\\(r &gt; 3\\): \\(A\\)\n\n(If \\(r\\) is only just above the boundary, convergence to the stationary distribution may be extremely slow.)\n(ii) The stationary distribution is the desired distribution for \\(r &gt; 3\\), which is the values for which the algorithm technically “works”, as \\(n \\to \\infty\\). To work “well”, \\(r\\) will need to be comfortably bigger than 3 (so that movement to and from the top-left is reasonably common) while not being so huge that most steps are to ridiculously far away.\n\n\n\n\n3.     [2017 exam, Question 5]\n\n(a)   Give the definition of a Markov chain with state space \\(\\mathcal S\\).\nLet \\((Z_i)\\) be a sequence of IID random variables whose distribution is known. Consider the stochastic processes \\((U_i)\\), \\((V_i)\\), \\((W_i)\\) defined by \\[ \\begin{align}\nU_i &= Z_1 + Z_2 + \\cdots + Z_i \\\\\nV_i &= \\frac{1}{i} (Z_1 + Z_2 + \\cdots + Z_i) \\\\\nW_i &= \\max \\{Z_j : 1 \\leq j \\leq i-1\\} + Z_i\n\\end{align} \\] for \\(i = 1, 2, \\dots\\). Which of these processes are Markov chains? Justify your answers.\n\n\n(b)   Defining any notation you use, state the random walk Metropolis algorithm. What is the purpose of this algorithm?\n\n\n(c)   Use the Metropolis–Hastings algorithm to define a Markov chain \\((X_n)\\) on \\(\\mathcal S = \\{0, 1, 2, \\dots \\}\\) whose stationary distribution is the Poisson distribution \\(\\pi(x) = \\mathrm{e}^{-\\lambda} \\lambda^x/x!\\).\n\n\n(d)   Let \\((X_i)\\) be a Markov chain with values in \\(\\mathbb R\\) and stationary distribution \\(\\pi\\). Consider the estimator \\[\\widehat{\\theta}_n^{\\mathrm{MCMC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i)  \\] for \\(\\theta = \\operatorname{\\mathbb E}(\\phi(X))\\) for \\(X\\) having PDF \\(\\pi\\). Assuming \\(X_1\\) has PDF \\(\\pi\\), derive the result \\[ \\operatorname{Var} \\big(\\widehat{\\theta}_n^{\\mathrm{MCMC}}\\big) \\approx \\frac{\\operatorname{Var}(\\phi(X))}{n} \\left(1 + 2 \\sum_{i=2}^\\infty \\operatorname{Corr}\\big(\\phi(X_1),\\phi(X_i)\\big) \\right) . \\]\n\n\n4.     Let \\(\\mathbf X = (X_1, \\dots, X_n)\\) be IID samples from a random variable \\(X\\) with cumulative distribution function \\(F\\). Let \\(F^*\\) be the empirical cumulative distribution function of the samples \\(\\mathbf X\\).\n\n(a)   For fixed \\(x\\), show that \\(\\operatorname{\\mathbb E}F^*(x) = F(x)\\).\n\n\n(a) For fixed \\(x\\), show that \\(\\operatorname{\\mathbb E}F^*(x) = F(x)\\).\n\nSolution. We have \\[ F^*(x) = \\frac{1}{n} \\sum_{i=1}^n\\mathbb{I}_{(-\\infty, x]}(X_i) . \\] So \\[ \\begin{multline}\n\\operatorname{\\mathbb E}F^*(x) = \\mathbb E \\left( \\frac{1}{n} \\sum_{i=1}^n\\mathbb{I}_{(-\\infty, x]}(X_i) \\right) = \\frac{1}{n} \\sum_{i=1}^n \\operatorname{\\mathbb E} \\mathbb{I}_{(-\\infty, x]}(X_i) \\\\ = \\frac{1}{n}\\,\\sum_{i=1}^n \\mathbb P\\big(X_i \\in (\\infty, x]\\big) = \\frac{1}{n}\\,n\\,\\mathbb P\\big(X_i \\in (\\infty, x]\\big) = F(x)\n\\end{multline} \\]\n\n\n\n(b) For fixed \\(x\\), show that \\(\\operatorname{Var}\\big(F^*(x)\\big) = \\displaystyle\\frac{1}{n}F(x)\\big(1 - F(x)\\big)\\).\n\nSolution. With the same notation as part (a), \\[ \\begin{multline}\n\\operatorname{Var}\\big(F^*(x)\\big) = \\operatorname{Var} \\left( \\frac{1}{n} \\sum_{i=1}^n\\mathbb{I}_{(-\\infty, x]}(X_i) \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}\\big(\\mathbb{I}_{(-\\infty, x]}(X_i)\\big) \\\\ = \\frac{1}{n^2}\\,\\sum_{i=1}^n F(x) \\big(1 - F(x)\\big) = \\frac{1}{n^2}\\,n\\,F(x) \\big(1 - F(x)\\big) = \\frac{1}{n} F(x) \\big(1 - F(x)\\big)\n\\end{multline} \\]\n\n\n\n(c) For fixed \\(x\\) and \\(y\\) with \\(y &gt; x\\), find \\(\\operatorname{Cov}\\big(F^*(y), F^*(x)\\big)\\). (c) For fixed \\(x\\) and \\(y\\) with \\(x \\leq y\\), find \\(\\operatorname{Cov}\\big(F^*(y), F^*(x)\\big)\\).\n\nSolution. Using the same notation again, \\[ \\begin{align}\n\\operatorname{Cov}\\big(F(x), F(y)\\big)\n&= \\operatorname{Cov} \\left( \\frac1n\\sum_{i=1}^n \\mathbb{I}_{(-\\infty, x]}(X_i) ,  \\frac1n\\sum_{i=1}^n \\mathbb{I}_{(-\\infty, y]}(X_i) \\right) \\\\\n&= \\frac{1}{n^2} \\left( \\sum_{i=1}^n \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_i)\\big) + 2 \\sum_{i&lt;j} \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_j)\\big) \\right) \\\\\n&= \\frac{1}{n^2} \\left(n \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_i)\\big) + 0\\right) \\\\\n&= \\frac{1}{n} \\operatorname{Cov} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i), \\mathbb{I}_{(-\\infty, y]}(X_i)\\big)\n\\end{align} .\\]\nSo we need to find that covariance. In particular, we need \\[\\operatorname{\\mathbb E} \\big( \\mathbb{I}_{(-\\infty, x]}(X_i) \\mathbb{I}_{(-\\infty, y]}(X_i)\\big) = F(x),\\] since the produce of indicator function is 1 if and only if both indicators are 1, which is if \\(X_i \\leq x\\) and \\(X_i \\leq y\\). Since \\(x\\) is the smaller, this is \\(F(x)\\).\nHence we have \\[ \\operatorname{Cov}\\big(F^*(y), F^*(x)\\big) = \\frac{1}{n} \\big(F(x) - F(x)F(y)\\big) = \\frac{1}{n}\\,F(x) \\big(1 - F(y)\\big) . \\]\n\n\n\n@@ -201,13 +235,52 @@ $$ (_n^{}) \n\n(a) Explain why the plug-in estimator is \\(\\theta^* = \\max \\{X_j : j = 1, \\dots, 24\\}\\), and find the \\(\\theta^*\\) for this data.\n\nSolution. This is basically just the definition of the plug-in estimator. (Not sure why I set that as part of the question…) The maximum value of \\(X_i\\) here is\n\nsamples &lt;- c(12.40,  2.99, 14.79,  3.59, 24.92,  4.11,\n             22.68, 28.30,  0.90,  8.84, 24.17, 31.31,\n             12.97, 12.43, 20.34, 14.75, 20.61, 10.93,\n             17.59, 30.88, 28.13,  0.83, 17.43, 20.78)\nplugin &lt;- max(samples)\nplugin\n\n[1] 31.31\n\n\nSo \\(\\theta^* = 31.31\\).\n\n\n\n(b) Use the bootstrap to estimate the bias of the plug-in estimator.\n\nSolution. The idea is to pick 24 samples with replacement and look at how the maximum of those differs from the overall maximum 31.31.\n\nboots &lt;- 1e5\nm &lt;- length(samples)\nbootests &lt;- rep(0, boots)\nfor (k in 1:boots) {\n  resample &lt;- sample(samples, m, replace = TRUE)\n  bootests[k] &lt;- max(resample)\n}\nbias &lt;- mean(bootests) - plugin\nbias\n\n[1] -0.5301925\n\n\n\n\n\n(c) Improve the plug-in estimator by using your estimation of the bias to (approximately) “de-bias” the plug-in estimate.\n\nSolution. We want to subtract the bias from the plugin estimator to debias it. In maths, this is \\(\\theta^* - (\\overline{T^*} - \\theta^*) = 2\\theta^* - \\overline{T^*}\\). In code, this is\n\nplugin - bias\n\n[1] 31.84019",
    "crumbs": [
      "Solutions"
    ]
  }
]