[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH5835M Statistical Computing",
    "section": "",
    "text": "About MATH5835",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#organisation-of-math5835",
    "href": "index.html#organisation-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Organisation of MATH5835",
    "text": "Organisation of MATH5835\nThis module is MATH5835M Statistical Computing.\nThis module lasts for 11 weeks from 29 September to 12 December 2025. The exam will take place sometime between 12 and 23 January 2026.\nThe module leader, the lecturer, and the main author of these notes is Dr Matthew Aldridge. (You can call me “Matt”, “Matthew”, or “Dr Aldridge”, pronounced “old-ridge”.) My email address is m.aldridge@leeds.ac.uk, although I much prefer questions in person at office hours (see below) rather than by email.\nThe HTML webpage is the best way to view the course material. There is also a PDF version, although I have been much less careful about the presentation of this material, and it does not include the problem sheets.\n\nLectures\nThe main way you will learn new material for this module is by attending lectures. There are three lectures per week:\n\nMondays at 1400\nThursdays at 1200\nFridays at 1000\n\nall in in Roger Stevens LT 14.\nI recommend taking your own notes during the lecture. I will put brief summary notes from the lectures on this website, but they will not reflect all the details I say out loud and write on the whiteboard. Lectures will go through material quite quickly and the material may be quite difficult, so it’s likely you’ll want to spend time reading through your notes after the lecture. Lectures should be recorded on the lecture capture system; I find it very difficult to read the whiteboard in these videos, but if you unavoidably miss a lecture, for example due to illness, you may find they are better than nothing.\nIn Weeks 3, 5, 7, 9 and 11, the Thursday lecture will operate as a “problems class” – see more on this below.\nAttendance at lectures in compulsory. You should record your attendance using the UniLeeds app and the QR code on the wall in the 15 minutes before the lecture or the 15 minutes after the lecture (but not during the lecture).\n\n\nProblem sheets and problem classes\nMathematics and statistics are “doing” subjects! To help you learn material for the module and to help you prepare for the exam, I will provide 5 unassessed problem sheets. These are for you to work through in your own time to help you learn; they are not formally assessed. You are welcome to discuss work on the problem sheets with colleagues and friends, although my recommendation would be to write-up your “last, best” attempt neatly by yourself.\nThere will be an optional opportunity to submit one or two questions from the problem sheet to me in advance of the problems class for some brief informal feedback on your work. See the problem sheets for details.\nYou should work through each problem sheet in preparation for the problems class in the Thursday lecture of Week 3, 5, 7, 9 and 11. In the problems class, you should be ready to explain your answers to questions you managed to solve, discuss your progress on questions you partially solved, and ask for help on questions you got stuck on.\nYou can also ask for extra help or feedback at office hours (see below).\n\n\nCoursework\nThere will be one piece of assessed coursework, which will make up 20% of your module mark. You can read more about the coursework here.\nThe coursework will be in the form of a worksheet. The worksheet will have some questions, mostly computational but also mathematical, and you will have to write a report containing your answers and computations.\nThe assessed coursework will be introduced in the computer practical sessions in Week 9.\nThe deadline for the coursework will be the penultimate day of the Autumn term, Thursday 12 December  at 1400. Feedback and marks will be returned on Monday 13 January, the first day of the Spring term.\n\n\nOffice hours\nI will run a n optional “office hours” drop-in session each week for feedback and consultation. You can come along if you want to talk to me about anything on the module, including if you’d like more feedback on your attempts at problem sheet questions. (For extremely short queries, you can approach me before or after lectures, but my response will often be: “Come to my office hours, and we can discuss it there!”)\nOffice hours will happen on Thursdays from 1300 to 1400 – so directly after the Thursday lecture / problems class – in my office, which is EC Stoner 9.10n in “Maths Research Deck” area on the 9th floor of the EC Stoner building. (One way to the Maths Research Deck is via the doors directly opposite the main entrance to the School of Mathematics; you can also get there from Staircase 1 on the Level 10 “red route” through EC Stoner, next to the Maths Satellite.) If you cannot make this time, contact me for an alternative arrangement.\n\n\nExam\nThere will be one exam, which will make up 80% of your module mark.\nThe exam will be in the January 2026 exam period (12–23 January); the date and time will be announced in December. The exam will be in person and on campus.\nThe exam will last 2 hours and 30 minutes. The exam will consist of 4 questions, all compulsory. You will be allowed to use a permitted calculator in the exam.",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "index.html#content-of-math5835",
    "href": "index.html#content-of-math5835",
    "title": "MATH5835M Statistical Computing",
    "section": "Content of MATH5835",
    "text": "Content of MATH5835\n\nNecessary background\nI recommend that students should have completed at least two undergraduate level courses in probability or statistics – although confidence and proficiency in basic material is more important than very deep knowledge of more complicated topics.\nFor Leeds undergraduates, MATH2715 Statistical Methods is an official prerequisite (please get in touch with me if you are/were a Leeds undergraduate and have not taken MATH2715), although confidence and proficiency in the more basic material of MATH1710 & MATH1712 Probability and Statistics 1 & 2 is probably more important.\nSome knowledge I will assume:\n\nProbability: Basic rules of probability; random variables, both continuous and discrete; “famous” distributions (especially the normal distribution and the continuous uniform distribution); expectation, variance, covariance, correlation; law of large numbers and central limit theorem.\nStatistics: Estimation of parameters; bias and error; sample mean and sample variance\n\nThis module will also include an material on Markov chains. I won’t assume any pre-existing knowledge of this, and I will introduce all new material we need, but students who have studied Markov chains before (for example in the Leeds module MATH2750 Introduction to Markov Processes) may find a couple of lectures here are merely a reminder of things they already know.\nThe lectures will include examples using the R program language. The coursework and problem sheets will require use of R. The exam, while just a “pencil and paper” exam, will require understanding and writing short portions of R code. We will assume basic R capability – that you can enter R commands, store R objects using the &lt;- assignment, and perform basic arithmetic with numbers and vectors. Other concepts will be introduced as necessary. If you want to use R on your own device, I recommend downloading (if you have not already) the R programming language and the program RStudio. (These lecture notes were written in R using RStudio.)\n\n\nSyllabus\nWe plan to cover the following topics in the module:\n\nMonte Carlo estimation: definition and examples; bias and error; variance reduction techniques: control variates, antithetic variables, importance sampling. [9 lectures]\nRandom number generation: pseudo-random number generation using linear congruential generators; inverse transform method; rejection sampling [7 lectures]\nMarkov chain Monte Carlo (MCMC): [7 lectures]\n\nIntroduction to Markov chains in discrete and continuous space\nMetropolis–Hastings algorithm: definition; examples; MCMC in practice; MCMC for Bayesian statistics\n\nResampling methods: Empirical distribution; plug-in estimation; bootstrap statistics; bootstrap estimation [4 lectures]\nFrequently-asked questions [1 lecture]\n\nTogether with the 5 problems classes, this makes 33 lectures.\n\n\nBook\nThe following book is strongly recommended for the module:\n\nJ Voss, An Introduction to Statistical Computing: A simulation-based approach, Wiley Series in Computational Statistics, Wiley, 2014\n\nThe library has electronic access to this book (and two paper copies).\nDr Voss is a lecturer in the School of Mathematics and the University of Leeds, and has taught MATH5835 many times. An Introduction to Statistical Computing grew out of his lecture notes for this module, so the book is ideally suited for this module. My lectures will follow this book closely – specifically:\n\nMonte Carlo estimation: Sections 3.1–3.3\nRandom number generation: Sections 1.1–1.4\nMarkov chain Monte Carlo: Section 2.3 and Sections 4.1–4.3\nBootstrap: Section 5.2\n\nFor a second look at material, for preparatory reading, for optional extended reading, or for extra exercises, this book comes with my highest recommendation!",
    "crumbs": [
      "About MATH5835"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html",
    "href": "lectures/L01-mc-intro.html",
    "title": "1  Introduction to Monte Carlo",
    "section": "",
    "text": "1.1 What is statistical computing?\nToday, we’ll start the first main topic of the module, which is called “Monte Carlo estimation”. But first, a bit about the subject as a whole.\n“Statistical computing” – or “computational statistics” – refers to the branch of statistics that involves not attacking statistical problems merely with a pencil and paper, but rather by combining human ingenuity with the immense calculating powers of computers.\nOne of the big ideas here is simulation. Simulation is the idea that we can understand the properties of a random model not by cleverly working out the properties using theory – this is usually impossible for anything but the simplest “toy models” – but rather by running the model many times on a computer. From these many simulations, we can observe and measure things like the typical (or “expected”) behaviour, the spread (or “variance”) of the behaviour, and other things. This concept of simulation is at the heart of the module MATH5835M Statistical Computing.\nIn particular, we will look at Monte Carlo estimation. Monte Carlo is about estimating a parameter, expectation or probability related to a random variable by taking many samples of that random variable, then computing a relevant sample mean from those samples. We will study Monte Carlo in its standard “basic” form, then look at ways we can make Monte Carlo estimation more accurate (Lectures 1–9).\nTo run a simulation – for example, when performing Monte Carlo estimation – one needs random numbers with the correct distribution. Random number generation (Lectures 10–16) will be an important part of this module. We will look first at how to generate randomness of any sort, and then how to manipulate that randomness into the shape of the distributions we want.\nSometimes, it’s not possible to generate perfectly independent samples from exactly the distribution you want. But we can use the output of a process called a “Markov chain” to get “fairly independent” samples from nearly the distribution we want. When we perform Monte Carlo estimation with the output of a Markov chain, this is called Markov chain Monte Carlo (MCMC) (Lectures 17–23). MCMC has become a vital part of modern Bayesian statistical analysis.\nThe final section of the module is about dealing with data. Choosing a random piece of data from a given dataset is a lot like generating a random number from a given distribution, and similar Monte Carlo estimation ideas can be used to find out about that data. We think of a dataset as being a sample from a population, and sampling again from that dataset is known as resampling (Lecture 24–27). The most important method of finding out about a population by using resampling from a dataset is called the “bootstrap”, and we will study the bootstrap in detail.\nMATH5835M Statistical Computing is a mathematics module that will concentrate on the mathematical ideas that underpin statistical computing. It is not a programming module that will go deeply into the practical issues of the most efficient possible coding of the algorithms we study. But we will want to investigate the behaviour of the methods we learn about and to explore their properties, so will be computer programming to help us do that. We will be using the statistical programming language R, (although one could just as easily have used Python or other similar languages). As my PhD supervisor often told me: “You don’t really understand a mathematical algorithm until you’ve coded it up yourself.”",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html#what-is-monte-carlo-estimation",
    "href": "lectures/L01-mc-intro.html#what-is-monte-carlo-estimation",
    "title": "1  Introduction to Monte Carlo",
    "section": "1.2 What is Monte Carlo estimation?",
    "text": "1.2 What is Monte Carlo estimation?\nLet \\(X\\) be a random variable. We recall the expectation \\(\\Ex X\\) of \\(X\\). If \\(X\\) is discrete with probability mass function (PMF) \\(p\\), then the expectation of \\(X\\) is \\[ \\Ex X = \\sum_x x\\,p(x) ;\\] while if \\(X\\) is continuous with probability density function (PDF) \\(f\\), then the expectation is \\[ \\Ex X = \\int_{-\\infty}^{+\\infty} x\\,f(x)\\,\\mathrm{d}x . \\] More generally, the expectation of a function \\(\\phi\\) of \\(X\\) is \\[ \\Exg \\phi(X) = \\begin{cases} {\\displaystyle \\sum_x \\phi(x)\\,p(x)} & \\text{for $X$ discrete}\\\\ {\\displaystyle \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x}  & \\text{for $X$ continuous.} \\end{cases}\\] (This matches with the “plain” expectation when \\(\\phi(x) = x\\).)\nBut how do we actually calculate an expectation like one of these? If \\(X\\) is discrete and can only take a small, finite number of values, then we can simply add up the sum \\(\\sum_x \\phi(x)\\,p(x)\\). But otherwise, we just have to hope that \\(\\phi\\) and \\(p\\) or \\(f\\) are sufficiently “nice” that we can manage to work out the sum/integral using a pencil and paper (and our brain). But while this is often possible in the sort of “toy example” one comes across in maths or statistics lectures, this is very rare in “real life” problems.\nMonte Carlo estimation is the idea that we can get an approximate answer for \\(\\Ex X\\) or \\(\\Exg \\phi(X)\\) if we have access to lots of samples from \\(X\\). If we have access to \\(X_1, X_2 \\dots, X_n\\) , independent and identically distributed (IID) samples with the same distribution as \\(X\\), then we already know that the mean \\[ \\overline X = \\frac{1}{n}(X_1 + X_2 + \\cdots + X_n) = \\frac{1}{n} \\sum_{i=1}^n X_i \\] can be used to estimate the expectation \\(\\mathbb EX\\). We know that \\(\\overline X\\) is usually close to the expectation \\(\\Ex X\\), at least if if the number of samples \\(n\\) is large; this is justified by the “law of large numbers”, which says that \\(\\overline X \\to \\mathbb EX\\) as \\(n \\to \\infty\\).\nSimilarly, we can use \\[ \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] to estimate \\(\\Exg \\phi(X)\\). The law of large numbers again says that this estimate tends to the correct value \\(\\Exg \\phi(X)\\) as \\(n \\to \\infty\\).\nIn this module we will write that \\(X_1, X_2, \\dots, X_n\\) is a “random sample from \\(X\\)” to mean that \\(X_1, X_2, \\dots, X_n\\) are IID with the same distribution as \\(X\\).\n\nDefinition 1.1 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and write \\(\\theta = \\Exg\\phi(X)\\). Then the Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) of \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\).\n\nWhile general ideas for estimating using simulation go back a long time, the modern theory of Monte Carlo estimation was developed by the physicists Stanislaw Ulam and John von Neumann. Ulam (who was Polish) and von Neumann (who was Hungarian) moved to the US in the early 1940s to work on the Manhattan project to build the atomic bomb (as made famous by the film Oppenheimer). Later in the 1940s, they worked together in the Los Alamos National Laboratory continuing their research on nuclear physics generally and nuclear weapons more specifically, where they used simulations on early computers to help them numerically solve difficult mathematical and physical problems.\nThe name “Monte Carlo” was chosen because the use of randomness to solve such problems reminded them of gamblers in the casinos of Monte Carlo, Monaco. Ulam and von Neumann also worked closely with another colleague Nicholas Metropolis, whose work we will study later in this module.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L01-mc-intro.html#examples",
    "href": "lectures/L01-mc-intro.html#examples",
    "title": "1  Introduction to Monte Carlo",
    "section": "1.3 Examples",
    "text": "1.3 Examples\nLet’s see some simple examples of Monte Carlo estimation using R.\n\nExample 1.1 Let’s suppose we’ve forgotten the expectation of the exponential distribution \\(X \\sim \\operatorname{Exp}(2)\\) with rate 2. In this simple case, we could work out the answer using the PDF \\(f(x) = 2\\mathrm{e}^{-2x}\\) as\n\\[ \\mathbb E X = \\int_0^\\infty x\\,2\\mathrm{e}^{-2x}\\,\\mathrm{d}x \\]and, without too much difficulty, get the answer \\(\\frac12\\). But instead, let’s do this the Monte Carlo way.\nIn R, we can use the rexp() function to get IID samples from the exponential distribution: the full syntax is rexp(n, rate), which gives n samples from an exponential distribution with rate rate. The following code takes the mean of \\(n = 100\\) samples from the exponential distribution.\n\nn &lt;- 100\nsamples &lt;- rexp(n, 2)\nMCest &lt;- (1 / n) * sum(samples)\nMCest\n\n[1] 0.5378136\n\n\nSo our Monte Carlo estimate is 0.5378, to 4 decimal places.\nThat’s fairly close to the correct answer of \\(\\frac12\\). But we should (hopefully) be able to get a more accurate estimation if we use more samples. We could also simplify the third line of our code by using the mean() function.\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.4994785\n\n\nIn the second line, 1e6 is R code for the scientific notation \\(1 \\times 10^6\\), or a million. I just picked this as “a big number, but where my code still only took a few seconds to run.”\nOur new Monte Carlo estimate is 0.4995, which is (probably) much closer to the true value of \\(\\frac12\\).\n\nBy the way: all R code “chunks” displayed in the notes should work perfectly if you copy-and-paste them into RStudio. (Indeed, when I compile these lecture notes in RStudio, all the R code gets run on my computer – so I’m certain in must work correctly!) If you hover over a code chunk, a little “clipboard” icon should appear in the top-right, and clicking on that will copy it so you can paste it into RStudio. I strongly encourage playing about with the code as a good way to learn this material and explore further!\n\nExample 1.2 Let’s try another example. Let \\(X \\sim \\operatorname{N}(1, 2^2)\\) be a normal distribution with mean 1 and standard deviation 2. Suppose we want to find out \\(\\mathbb E(\\sin X)\\) (for some reason). While it might be possible to somehow calculate the integral \\[ \\mathbb E(\\sin X) = \\int_{-\\infty}^{+\\infty} (\\sin x) \\, \\frac{1}{\\sqrt{2\\pi\\times 2^2}} \\exp\\left(-\\frac{(x - 1)^2}{2\\times 2^2}\\right) \\, \\mathrm{d} x , \\] that looks extremely difficult to me.\nInstead, a Monte Carlo estimation of \\(\\mathbb{E}(\\sin X)\\) is very straightforward: we just take the mean of the sine of a bunch of normally distributed random numbers. That is we get a random samples \\(X_1, X_2, \\dots, X_n\\) from \\(X\\); then take the mean of the values \\[\\sin(X_1), \\sin(X_2), \\dots, \\sin(X_n) .\\]\n(We must remember, though, when using the rnorm() function to generate normally distributed random variates, that the third argument is the standard deviation, here \\(2\\), not the variance, here \\(2^2 = 4\\).)\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n, 1, 2)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.1129711\n\n\nOur Monte Carlo estimate is 0.11297.\n\nNext time: We look at more examples of things we can estimate using the Monte Carlo method.\n\nSummary:\n\nStatistical computing is about solving statistical problems by combining human ingenuity with computing power.\nThe Monte Carlo estimate of \\(\\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, \\dots, X_n\\) are IID random samples from \\(X\\).\nMonte Carlo estimation typically gets more accurate as the number of samples \\(n\\) gets bigger.\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html",
    "href": "lectures/L02-mc-uses.html",
    "title": "2  Uses of Monte Carlo",
    "section": "",
    "text": "2.1 Monte Carlo for probabilities\nQuick recap: Last time we defined the Monte Carlo estimator for an expectation of a function of a random variable \\(\\theta = \\Exg \\phi(X)\\) to be \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are independent random samples from \\(X\\).\nToday we look at two other things we can estimate using Monte Carlo simulation: probabilities, and integrals.\nWhat if we want to find a probability, rather than an expectation? What if we want \\(\\mathbb P(X = x)\\) for some \\(x\\), or \\(\\mathbb P(X \\geq a)\\) for some \\(a\\), or, more generally, \\(\\mathbb P(X \\in A)\\) for some set \\(A\\)?\nThe key thing that will help us here is the indicator function. The indicator function simply tells us whether an outcome \\(x\\) is in a set \\(A\\) or not.\nThe set \\(A\\) could just be a single element \\(A = \\{y\\}\\). In that case \\(\\Ind_A(x)\\) is 1 if \\(x = y\\) and 0 if \\(x \\neq y\\). Or \\(A\\) could be a semi-infinite interval, like \\(A = [a, \\infty)\\). In that case \\(\\Ind_A(x)\\) is 1 if \\(x \\geq a\\) and 0 if \\(x &lt; a\\).\nWhy is this helpful? Well \\(\\Ind_A\\) is a function, so let’s think about what the expectation \\(\\Exg \\Ind_A(X)\\) would be for some random variable \\(X\\). Since \\(\\Ind_A\\) can only take two values, 0 and 1, we have \\[\\begin{align*}\n\\Exg \\Ind_A(X) &= \\sum_{y \\in\\{0,1\\}} y\\,\\mathbb P\\big( \\Ind_A(X) = y \\big) \\\\\\\n  &= 0 \\times \\mathbb P\\big( \\Ind_A(X) = 0 \\big) + 1 \\times \\mathbb P\\big( \\Ind_A(X) = 1 \\big) \\\\\n  &= 0 \\times \\mathbb P(X \\notin A) + 1 \\times \\mathbb P(X \\in A) \\\\\n  &= \\mathbb P(X \\in A) .\n\\end{align*}\\] P(X A) . \\end{align*} In line three, we used that \\(\\Ind_A(X) = 0\\) if and only if \\(X \\notin A\\), and that \\(\\Ind_A(X) = 1\\) if and only if \\(X \\in A\\).\nSo the expectation of an indicator function a set is the probability that \\(X\\) is in that set. This idea connects “expectations of functions” back to probabilities: if we want to find \\(\\mathbb P(X \\in A)\\) we can find the expectation of \\(\\Ind_A(X)\\).\nWith this idea in hand, how do we estimate \\(\\theta = \\mathbb P(X \\in A)\\) using the Monte Carlo method? We write \\(\\theta = \\Exg\\Ind_A(X)\\). Then our Monte Carlo estimator is \\[  \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\Ind_A(X_i) . \\]\nWe remember that \\(\\Ind_A(X_i)\\) is 1 if \\(X_i \\in A\\) and 0 otherwise. So if we add up \\(n\\) of these, we count an extra +1 each time we have an \\(X_i \\in A\\). So \\(\\sum_{i=1}^n \\Ind_A(X_i)\\) counts the total number of the \\(X_i\\) that are in \\(A\\). So the Monte Carlo estimator can be written as \\[  \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{\\# \\text{ of } X_i \\text{ that are in $A$}}{n} . \\] (I’m using \\(\\#\\) as shorthand for “the number of”.)\nAlthough we’ve had to do a bit of work to get here, this a totally logical outcome! The right-hand side here is the proportion of the samples for which \\(X_i \\in A\\). And if we want to estimate the probability something happens, looking at the proportion of times it happens in a random sample is very much the “intuitive” estimate to take. And that intuitive estimate is indeed the Monte Carlo estimate!\nWe should explain the third line in the code we used for the Monte Carlo estimation mean(samples &gt; 2). In R, some statements can be answered “true” or “false”: these are often statements involving equality == (that’s a double equals sign) or inequalities like &lt;, &lt;=, &gt;=, &gt;, for example. So 5 &gt; 2 is TRUE but 3 == 7 is FALSE. These can be applied “component by component” to vectors. So, for example, testing which numbers from 1 to 10 are greater than or equal to 7, we get\n1:10 &gt;= 7\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\nsix FALSEs (for 1 to 6) followed by four TRUEs (for 7 to 10).\nWe can also use & (“and”) and | (“or”) in true/false statements like these.\nBut R also knows to treat TRUE like the number 1 and FALSE like the number 0. (This is just like the concept of the indicator function we’ve been discussing.) So if we add up some TRUEs and FALSEs, R simply counts how many TRUEs there are\nsum(1:10 &gt;= 7)\n\n[1] 4\nSo in our Monte Carlo estimation code, samples &gt; 2 was a vector of TRUEs and FALSEs, depending on whether each sample was greater than 2 or not, then mean(samples &gt; 2) took the proportion of the samples that were greater than 2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html#monte-carlo-for-probabilities",
    "href": "lectures/L02-mc-uses.html#monte-carlo-for-probabilities",
    "title": "2  Uses of Monte Carlo",
    "section": "",
    "text": "Definition 2.1 Let \\(A\\) be a set. Then the indicator function \\(\\Ind_A\\) is defined by \\[ \\Ind_A(x) = \\begin{cases} 1 & \\text{if $x \\in A$} \\\\ 0 & \\text{if $x \\notin A$.} \\end{cases} \\]\n\n\n\n\n\n\n\n\nExample 2.1 Let \\(Z \\sim \\operatorname{N}(0,1)\\) be a standard normal distribution. Estimate \\(\\mathbb P(Z &gt; 2)\\).\nThis is a question that it is impossible to answer exactly using a pencil and paper: there’s no closed form for \\[ \\mathbb P(Z &gt; 2) = \\int_2^\\infty \\frac{1}{\\sqrt{2\\pi}}\\,\\mathrm{e}^{-z^2/2}\\,\\mathrm{d}z , \\] so we’ll have to use an estimation method.\nThe Monte Carlo estimate means taking a random sample \\(Z_1, Z_2, \\dots, Z_n\\) of standard normals, and calculating what proportion of them are greater than 2. In R, we can do this as follows.\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.022873\n\n\nIn the second line, we could have written rnorm(n, 0, 1). But, if you don’t give the parameters mean and sd to the function rnorm(), R just assumes you want the standard normal with mean = 0 and sd = 1.\nWe can check our answer: R’s inbuilt pnorm() function estimates probabilities for the normal distribution (using a method that, in this specific case, is much quicker and more accurate than Monte Carlo estimation). The true answer is very close to\n\npnorm(2, lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nso our estimate was pretty good.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L02-mc-uses.html#monte-carlo-for-integrals",
    "href": "lectures/L02-mc-uses.html#monte-carlo-for-integrals",
    "title": "2  Uses of Monte Carlo",
    "section": "2.2 Monte Carlo for integrals",
    "text": "2.2 Monte Carlo for integrals\nThere’s another thing – a non-statistics thing – that Monte Carlo estimation is useful for. We can use Monte Carlo estimation to approximate integrals that are too hard to do by hand.\nThis might seem surprising. Estimating the expectation of (a function of) a random variable seems a naturally statistical thing to do. But an integral is just a straight maths problem – there’s not any randomness at all. But actually, integrals and expectations are very similar things.\nLet’s think of an integral: say, \\[ \\int_a^b h(x) \\,\\mathrm{d}x ,\\] the integral of some function \\(h\\) (the “integrand”) between the limits \\(a\\) and \\(b\\). Now let’s compare that to the integral \\(\\Exg \\phi(X)\\) of a continuous random variable that we can estimate using Monte Carlo estimation, \\[ \\Exg \\phi(X) = \\int_{-\\infty}^\\infty \\phi(x)\\,f(x)\\, \\mathrm{d} x. \\] Matching things up, we can see that we if we were to a function \\(\\phi\\) and a PDF \\(f\\) such that \\[ \\phi(x)\\,f(x) = \\begin{cases} 0 & x &lt; a \\\\ h(x) & a \\leq x \\leq b \\\\ 0 & x &gt; b , \\end{cases}  \\tag{2.1}\\] then we would have \\[ \\Exg \\phi(X) = \\int_{-\\infty}^\\infty \\phi(x)\\,f(x)\\, \\mathrm{d} x = \\int_a^b h(x) \\,\\mathrm{d}x, \\] so the value of the expectation would be precisely the value of the integral we’re after. Then we could use Monte Carlo to estimate that expectation/integral.\nThere are lots of choices of \\(\\phi\\) and \\(f\\) that would satisfy this the condition in Equation 2.1. But a “common-sense” choice that often works is to pick \\(f\\) to be the PDF of \\(X\\), a continuous uniform distribution on the interval \\([a,b]\\). (This certainly works when \\(a\\) and \\(b\\) are finite, anyway.) Recall that the continuous uniform distribution means that \\(X\\) has PDF \\[ f(x) = \\begin{cases} 0 & x &lt; a \\\\ \\displaystyle{\\frac{1}{b-a}} & a \\leq x \\leq b \\\\ 0 & x &gt; b . \\end{cases} \\] Comparing this equation with Equation 2.1, we then have to choose \\[\\phi(x) = \\frac{h(x)}{f(x)} = (b-a)h(x).\\]\nPutting this all together, we have \\[ \\Exg \\phi(X) = \\int_{-\\infty}^{+\\infty} \\phi(x)\\,f(x)\\,\\mathrm{d}x = \\int_a^b (b-a)h(x)\\,\\frac{1}{b-a}\\,\\mathrm{d}x = \\int_a^b h(x) \\,\\mathrm{d}x ,\\] as required. This can then be estimated using the Monte Carlo method.\n\nDefinition 2.2 Consider an integral \\(\\theta = \\int_a^b h(x)\\,\\mathrm{d}x\\). Let \\(f\\) be the probability density function of a random variable \\(X\\) and let \\(\\phi\\) be function such that Equation 2.1 holds. Then the Monte Carlo estimator \\(\\widehat\\theta_n^{\\mathrm{MC}}\\) of the integral \\(\\theta\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) , \\] where \\(X_1, X_2, \\dots, X_n\\) are a random sample from \\(X\\).\n\n\nExample 2.2 Suppose we want to approximate the integral \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x . \\]\nSince this is an integral on the finite interval \\([0,2]\\), it would seem to make sense to pick \\(X\\) to be uniform on \\([0,2]\\). This means we should take \\[\\phi(x) = \\frac{h(x)}{f(x)} = (2-0)h(x) = 2\\,x^{1.6}(2-x)^{0.7}.\\] We can then approximate this integral in R using the Monte Carlo estimator \\[ \\int_0^2 x^{1.6} (2-x)^{0.7} \\, \\mathrm{d}x = \\operatorname{\\mathbb{E}} \\phi(X) \\approx \\frac{1}{n} \\sum_{i=1}^n 2\\,X_i^{1.6} (2-X_i)^{0.7} . \\]\n\nn &lt;- 1e6\nintegrand &lt;- function(x) x^1.6 * (2 - x)^0.7\na &lt;- 0\nb &lt;- 2\nsamples &lt;- runif(n, a, b)\nmean((b - a) * integrand(samples))\n\n[1] 1.443793\n\n\nYou have perhaps noticed that, here and elsewhere, I tend to split my R code up into lots of small bits, perhaps slightly unnecessarily. After all, those 6 lines of code could simply have been written as just 2 lines\n\nsamples &lt;- runif(1e6, 0, 2)\nmean(2 * samples^1.6 * (2 - samples)^0.7)\n\nThere’s nothing wrong with that. However, I find that code is easier to read if divided into small pieces. It also makes it easier to tinker with, if I want to use it to solve some similar but slightly different problem.\n\n\nExample 2.3 Suppose we want to approximate the integral \\[ \\int_{-\\infty}^{+\\infty}\n\\mathrm{e}^{-0.1|x|} \\cos x \\, \\mathrm{d}x . \\] This one is an integral on the whole real line, so we can’t take a uniform distribution. Maybe we should take \\(f(x)\\) to be the PDF of a normal distribution, and then put \\[ \\phi(x) = \\frac{h(x)}{f(x)} = \\frac{\\mathrm{e}^{-0.1|x|} \\cos x}{f(x)} . \\]\nBut which normal distribution should we take? Well, we’re allowed to take any one – we will still get an accurate estimate in the limit as \\(n \\to \\infty\\). But we’d like an estimator that gives accurate results at moderate-sized \\(n\\), and picking a “good” distribution for \\(X\\) will help that.\nWe’ll probably get the best results if we pick a distribution that is likely to mostly take values where \\(h(x)\\) is big – or, rather, where the absolute value \\(|h(x)|\\) is big, to be precise. That is because we don’t want to “waste” too many samples where \\(h(x)\\) is very small, because they don’t contribute much to the integral. But we don’t want to “miss” – or only sample very rarely – places where \\(h(x)\\) is big, which contribute a lot to the integral.\nLet’s have a look at the graph of \\(h(x) = \\mathrm{e}^{-0.1|x|} \\cos x\\).\n\n\nCode for drawing this graph\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\n\ncurve(\n  integrand, n = 1001, from = -55, to = 55,\n  col = \"blue\", lwd = 3,\n  xlab = \"x\", ylab = \"integrand h(x)\", xlim = c(-50,50)\n)\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nThis suggests to me that a mean of 0 and a standard deviation of 20 might work quite well, since this will tend to take values in \\([-40,40]\\) or so.\nWe will use R’s function dnorm() for the probability density function of the normal distribution (which saves us from having to remember what that is).\n\nn &lt;- 1e6\nintegrand &lt;- function(x) exp(-0.1 * abs(x)) * cos(x)\npdf       &lt;- function(x) dnorm(x, 0, 20)\nphi       &lt;- function(x) integrand(x) / pdf(x)\n\nsamples &lt;- rnorm(n, 0, 20)\nmean(phi(samples))\n\n[1] 0.235336\n\n\n\nNext time: We will analyse the accuracy of these Monte Carlo estimates.\n\nSummary:\n\nThe indicator \\(\\Ind_A(x)\\) function of a set \\(A\\) is 1 if \\(x \\in A\\) or 0 if \\(x \\notin A\\).\nWe can estimate a probability \\(\\mathbb P(X \\in A)\\) by using the Monte Carlo estimate for \\(\\Exg\\Ind_A(X)\\).\nWe can estimate an integral \\(\\int h(x) \\, \\mathrm{d}x\\) by using a Monte Carlo estimate with \\(\\phi(x)\\,f(x) = h(x)\\).\n\nRead more: Voss, An Introduction to Statistical Computing, Section 3.1 and Subsection 3.2.1.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uses of Monte Carlo</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html",
    "href": "lectures/L03-mc-error-1.html",
    "title": "3  Monte Carlo error I: theory",
    "section": "",
    "text": "3.1 Estimation error\nToday we are going to analysing the accuracy of Monte Carlo estimation. But before talking about Monte Carlo estimation specifically, let’s first remind ourselves of some concepts about error in statistical estimation more generally. We will use the following definitions.\nUsually, the main goal of estimation is to get the mean-square error of an estimate as small as possible. This is because the MSE measures by what distance we are missing on average. It can be easier to interpret what the root-mean-square error means, as the RMSE has the same units as the parameter being measured: if \\(\\theta\\) and \\(\\widehat{\\theta}\\) are in metres, say, then the MSE is in metres-squared, whereas the RMSE error is in metres again. If you minimise the MSE you also minimise the RMSE and vice versa.\nIt’s nice to have an “unbiased” estimator – that is, one with bias 0. This is because bias measures any systematic error in a particular direction. However, unbiasedness by itself is not enough for an estimate to be good – we need low variance too. (Remember the old joke about the statistician who misses his first shot ten yards to the left, misses his second shot ten yards to the right, then claims to have “hit the target on average.”)\n(Remember also that “bias” is simply the word statisticians use for \\(\\mathbb E(\\widehat\\theta - \\theta)\\); we don’t mean “bias” in the derogatory way it is sometimes used in political arguments, for example.)\nYou probably also remember the relationship between the mean-square error, the bias, and the variance:\nSince the bias contributes to the mean-square error, that’s another reason to like estimator with low – or preferably zero – bias. But again, unbiasedness isn’t enough by itself; we want low variance too. (There are some situations where there’s a “bias–variance tradeoff”, where allowing some bias reduces the variance and so can reduce the MSE. It turns out that Monte Carlo is not one of these cases, however.)",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#estimation-error",
    "href": "lectures/L03-mc-error-1.html#estimation-error",
    "title": "3  Monte Carlo error I: theory",
    "section": "",
    "text": "Definition 3.1 Let \\(\\widehat\\theta\\) be an estimator of a parameter \\(\\theta\\). Then we have the following definitions of the estimator \\(\\widehat\\theta\\):\n\nThe bias is \\(\\operatorname{bias}\\big(\\widehat\\theta\\big) = \\mathbb E\\big(\\widehat\\theta - \\theta\\big)  = \\mathbb E\\widehat\\theta - \\theta\\).\nThe mean-square error is \\(\\operatorname{MSE}\\big(\\widehat\\theta\\big) = \\mathbb E \\big(\\widehat\\theta - \\theta\\big)^2\\).\nThe root-mean-square error is the square-root of the mean-square error, \\[\\operatorname{RMSE}\\big(\\widehat\\theta\\big) = \\sqrt{\\operatorname{MSE}(\\widehat\\theta)} = \\sqrt{\\mathbb E (\\widehat\\theta - \\theta)^2} . \\]\n\n\n\n\n\n\n\nTheorem 3.1   \\(\\operatorname{MSE}\\big(\\widehat\\theta\\big) = \\operatorname{bias}\\big(\\widehat\\theta\\big)^2 + \\operatorname{Var}\\big(\\widehat\\theta\\big)\\).\n\n\nProof. The MSE is \\[\\begin{align}\n  \\operatorname{MSE}\\big(\\widehat\\theta\\big) = \\Exg\\big(\\widehat\\theta - \\theta\\big)^2\n    &= \\Exg \\big(\\widehat\\theta^2 - 2\\theta\\widehat\\theta + \\theta\\big)^2 \\\\\n    &= \\Exg \\widehat\\theta^2 - 2\\theta \\Exg \\widehat\\theta + \\theta^2 ,\n\\end{align}\\] where we have expanded the brackets and bought the expectation inside (remembering that \\(\\theta\\) is a constant). Since the variance can be written as \\(\\Var(\\widehat\\theta) = \\Exg\\widehat\\theta^2 - (\\Exg \\widehat\\theta)^2\\), we can use a cunning trick of both subtracting and adding \\((\\Exg \\widehat\\theta)^2\\). This gives \\[\\begin{align}\n\\operatorname{MSE}\\big(\\widehat\\theta\\big)\n  &= \\Exg \\widehat\\theta^2 - \\big(\\!\\Exg \\widehat\\theta\\big)^2 + \\big(\\!\\Exg \\widehat\\theta\\big)^2 - 2\\theta \\Exg \\widehat\\theta + \\theta^2 \\\\\n  &= \\Var\\big(\\widehat\\theta\\big) + \\big( (\\Exg \\widehat\\theta)^2 - 2\\theta \\Exg \\widehat\\theta + \\theta^2 \\big) \\\\\n  &= \\Var\\big(\\widehat\\theta\\big) + \\big( \\! \\Exg \\widehat\\theta - \\theta\\big)^2 \\\\\n  &= \\Var\\big(\\widehat\\theta\\big) + \\operatorname{bias}(\\widehat\\theta)^2 .\n\\end{align}\\] This proves the result.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#error-of-monte-carlo-estimator-theory",
    "href": "lectures/L03-mc-error-1.html#error-of-monte-carlo-estimator-theory",
    "title": "3  Monte Carlo error I: theory",
    "section": "3.2 Error of Monte Carlo estimator: theory",
    "text": "3.2 Error of Monte Carlo estimator: theory\nIn this lecture, we’re going to be looking more carefully at the size of the errors made by the Monte Carlo estimator \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\big(\\phi(X_1) + \\phi(X_2) + \\cdots + \\phi(X_n) \\big) = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) . \\]\nOur main result is the following.\n\nTheorem 3.2 Let \\(X\\) be a random variable, \\(\\phi\\) a function, and \\(\\theta = \\Exg\\phi(X)\\). Let \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) \\] be the Monte Carlo estimator of \\(\\theta\\). Then:\n\n\\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is unbiased, in that \\(\\operatorname{bias}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = 0\\).\nThe variance of of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nThe mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\({\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nThe root-mean-square error of \\(\\widehat{\\theta}_n^{\\mathrm{MC}}\\) is \\[{\\displaystyle \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} = \\frac{1}{\\sqrt{n}} \\, \\operatorname{sd}\\big(\\phi(X)\\big)}. \\]\n\n\nBefore we get to the proof, let’s recap some relevant probability.\nLet \\(Y_1, Y_2, \\dots\\) be IID random variables with common expectation \\(\\mathbb EY_1 = \\mu\\) and common variance \\(\\operatorname{Var}(Y_1) = \\sigma^2\\). Consider the mean of the first \\(n\\) random variables, \\[ \\overline{Y}_n = \\frac{1}{n} \\sum_{i=1}^n Y_i . \\] Then the expectation of \\(\\overline{Y}_n\\) is \\[ \\mathbb E \\overline{Y}_n = \\mathbb E\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\frac{1}{n}\n\\sum_{i=1}^n \\mathbb{E}Y_i = \\frac{1}{n}\\,n\\,\\mu = \\mu . \\] The variance of \\(\\overline{Y}_n\\) is \\[ \\operatorname{Var}\\big(  \\overline{Y}_n \\big)= \\operatorname{Var} \\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\bigg(\\frac{1}{n}\\bigg)^2\n\\sum_{i=1}^n \\operatorname{Var}(Y_i) = \\frac{1}{n^2}\\,n\\,\\sigma^2 = \\frac{\\sigma^2}{n} , \\] where, for this one, we used the independence of the random variables.\n\nProof. Apply the probability facts from above with \\(Y = \\phi(X)\\). This gives:\n\n\\(\\Ex \\widehat{\\theta}_n^{\\mathrm{MC}} = \\Ex \\overline Y_n = \\Ex Y = \\Exg \\phi(X)\\), so \\(\\operatorname{bias}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\Exg \\phi(X) - \\Exg \\phi(X) = 0\\).\n\\({\\displaystyle \\operatorname{Var}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\operatorname{Var}\\big(\\overline Y_n\\big) = \\frac{1}{n} \\operatorname{Var}(Y) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)}\\).\nUsing Theorem 3.1, \\[\\operatorname{MSE}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\operatorname{bias}(\\widehat{\\theta}_n^{\\mathrm{MC}})^2 + \\operatorname{Var}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = 0^2 + \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) . \\]\nTake the square root of part 3.\n\n\nLet’s think about MSE \\(\\frac{1}{n} \\Var(\\phi(X))\\). The variance terms is some fixed fact about the random variable \\(X\\) and the function \\(\\phi\\). So as \\(n\\) gets bigger, \\(\\frac{1}{n}\\) gets smaller, so the MSE gets smaller, and the estimator gets more accurate. This goes back to what we said when we introduced the Monte Carlo estimator: we get a more accurate estimate by increasing \\(n\\). More specifically, the MSE scales like \\(1/n\\), or – perhaps a more useful result – the RMSE scales like \\(1/\\sqrt{n}\\). We’ll come back to this in the next lecture.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L03-mc-error-1.html#error-of-monte-carlo-estimator-practice",
    "href": "lectures/L03-mc-error-1.html#error-of-monte-carlo-estimator-practice",
    "title": "3  Monte Carlo error I: theory",
    "section": "3.3 Error of Monte Carlo estimator: practice",
    "text": "3.3 Error of Monte Carlo estimator: practice\nSo when we form a Monte Carlo estimate \\(\\hat\\theta_n^{\\text{MC}}\\), we now know it will be unbiased. We’d also like to know it’s mean-square and/or root-mean-square error too.\nThere’s a problem here, though. The reason we are doing Monte Carlo estimation in the first place is that we couldn’t calculate \\(\\Exg \\phi(X)\\). So it seems very unlikely we’ll be able to calculate the variance \\(\\operatorname{Var}(\\phi(X))\\) either. So how will be able to assess the mean-square (or root-mean-square) error of our Monte Carlo estimator?\nWell, we can’t know it exactly. But we can estimate the variance from the samples we are already using: by taking the sample variance of the samples \\(\\phi(x_i)\\). That is, we can estimate the variance of the Monte Carlo estimator by the sample variance \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\big(\\phi(X_i) - \\widehat{\\theta}_n^{\\mathrm{MC}} \\big)^2 . \\] Then we can similarly estimate the mean-square and root-mean-square errors by \\[ \\text{MSE} \\approx \\frac{1}{n}S^2 \\qquad \\text{and} \\qquad \\text{RMSE} \\approx \\sqrt{\\frac{1}{n} S^2} = \\frac{1}{\\sqrt{n}}\\,S  \\] respectively.\n\nExample 3.1 Let’s go back to the very first example in the module, Example 1.1, where we were trying to find the expectation of an \\(\\operatorname{Exp}(2)\\) random variable. We used this R code:\n\nn &lt;- 1e6\nsamples &lt;- rexp(n, 2)\nMCest &lt;- mean(samples)\nMCest\n\n[1] 0.4997326\n\n\n(Because Monte Carlo estimation is random, this won’t be the exact same estimate we had before, of course.)\nSo if we want to investigate the error, we can use the sample variance of these samples. We will use the sample variance function var() to calculate the sample variance. In this simple case, the function is \\(\\phi(x) = x\\), so we need only use the variance of the samples themselves.\n\nvar_est &lt;- var(samples)\nMSEest  &lt;- var_est / n\nRMSEest &lt;- sqrt(MSEest)\nc(var_est, MSEest, RMSEest)\n\n[1] 2.497426e-01 2.497426e-07 4.997425e-04\n\n\nThe first number is var_est \\(= 0.2497\\), the sample variance of our \\(\\phi(x_i)\\)s:\n\\[ s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\big(\\phi(x_i) - \\widehat{\\theta}_n^{\\mathrm{MC}}\\big)^2 . \\] This should be a good estimate of the true variance \\(\\operatorname{Var}(\\phi(X))\\). (In fact, in this simple case, we know that \\(\\operatorname{Var}(X) = \\frac{1}{2^2} = 0.25\\), so we know that the estimate is good.) In calculating this in the code, we used R’s var() function, which calculates the sample variance of some values.\nThe second number is MSEest \\(= 2.497\\times 10^{-7}\\), our estimate of the mean-square error. Since \\(\\operatorname{MSE}(\\widehat{\\theta}_n^{\\mathrm{MC}}) = \\frac{1}{n} \\operatorname{Var}(\\phi(X))\\), then \\(\\frac{1}{n} S^2\\) should be a good estimate of the MSE.\nThe third number is RMSEest \\(= 5\\times 10^{-4}\\) our estimate of the root-mean square error, which is simply the square-root of our estimate of the mean-square error.\n\n\nExample 3.2 In Example 2.1, we were estimating \\(\\mathbb P(Z &gt; 2)\\), where \\(Z\\) is a standard normal.\nOur code was\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest &lt;- mean(samples &gt; 2)\nMCest\n\n[1] 0.022299\n\n\nSo our root-mean-square error can be approximated as\n\nMSEest &lt;- var(samples &gt; 2) / n\nsqrt(MSEest)\n\n[1] 0.0001476542\n\n\nsince samples &gt; 2 is the indicator function of whether \\(X_i &gt; 2\\) or not.\n\nNext time: We’ll continue analysing Monte Carlo error, looking at confidence intervals and assessing how many samples to take..\n\nSummary:\n\nThe Monte Carlo estimator is unbiased.\nThe Monte Carlo estimator has mean-square error \\(\\Var(\\phi(X))/n\\), so the root-mean-square error scales like \\(1/\\sqrt{n}\\).\nThe mean-square error can be estimated by \\(S^2 / n\\), where \\(S^2\\) is the sample variance of the \\(\\phi(X_i)\\).\n\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.2.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Monte Carlo error I: theory</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html",
    "href": "lectures/L04-mc-error-2.html",
    "title": "4  Monte Carlo error II: practice",
    "section": "",
    "text": "4.1 Recap\nLet’s recap where we’ve got to. We know that the Monte Carlo estimator for \\(\\theta = \\Exg \\phi(X)\\) is \\[ \\widehat{\\theta}_n^{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n \\phi(X_i) .\\] Last time, we saw that the Monte Carlo estimator is unbiased, and that its mean-square and root-mean-square errors are \\[ \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big) \\qquad \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} . \\] We saw that these themselves can be estimated as \\(S^2/n\\) and \\(S/\\sqrt{n}\\) respectively, where \\(S^2\\) is the sample variance of the \\(\\phi(X_i)\\)s.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#confidence-intervals",
    "href": "lectures/L04-mc-error-2.html#confidence-intervals",
    "title": "4  Monte Carlo error II: practice",
    "section": "4.2 Confidence intervals",
    "text": "4.2 Confidence intervals\nSo far, we have described our error tolerance in terms of the MSE or RMSE. But we could have talked about “confidence intervals” or “margins of error” instead. This might be easier to understand for non-mathematicians, for whom “root-mean-square error” doesn’t really mean anything.\nHere, we will want to appeal to the central limit theorem approximation. A bit more probability revision: Let \\(Y_1, Y_2, \\dots\\) be IID again, with expectation \\(\\mu\\) and variance \\(\\sigma^2\\). Write \\(\\overline Y_n\\) for the mean. We’ve already reminded ourselves of the law of large numbers, which says that \\(\\overline Y_n \\to \\mu\\) as \\(n \\to infty\\). Then in the last lecture we saw that \\(\\mathbb E \\overline Y_n = \\mu\\) and \\(\\Var(\\overline{Y}_n) = \\sigma^2/n\\). The central limit theorem says that the distribution of \\(\\overline Y_n\\) is approximately normally distributed with those parameters, so \\(\\overline Y_n \\approx \\operatorname{N}(\\mu, \\sigma^2/n)\\) when \\(n\\) is large. (This is an informal statement of the central limit theorem: you probably know some more formal ways to more precisely state it, but this will do for us.)\nRecall that, in the normal distribution \\(\\operatorname{N}(\\mu, \\sigma^2)\\), we expect to be within \\(1.96\\) standard deviations of the mean with 95% probability. More generally, the interval \\([\\mu - q_{1-\\alpha/2}\\sigma, \\mu + q_{1-\\alpha/2}\\sigma]\\), where \\(q_{1-\\alpha/2}\\) is the \\((1- \\frac{\\alpha}{2})\\)-quantile of the normal distribution, contains the true value with probability approximately \\(1 - \\alpha\\).\nWe can form an approximate confidence interval for a Monte Carlo estimate using this idea. We have our Monte Carlo estimator \\(\\widehat{\\theta}_n^\\mathrm{MC}\\) as our estimator of the \\(\\mu\\) parameter, and our estimator of the root-mean-square error \\(S/\\sqrt{n}\\) as our estimator of the \\(\\sigma\\) parameter. So our confidence interval is estimated as \\[\\bigg[ \\widehat{\\theta}_n^\\mathrm{MC} - q_{1-\\alpha/2}\\,\\frac{S}{\\sqrt{n}}, \\ \\widehat{\\theta}_n^\\mathrm{MC} + q_{1-\\alpha/2}\\,\\frac{S}{\\sqrt{n}} \\bigg] . \\]\n\nExample 4.1 We continue the example of Example 2.1 and Example 3.2, where we were estimating \\(\\mathbb P(Z &gt; 2)\\) for \\(Z\\) a standard normal.\n\nn &lt;- 1e6\nsamples &lt;- rnorm(n)\nMCest   &lt;- mean(samples &gt; 2)\nRMSEest &lt;- sqrt(var(samples &gt; 2) / n)\nMCest\n\n[1] 0.022818\n\n\nOur confidence interval is estimates as follows\n\nalpha &lt;- 0.05\nquant &lt;- qnorm(1 - alpha / 2)\nc(MCest - quant * RMSEest, MCest + quant * RMSEest)\n\n[1] 0.02252533 0.02311067",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L04-mc-error-2.html#how-many-samples-do-i-need",
    "href": "lectures/L04-mc-error-2.html#how-many-samples-do-i-need",
    "title": "4  Monte Carlo error II: practice",
    "section": "4.3 How many samples do I need?",
    "text": "4.3 How many samples do I need?\nIn our examples we’ve picked the number of samples \\(n\\) for our estimator, then approximated the error based on that. But we could do things the other way around – fix an error tolerance that we’re willing to deal with, then work out what sample size we need to achieve it.\nWe know that the root-mean-square error is \\[ \\operatorname{RMSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} \\] So if we want to get the RMSE down to \\(\\epsilon\\), say, then this shows that we need \\[ \\epsilon = \\sqrt{\\frac{1}{n} \\operatorname{Var}\\big(\\phi(X)\\big)} .\\] Squaring both sides, multiplying both sides by \\(n\\), and dividing both sides by \\(\\epsilon^2\\) gives \\[ n = \\frac{1}{\\epsilon^2} \\Var\\big(\\phi(X)\\big) . \\]\nSo this tells us how many samples \\(n\\) we need. Except we still have a problem here, though. We (usually) don’t know \\(\\Var(\\phi(X))\\). But we can’t even estimate \\(\\Var(\\phi(X))\\) until we’ve already taken the samples. So it seems we’re stuck.\nBut we can use this idea with a three-step process:\n\nRun an initial “pilot” Monte Carlo algorithm with a small number of samples \\(n\\). Use the results of the “pilot” to estimate the variance \\(S^2 \\approx \\Var(\\phi(X))\\). We want \\(n\\) small enough that this runs very quickly, but big enough that we get a reasonably OK estimate of the variance.\nPick a desired RMSE accuracy \\(\\epsilon\\). We now know that we require roughly \\(N = S^2 / \\epsilon^2\\) samples to get our desired accuracy.\nRun the “real” Monte Carlo algorithm with this big number of samples \\(N\\). We will put up with this being quite slow, because we know we’re definitely going to get the error tolerance we need.\n\n(We could potentially use further steps, where we now check the variance with the “real” big-\\(N\\) samples, and, if we learn we had underestimated in Step 1, take even more samples to correct for this.)\n\nExample 4.2 Let’s try this with Example 1.2 from before. We were trying to estimate \\(\\mathbb{E}(\\sin X)\\), where \\(X \\sim \\operatorname{N}(1, 2^2)\\).\nWe’ll start with just \\(n = 1000\\) samples, for our pilot study.\n\nn_pilot &lt;- 1000\nsamples &lt;- rnorm(n_pilot, 1, 2)\nvar_est &lt;- var(sin(samples))\nvar_est\n\n[1] 0.4995853\n\n\nThis was very quick! We won’t have got a super-accurate estimate of \\(\\mathbb E\\phi(X)\\), but we have a reasonable idea of roughly what \\(\\operatorname{Var}(\\phi(X))\\) is. This will allow us to pick out “real” sample size in order to get a root-mean-square error of \\(10^{-4}\\).\n\nepsilon &lt;- 1e-4\nn_real  &lt;- round(var_est / epsilon^2)\nn_real\n\n[1] 49958529\n\n\nThis tells us that we will need about 50 million samples! This is a lot, but now we know we’re going to get the accuracy we want, so it’s worth it. (In this particular case, 50 million samples will only take a few second on a modern computer. But generally, once we know our code works and we know how many samples we will need for the desired accuracy, this is the sort of thing that we could leave running overnight or whatever.)\n\nsamples &lt;- rnorm(n_real, 1, 2)\nMCest &lt;- mean(sin(samples))\nMCest\n\n[1] 0.1138543\n\nRMSEest &lt;- sqrt(var(sin(samples)) / n_real)\nRMSEest\n\n[1] 9.873866e-05\n\n\nThis second step was quite slow (depending on the speed of the computer being used – it was only about 5 seconds on my pretty-new laptop, but slower on my ancient work desktop). But we see that we have indeed got our Monte Carlo estimate to (near enough) the desired accuracy.\n\nGenerally, if we want a more accurate Monte Carlo estimator, we can just take more samples. But the equation \\[ n = \\frac{1}{\\epsilon^2} \\Var\\big(\\phi(X)\\big) \\] is actually quite bad news. To get an RMSE of \\(\\epsilon\\) we need order \\(1/\\epsilon^2\\) samples. That’s not good. Think of it like this: to double the accuracy we need to quadruple the number of samples. Even worse: to get “one more decimal place of accuracy” means dividing \\(\\epsilon\\) by ten; but that means multiplying the number of samples by one hundred!\nMore samples take more time, and cost more energy and money. Wouldn’t it be nice to have some better ways of increasing the accuracy of a Monte Carlo estimate besides just taking more and more samples?\nNext time: We begin our study of clever “variance reduction” methods for Monte Carlo estimation.\n\nSummary:\n\nWe can approximate confidence intervals for a Monte Carlo estimate by using a normal approximation.\nTo get the root-mean-square error below \\(\\epsilon\\) we need \\(n = \\Var(\\phi(X))/\\epsilon^2\\) samples.\nWe can use a two-step process, where a small “pilot” Monte Carlo estimation allows us to work out how many samples we will need for the big “real” estimation.\n\nRead more: Voss, An Introduction to Statistical Computing, Subsections 3.2.2–3.2.4.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Monte Carlo error II: practice</span>"
    ]
  },
  {
    "objectID": "lectures/L05-cv.html",
    "href": "lectures/L05-cv.html",
    "title": "5  Control variate",
    "section": "",
    "text": "\\[ \\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}} \n\\newcommand{\\Ex}{\\mathbb{E}} \n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}} \\]\n\n\n\n\n\n\n\\[ \\]\n\n\n\nSummary:\n\nVariance reduction techniques attempt to improve on Monte Carlo estimation making the variance smaller.\nIf we know \\(\\eta = \\Exg \\psi(X)\\), then the control variate Monte Carlo estimate is \\[ \\widehat{\\theta}_n^{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(\\phi(X_i) - \\psi(X_i)\\big) + \\eta.\\]\nThe mean-square error of the control variate Monte Carlo estimate is \\[{\\displaystyle \\operatorname{MSE}\\big(\\widehat{\\theta}_n^{\\mathrm{MC}}\\big) = \\frac{1}{n} \\operatorname{Var}\\big(\\phi(X) - \\psi(X)\\big)}.\\]\n\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.3.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Control variate</span>"
    ]
  },
  {
    "objectID": "lectures/L06-antithetic-1.html",
    "href": "lectures/L06-antithetic-1.html",
    "title": "6  Antithetic variables I",
    "section": "",
    "text": "\\[ \\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}} \n\\newcommand{\\Ex}{\\mathbb{E}} \n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}} \\]\n\n\n\n\n\n\n\\[ \\]\n\n\n\nSummary:\n\nEstimation is helped by combining individual estimates that are negatively correlated.\nFor antithetic variables Monte Carlo estimation, we take pairs of non-independent variables \\((X, X')\\), to get the estimator \\[ \\widehat{\\theta}_n^{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n/2} \\big(\\phi(X_i) + \\phi(X'_i) \\big) . \\]\n\nOn Problem Sheet 1, you should now be able to answer all questions. You should work through this problem sheet in advance of the problems class on Thursday 17 October.\nRead more: Voss, An Introduction to Statistical Computing, Subsection 3.3.2.",
    "crumbs": [
      "Monte Carlo estimation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Antithetic variables I</span>"
    ]
  },
  {
    "objectID": "problems/P1.html",
    "href": "problems/P1.html",
    "title": "Problem Sheet 1",
    "section": "",
    "text": "\\[ \\newcommand{\\Exg}{\\operatorname{\\mathbb{E}}} \n\\newcommand{\\Ex}{\\mathbb{E}} \n\\newcommand{\\Ind}{\\mathbb{I}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\\newcommand{\\ee}{\\mathrm{e}} \\]\n\n\n\n\n\n\n\n\nThis is Problem Sheet 1, which covers material from Lectures 1 to 6. You should work through all the questions on this problem sheet in advance of the problems class, which takes place in the lecture of Thursday 16 October. If you are stuck on any of the questions, you are welcome to discuss them with me in my office hours on Thursdays at 1300.\nThis problem sheet is to help you practice material from the module and to help you check your learning. It is not for formal assessment and does not count towards your module mark.\nHowever, if, optionally, you would like some brief informal feedback on Questions 4, 6 and 8 (marked ★), I am happy to provide some. If you want some brief feedback, you should submit your work electronically through Gradescope via the module’s Minerva page by 1400 on Tuesday 14 October. I will return some brief comments on your those two questions by the problems class on Thursday 16 October. Because this informal feedback, not part of the official assessment, I cannot accept late work for any reason – but I am always happy to discuss any of your work on any question in my office hours.\nMany of these questions will require use of the R programming language (for example, by using the program RStudio).\nFull solutions should be released on Friday 17 October.",
    "crumbs": [
      "Monte Carlo estimation",
      "Problem Sheet 1"
    ]
  }
]