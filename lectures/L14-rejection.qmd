# Rejection sampling

## Rejection

In the last two lectures, we have taken standard uniform $U \sim \operatorname{U}[0,1]$ random variables, and have applied a function to them to transform into some other distribution $X$. One $U$ gets turned into one $X$. (Or, for the Box--Muller transform, two $U$s become two $X$s.)

But so far, we have taken each sample we are given. But another way to get a different distribution is to throw out samples we don't like and wait until we get a sample we do like. This is called **rejection sampling**.

Suppose we want not a $\operatorname{U}[0,1]$ random variable but instead a $\operatorname{U}[0,\tfrac12]$ random variable. One way we've already seen to do this is by the inverse transform method: simply multiply $U$ by $\tfrac12$. But we could also do this by rejection: if $U \leq \tfrac12$, we keep the sample; but if $U > \tfrac12$, we throw it away and ask for a new one. We keep taking samples until we get one less than $\tfrac12$. It should be easy to convince yourself that we get a $\operatorname{U}[0,\tfrac12]$ random variable this way. (But we'll prove it later, if not.)

The advantage of rejection sampling is that it can help us get samples from some distributions that we couldn't access with the inverse transform method. The disadvantage is that it can be costly or slow, because we may have to reject lots of samples before finding one we can keep. The more often we reject samples, the slower the procedure will be.

<!--
Suppose we are already sampling from some distribution $Y$ (perhaps generated via the inverse transform method, for example); we keep the sample if is in some set $A$, and reject it if it is not in $A$. What is the distribution of the first accepted sample $X$? In the discrete case we have $\mathbb P(X = x) \propto \mathbb P(Y = x)$ for $x \in A$ and $\mathbb P(X = x) = 0$ for $x \notin A$. In the continuous case, have $f(x) \propto g(x)$ for $x \in A$ and $f(x) = 0$ for $x \notin A$, where $f$ is the PDF. (This might seem intuitive; if not, we give a formal proof later.-->

This method is particularly useful for sampling from a conditional distribution, such as the conditional distribution of $Y$ given that $Y \in A$: we simply accept a sample $y$ if $y \in A$ and reject it if not.

::: {exm-cond}
Let $Y \sim \operatorname{N}(0, 1)$. Suppose we wish to use Monte Carlo estimation to estimate $\mathbb E(Y \mid Y \geq 1)$.

To do this, we will need samples from the conditional distribution $Y \mid Y \geq 1$. So we accept samples that really are at least 1, and reject samples that are less than 1.

There are two ways we could run this in practice. First, we could decide to take $n$ samples from $Y$, and just see how many get accepted.

```{r}
n_samp <- 1e6
samples <- rnorm(n_samp)
samples <- samples[samples >= 1]
length(samples)
MCest1 <- mean(samples)
MCest1
```

We end up accepting around 160,000 samples out of the 1,000,000 samples we took to start with.

Second, we could keep taking as many samples as needed until we reach some desired number of acceptances.

```{r}
n_acc <- 1e5

samples <- rep(0, n_acc)
count <- 0
for (i in 1:n_acc) {
  newsample <- 0
  while (newsample < 1) {
    newsample <- rnorm(1)
    count <- count + 1
  }
  samples[i] <- newsample
}
count

MCest1 <- mean(samples)
MCest1
```
This required taking about 630,000 samples to get 100,000 acceptances.
:::

## Acceptance probability

So far, we have looked at always accepting or always rejecting a given sample, depending on the value it outputs. But we could "perhaps" accept some values too. Suppose we are already sampling from some distribution $Y$ (perhaps generated via the inverse transform method, for example). If we see the sample $Y = x$, we could accept with some **acceptance probability** $\alpha(x) \in [0,1]$. We can control the output more delicately by adjusting this acceptance function $\alpha$ to values that aren't just 0 or 1.

What is the distribution of the first accepted sample $X$?

Well, using Bayes' theorem, we have in the discrete case
$$\mathbb P(X = x) = \mathbb P(Y = x \mid \text{accept}) = \frac{\mathbb P(Y = x)\,\mathbb P(\text{accept} \mid Y = x)}{\mathbb P(\text{accept})} = \frac{1}{Z} \alpha(x)\,\mathbb P(Y = x) .$$
where $Z = \mathbb P(\text{accept})$ is the normalising constant.
In the continuous case, with $g$ the PDF of the original $Y$ and $f$ the PDF of the accepted $X$, we have
$$ f(x) = g(x \mid \text{accept}) = \frac{g(x)\,\mathbb P(\text{accept} \mid X = x)}{\mathbb P(\text{accept})} = \frac{1}{Z}\,\alpha(x)\,g(x) , $$
where $Z = \mathbb P(\text{accept})$ again.

::: {#exm-normcos}
Suppose we wish to sample from the distribution
$$ f(x) \propto \exp\big(-\tfrac12x^2\big)\,(\sin^2 x) .$$ {#eq-distprop}
How can we do this?

Well, we can note that the PDF of the standard normal is
$$ g(x) = \frac{1}{2\pi}\,\exp\big(-\tfrac12x^2\big) \propto \exp\big(-\tfrac12x^2\big) $$
and that
$$ 0 \leq \sin^2x\leq 1 .$$
(Here, $\sin^2 x$ means $(\sin x)^2$, by the way.) This means that, if we sample $Y \sim \operatorname{N}(0,1)$, and then accept an output with probability $\alpha(x) = \sin^2 x$, that will give us the distribution @eq-distprop.

```{r}
n_samp <- 1e6
samples <- rnorm(n_samp)
samples <- samples[runif(n_samp) <= sin(samples)^2]
length(samples)

hist(samples, probability = TRUE, breaks = 50)
curve(
  0.92 * exp(-x^2 / 2) * sin(x)^2, add = TRUE, n = 1001,
  lwd = 2, col = "blue"
)
```

Let's explain line 3 more carefully. We want to accept a value $x$ with probability $\sin^2 x$. We saw in Lecture 12 that we can simulate a Bernoulli$(p)$ distribution by taking the value 1 is $U \leq p$ and taking 0 if $U > p$. So in line 3, we are accepting each sample $x$ if a standard uniform distribution $U \leq \sin^2 x$.
:::

In this example, note that we managed to sample from the PDF in @eq-distprop,
$$ f(x) = \frac{1}{Z}\,\exp\big(-\tfrac12x^2\big)\,(\sin^2 x) ,$$
without every finding out what the normalising constant actually is. This idea -- that we can sample from a distribution even if we only know it up to a multiplicative constant -- is a very important one that will come up a lot later in this module.

We won't go into that idea deeply now, but we will briefly mention that it is very important in Bayesian statistics. That's because in Bayesian statistics, the posterior distribution is often known only up to proportionality. That's because we have
$$ \begin{align}
\text{posterior} &\propto \text{prior}\times\text{likelihood} \\
\pi(\theta \mid x) &\propto\, \pi(x) \times p(x \mid \theta)
\end{align} $$
It's often very difficult (or impossible) to find the normalising constant in this expression. So being able to sample from such a posterior distribution without finding that constant is very important for Bayesian statisticians.


## How many samples?