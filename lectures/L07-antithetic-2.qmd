# Antithetic variables II

{{< include ../_header.qmd >}}


## Error with antithetic variables

Recall from last time the antithetic variables Monte Carlo estimator. We take sample pairs
$$ (X_1, X'2), (X_2, X'2), \dots, (X_{n/2}, X_{n/2}') , $$
where samples are independent between different pairs but *not* independent within the same pair. The estimator of $\theta = \Ex \phi(X)$ is
$$ \widehat{\theta}_n^{\mathrm{AV}} = \frac{1}{n} \sum_{i=1}^{n/2} \big(\phi(X_i) + \phi(X'_i) \big) .$$
We hope this is better than the standard Monte Carlo estimator if $\phi(X)$ and $\phi(X')$ are negatively correlated.

::: {#thm-AVerr}
Let $X$ be a random variable, $\phi$ a function, and $\theta = \Exg\phi(X)$. Let $X'$ have the same distribution as $X$, and write $\rho = \operatorname{Corr}(\phi(X_i),\phi(X'_i))$. Let $$ \widehat{\theta}_n^{\mathrm{AV}} = \frac{1}{n} \sum_{i=1}^n \big(\phi(X_i) + \phi(X_i')\big) $$ be the antithetic variables Monte Carlo estimator of $\theta$. Then:

1.  $\widehat{\theta}_n^{\mathrm{AV}}$ is unbiased, in that $\operatorname{bias}\big(\widehat{\theta}_n^{\mathrm{AV}}\big) = 0$.

2.  The variance of of $\widehat{\theta}_n^{\mathrm{AV}}$ is $$ \operatorname{Var}\big(\widehat{\theta}_n^{\mathrm{AV}}\big) = \frac{1}{n/2} \operatorname{Var}\big(\phi(X) + \phi(X')\big) = \frac{1+\rho}{n}\Var\big(\phi(X)\big). $$

3.  The mean-square error of $\widehat{\theta}_n^{\mathrm{MC}}$ is $$ \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{AV}}\big) = \frac{1}{n/2} \operatorname{Var}\big(\phi(X) + \phi(X')\big) = \frac{1+\rho}{n}\Var\big(\phi(X)\big). $$

4.  The root-mean-square error of $\widehat{\theta}_n^{\mathrm{MC}}$ is $$ \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{AV}}\big) = \frac{1}{n/2} \operatorname{Var}\big(\phi(X) + \phi(X')\big) = \frac{\sqrt{1+\rho}}{\sqrt{n}}\sqrt{\Var\big(\phi(X)\big)}. $$
:::
:::

## Another example

::: {#exm-AVest2}
Let $Z \sim \operatorname{N}(0,1)$ be a standard normal, and suppose we wish to estimate $\theta = \mathbb P(1 \leq Z \leq 3)$.

We could do this with standard Monte Carlo estimation of the indicator function $\Ind_[1,3]$.

```{r, cache = TRUE}
n <- 1e6
samples <- rnorm(n)
MCest <- mean(samples >= 1 & samples <= 3)
MCest
```

But perhaps we can improve on it with an antithetic pair. Remember that $Z'$ should still have the same distribution $Z' \sim \operatorname{N}(0, 1)$, but shouldn't just be independent of $Z$. Well, if $Z$ is standard normal, then its "mirror image" $Z' = -Z$ is standard normal too. Maybe that will work?

```{r, cache = TRUE}
n <- 1e6
samples1 <- rnorm(n/2)
samples2 <- -samples1
AVest1 <- mean(samples1 >= 1 & samples1 <= 3)
AVest2 <- mean(samples2 >= 1 & samples2 <= 3)
AVest <- mean(c(AVest1, AVest2))
AVest
```
:::

Note that, in this code, I found it more convenient to calculate the antithetic variables estimator grouped as
$$\widehat{\theta}_n^{\mathrm{AV}} = \frac{1}{n} \sum_{i=1}^{n/2} \big(\phi(X_i) + \phi(X'_i) \big) = \frac{1}{2} \left( \frac{1}{n/2} \sum_{i=1}^{n/2} \phi(X_i) + \frac{1}{n/2} \sum_{i=1}^{n/2} \phi(X_i')\right) , $$
which is clearly equivalent.

Was our antithetic variables method better? It depends on whether $Y = \Ind_{[1,3]}(Z)$ and $Y' = \Ind_{[1,3]}(-Z)$ are negatively correlated or not. The covariance here is
$$ \Cov(Y, Y') = \Exg YY' - (\Ex Y)(\Ex Y') . $$
We know that $\Ex Y = \Exg\Ind_{[1,3]}(Z) = \mathbb P(1 \leq Z \leq 3) = \theta$ and $\Ex Y' = \Exg\Ind_{[1,3]}(-Z) = \mathbb P(1 \leq Z \leq 3) = \theta$ too, since $Z$ and $-Z$ both have the same distribution.

What about $\Exg YY'$? Well $Y Y' = \Ind_{[1,3]}(Z) \Ind_{[1,3]}(-Z)$ is 1 if and only if both $Z \in [1,3]$ (making the first term 1) *and* $-Z \in [1,3]$ (making the second term 1). Otherwise, at least one of the terms is 0, so the whole thing is 0. But it's impossible for both $Z$ to be in $[1,3]$ *and* $-Z$ to be in $[1,3]$; after all, if $Z \in [1, 3]$ then $-Z$ must be in $[-3,-1]$. So $\Ind_{[1,3]}(Z) \Ind_{[1,3]}(-Z)$ is always 0, so $\Exg YY' = \Exg \Ind_{[1,3]}(Z) \Ind_{[1,3]}(-Z) = 0$ too.

So the covariance is
$$ \Cov(Y, Y') = \Exg YY' - (\Ex Y)(\Ex Y') = 0 - \theta \times \theta = -\theta^2 $$

So the covariance is negative, which means the correlation is negative too. So our pairs really are antithetic, so we really have improved our estimator.

We can calculate the correlation in terms of $\theta$. Hopefully you remember that the variance of a Bernoulli$(\theta)$ random variable is $\theta(1-\theta)$. (If not, you might like to get a scrap of paper and prove it -- it should only take 30 seconds.) So the correlation is
$$ \rho = \Corr(Y, Y') = \frac{\Cov(Y, Y')}{\sqrt{\Var(Y)\Var(Y')}} 
= \frac{-\theta^2}{\theta(1-\theta)} = - \frac{\theta}{1-\theta} . $$

Since we estimated $\theta \approx 0.157$, that suggests that the correlation was roughly $-0.187$.

## Finding antithetic variables

**Next time:** *We come to the third variance reduction scheme: importance sampling.*

**Read more:** [Voss, *An Introduction to Statistical Computing*](https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/1fj430b/cdi_askewsholts_vlebooks_9781118728031), Subsection 3.3.2.