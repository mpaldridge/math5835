# Control variate

::: {.hidden}
$$\newcommand{\Exg}{\operatorname{\mathbb{E}}} 
\newcommand{\Ex}{\mathbb{E}} 
\newcommand{\Ind}{\mathbb{I}}
\newcommand{\Var}{\operatorname{Var}}$$
:::

## Variance reduction

Let's recap where we've got to. The Monte Carlo estimator of $\theta = \Exg \phi(X)$ is
$$ \widehat{\theta}_n^{\mathrm{MC}} = \frac{1}{n} \sum_{i=1}^n \phi(X_i), $$
where $X_1, X_2, \dots, X_n$ are IID random samples from $X$.
The mean-square error of this estimate is
$${\displaystyle \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \frac{1}{n} \operatorname{Var}\big(\phi(X)\big)} . $$

If we want a more accurate estimate, we can just take more samples $n$. But the problem is that the root-mean-square error scales like $1/\sqrt{n}$. To double the accuracy, we need four times as many samples; for one more decimal place of accuracy, we need one hundred times as many samples. And it's generating the samples that takes time, money, energy, etc.

Are there other ways we could reduce the error of Monte Carlo estimation, so we need fewer samples? That is, can we use some mathematical ingenuity to adapt the Monte Carlo estimate to one with a smaller error?

Well, the mean-square error is the variance divided by $n$. So if we can't (or don't want to) increase $n$, perhaps we can *decrease* the *variance* instead? Strategies to do this are called **variance reduction strategies**. In this module, we will look at three variance reduction strategies:

* **Control variate:** We can "anchor" our estimate of $\Exg \phi(X)$ to a very similar but easier-to-calculate value $\Exg \psi(X)$. [This lecture]

* **Antithetic variables:** Instead of using independent samples, we could use correlated samples. If the correlation is negative this can improve our estimate. [Lectures 6 and 7]

* **Importance sampling:** Instead of sampling from $X$, sample from some other more suitable distribution instead, then readjust the answer we get.
[Lectures 8 and 9]


## Control variate estimation

Consider this problem: *Estimate the average time it takes to fly from London to Washington D.C.*

This is quite a tricky problem for me. (Maybe for you too?) I have flown there once before for a maths conference, but it was a long time ago. It was a long flight, many hours, more than just 3 or 4. But it wasn't super-long either, like 13 hours or anything. I'm going to guess 9 hours.

Now consider this problem when you are given the following hint: *PS: The average time it takes to fly from London to New York is 8 hours and 10 minutes.*

Well now, this helps a lot! The answer's going to pretty similar, about 8 hours 10 minutes. But Washington D.C. is a little bit further south and east compared to New York, so it will take a bit longer to get there. But not much, though. I'll guess an extra 15 minutes, so I'll go for 8 hours 25 minutes.

The true answer is 8 hours 29 minutes. My first guess was 31 minutes off (better than I expected!) but my second guess, with the hint, only missed by 4 minutes!

Why was my second guess better? We were trying to estimate $\theta^{\mathrm{DC}}$, the distance to D.C. But that's a big number, and my estimate had a big error (31 minutes). But after the hint, we could write
$$\theta^{\mathrm{DC}} = \theta^{\mathrm{DC}} + \big(\theta^{\mathrm{NY}} - \theta^{\mathrm{NY}}\big) =  \underbrace{\big(\theta^{\mathrm{DC}} - \theta^{\mathrm{NY}}\big)}_\text{small} + \underbrace{\theta^{\mathrm{NY}}}_{\text{known}} . $$
In that equation, the second term, $\theta^{\mathrm{NY}} =$ 8:10 was completely known, so had error 0, while the first term $\theta^{\mathrm{DC}} - \theta^{\mathrm{NY}}$ was a small number, so only had a small error (4 minutes!).

We can apply this idea to Monte Carlo estimation too. Suppose we are trying to estimate $\theta = \Exg \phi(X)$. We want to look for a function $\psi$ that is similar to $\phi$ (at least for the values of $x$ that have high probability for the random variable $X$), but where we know for certain what $\Exg \psi(X)$ is. Then we can write
$$ \theta = \Exg \phi(X) = \Exg \big(\phi(X) - \psi(X) + \psi(X)\big) = \underbrace{\Exg\big(\phi(X) - \psi(X)\big)}_{\text{estimate this with Monte Carlo}} + \underbrace{\Exg \psi(X)}_{\text{known}} . $$

Here, $\psi(X)$ is known as the **control variate**.

::: {#def-CVest}
Let $X$ be a random variable, $\phi$ a function, and write $\theta = \Exg\phi(X)$. Let $\psi$ be a function such that $\eta = \Exg\psi(X)$ is known. Suppose that $X_1, X_2, \dots, X_n$ are a random sample from $X$. Then the **control variate Monte Carlo estimate** $\widehat\theta_n^{\mathrm{CV}}$ of $\theta$ is
$$ \widehat{\theta}_n^{\mathrm{CV}} = \frac{1}{n} \sum_{i=1}^n \big(\phi(X_i) - \psi(X_i)\big) + \eta . $$
:::

::: {#exm-control}
Let's try to estimate $\Ex \cos(X)$, where $X \sim \operatorname{N}(0,1)$ is a standard normal distribution.

We could do this the "usual" Monte Carlo way.

```{r, cache = TRUE}
n <- 1e6
phi <- function(x) cos(x)
samples <- rnorm(n)
MCest <- mean(phi(samples))
MCvar <- var(phi(samples))
MCest
```

But we could see if we can do better with a control variate. But what should we pick for the control function $\psi$? We want something that's similar to $\phi(x) = \cos(x)$, but where we can actually calculate the expectation.

Here's a suggestion. If we remember our Taylor series, we know that, for $x$ near $0$,
$$ \cos x \approx 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots . $$
So how about taking the first two nonzero terms in the Taylor series
$$ \psi(x) = 1 - \frac{x^2}{2} . $$
That is quite close to $\cos x$, at least for the small values of $x$ that $X \sim \operatorname{N}(0,1)$ is likely to take.

```{r taylor}
#| code-fold: true
curve(
  cos(x), from = -4.5, to = 4.5,
  col = "blue", lwd = 3,
  xlab = "x", ylab = "", xlim = c(-4,4), ylim = c(-1.2,1.2)
)
curve(1 - x^2 / 2, add = TRUE, col = "red", lwd = 2)
legend(
  "topright", c("cos x", expression(1 - x^2 / 2)),
  lwd = c(3, 2), col = c("blue", "red")
)
```
Not only that, but we know that, for $Y \sim \operatorname{N}(\mu, \sigma^2)$, we have $\Ex Y^2 = \mu^2 + \sigma^2$. So
$$ \Exg \psi(X) = \Exg \left(1 - \frac{X^2}{2} \right) = 1 - \frac{\Ex X^2}{2} = 1 - \frac{0^2 + 1}{2} = \frac12 . $$

So our control variate estimate is:

```{r, cache = TRUE}
psi <- function(x) 1 - x^2 / 2
CVest <- mean(phi(samples) - psi(samples)) + 1/2
CVest
```
:::

## Error of control variate estimate

What is the error in a control variate estimate?

::: {#thm-CVerr}
Let $X$ be a random variable, $\phi$ a function, and $\theta = \Exg\phi(X)$.  Let $\psi$ be a function fuch that $\Exg\psi(X)$ is known. Let
$$ \widehat{\theta}_n^{\mathrm{CV}} = \frac{1}{n} \sum_{i=1}^n \big(\phi(X_i) - \psi(X_i)\big) + \Exg \psi(X) $$
be the control variate Monte Carlo estimator of $\theta$. Then:

1. $\widehat{\theta}_n^{\mathrm{CV}}$ is unbiased, in that $\operatorname{bias}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = 0$.

1. The variance of of $\widehat{\theta}_n^{\mathrm{MC}}$ is 
${\displaystyle \operatorname{Var}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \frac{1}{n} \operatorname{Var}\big(\phi(X) - \psi(X)\big)}$.

1. The mean-square error of $\widehat{\theta}_n^{\mathrm{MC}}$ is 
${\displaystyle \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \frac{1}{n} \operatorname{Var}\big(\phi(X) - \psi(X)\big)}$.

1. The root-mean-square error of $\widehat{\theta}_n^{\mathrm{MC}}$ is 
${\displaystyle \operatorname{RMSE}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \frac{1}{\sqrt{n}} \operatorname{sd}\big(\phi(X) - \psi(X)\big)}$.
:::

::: {.proof}
This is very similar to @thm-MCerr, so we'll just sketch the important differences.

In part 1, we have
\begin{align*}
\Exg \widehat{\theta}_n^{\mathrm{CV}}
  &= \Exg \left(\frac{1}{n} \sum_{i=1}^n \big(\phi(X_i) - \psi(X_i)\big) + \Exg \psi(X)\right) \\
  &= \frac{1}{n}\Exg \left(\sum_{i=1}^n \big(\phi(X_i) - \psi(X_i)\big)\right) + \Exg \psi(X) \\
  &= \frac{n}{n}\Exg\big(\phi(X) - \psi(X)\big) + \Exg \psi(X) \\
  &= \Exg\phi(X) - \Exg\psi(X) + \Exg\phi(X) \\
  &= \Exg\phi(X) ,
\end{align*}
so the estimator is unbiased.

For part 2, remembering that $\Exg \psi(X)$ is a constant, so doesn't affect the variance, we have
\begin{align*}
\Var \big(\widehat{\theta}_n^{\mathrm{CV}}\big)
&= \Var \left(\frac{1}{n} \sum_{i=1}^n \big(\phi(X_i) - \psi(X_i)\big) + \Exg \psi(X)\right) \\
&= \Big( \frac{1}{n}\Big)^2 \Var \left(\sum_{i=1}^n \big(\phi(X_i) - \psi(X_i)\big) \right) \\
&= \frac{1}{n} \Var \big(\phi(X_i) - \psi(X_i)\big) .
\end{align*}

Parts 3 and 4 follow in the usual way.
:::

::: {#exm-control2}
We return to @exm-control, where we were estimating $\Ex \cos(X)$ for $X \sim \operatorname{N}(0,1)$.

The naive Monte Carlo estimate had variance and root-mean-square error
```{r, cache = TRUE}
c(MCvar, sqrt(MCvar / n))
```

The variance and root-mean-square error of our control variate estimate, on the other hand, are

```{r, cache = TRUE}
CVvar <- var(phi(samples) - psi(samples))
c(CVvar, sqrt(CVvar / n))
```

This was a success! The variance roughly halved, from $`r round(MCvar, 2)`$ to $`r round(CVvar, 2)`$. This meant the root-mean-square went down by about a third, from $`r signif(sqrt(MCvar / n), 2)`$ to $`r signif(sqrt(CVvar / n), 2)`$.
:::

**Next time:** *We look at our second variance reduction technique: antithetic variables.*

**Read more:** [Voss, *An Introduction to Statistical Computing*](https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/1fj430b/cdi_askewsholts_vlebooks_9781118728031), Subsection 3.3.3.