# Importance sampling I

{{< include ../_header.qmd >}}

<!--
## Sampling from other distributions

So far, we have looked at estimating $\Exg \phi(X)$ using samples $X_1, X_2, \dots, X_n$ that are from the same distribution as $X$. **Importance sampling** is based on the idea of taking samples $Y_1, Y_2, \dots, Y_n$ from some *different* distribution $Y$, but then making an appropriate adjustment, so that we're still estimating $\Exg \phi(X)$.

Why might we want to do this? There are two main reasons:

* First, we might not be able to sample from $X$, so we might be forced into sampling from some other distribution $Y$ instead. So far, $X$ has always been a nice pleasant distribution, like a normal, exponential or continuous uniform distribution, for which we can use R's built-in sampling function. But what if $X$ were instead a very unusual or awkward distribution? In that case, we might not be able to sample directly from $X$, so would be forced into sampling from a different distribution.

* Second, we might *prefer* to sample from a distribution other than $Y$. This might be the case if $\phi(x)$ varies a lot over different values of $x$. There might be some areas of $x$ where it's very important to get an accurate estimation, because they contribute a lot to $\Exg\phi(X)$, so we'd like to "oversample" (take lots of samples) there; meanwhile, other areas of $x$ where it is not very important to get an accurate estimation, because they contribute very little to $\Exg\phi(X)$, so we don't mind "undersampling" (taking relatively few samples) there. Then we could sample instead from a distribution $Y$ that concentrates on the most important areas for $\phi$; although we'll need to make sure to adjust our estimator by "down-weighting" the places that we have oversampled.

Consider, for example, trying to estimate $\Exg\phi(X)$ where $X$ is uniform on $[0, 20]$ and $\phi$ is the function shown below.

```{r importance}
#| code-fold: true
phi <- function(x) sin(5 * x) / (5 * x)
curve(
  phi, n = 10001, from = 0, to = 20,
  lwd = 3, col = "blue",
  xlab = "x", ylab = expression(phi(x)), ylim = c(-0.2, 1)
)
abline(h = 0)
```

We can see that what happens for small $x$ -- say, for $x$ between 0 and 2, or so -- will have an important effect on the value of $\Exg \phi(X)$, because that where $\phi$ has the biggest (absolute) values. But what happens for large $x$ -- say for $x \geq 10$ or so -- will be much less important for estimating $\Exg\phi(X)$. So it seems wasteful to have all values in $[0, 20]$ to be sampled equally, and it would seem to make sense to take more samples from small values of $x$.

This is all very well in practice, but how exactly should we down-weight those over-sampled areas?

Let's start by thinking of just $n = 1$ single sample. We want to estimate $\Exg \phi(X)$. Let's assume that $X$ is continuous with probability density function $f$. (Throughout this lecture and the next, we will assume all our random variables are continuous. The arguments for discrete random variables are very similar -- just swap probability density functions with probability mass functions and integrals with sums. You can fill in the details yourself, if you like.) Then we are trying to estimate
$$ \Exg \phi(X) = \int_{-\infty}^{+\infty} \phi(X)\,f(x)\,\mathrm{d}x . $$

Now suppose we sample from some other continuous distribution $Y$, with PDF $g$. If we estimate $\Exg \psi(Y)$, say, for some function $\psi$, then we are estimating
$$\Exg \psi(Y) = \int_{-\infty}^{+\infty} \psi(y)\,g(y) \, \mathrm{d}y = \int_{-\infty}^{+\infty} \psi(x)\,g(x) \, \mathrm{d}x . $$
(In the second equality, we merely changed the dummy variable from $y$ to $x$, as we are at liberty to do.)

But we want to be estimating $\Exg\phi(X)$, not $\Exg\psi(Y)$. So we will need to pick $\psi$ such that
$$ \Exg \phi(X) = \int_{-\infty}^{+\infty} \phi(X)\,f(x)\,\mathrm{d}x = \int_{-\infty}^{+\infty} \psi(x)\,g(x) \, \mathrm{d}x = \Exg \psi(Y) . $$
So we need to pick $\psi$ such that
$\phi(x)\,f(x) = \psi(X)\,g(x)$. That means that we should take
$$\psi(x) = \frac{\phi(x) f(x)}{g(x)} . $$

So we could build a Monte Carlo estimate for $\Exg \phi(X)$ instead as a Monte Carlo estimate for 
$$ \Exg \psi(Y) = \Exg \frac{\phi(Y)\,f(Y)}{g(Y)} . $$

There is one other thing: we need to be careful of division by $0$ errors. So we should make sure that $g$ is only 0 when $f$ is 0. In other words, if it's possible for $X$ to take some value, then it must be possible for $Y$ to take that value too.

We are finally ready to define our estimator.

::: {#def-IS}
Let $X$ be a continuous random variable with probability density function $f$, let $\phi$ be a function, and write $\theta = \Exg\phi(X)$. Let $Y$ be a continuous random variable with probability desnity function $g$, where $g(y) > 0$ for all $y$ where $f(y) > 0$. Then the **importance sampling Monte Carlo estimator** $\widehat\theta_n^{\mathrm{IS}}$ of $\theta$ is
$$ \widehat{\theta}_n^{\mathrm{IS}} = \frac{1}{n} \sum_{i=1}^{n} \frac{f(Y_i)}{g(Y_i)}\, \phi(Y_i)   ,$$
where $Y_1, Y_2, \dots, Y_n$ are independent random samples from $Y$.
:::

We can think of this as taking a weighted mean of the $\phi(Y_i)$s, where the weights are $f(Y_i)/g(Y_i)$. So if a value is much more likely under $Y$ than under $X$, then it gets a low weight; while if a value is less likely under $Y$ than under $X$, then it gets a high weight. Thus we see that the weighting compensates for values that are likely to be over- or under-sampled.

Note that if we set $g$ equal to $f$, and therefore sample from $X = Y$, we get back the standard Monte Carlo estimator.

## Example
-->
::: {#exm-IS1}
{{< include ../examples/L08-1-IS1.qmd >}}
:::
<!--
## Errors in importance sampling

The following theorem should not by now be a surprise.

::: {#thm-Iserr}
Let $X$ be a continuous random variable with probability density function $f$, let $\phi$ be a function, and write $\theta = \Exg\phi(X)$. Let $Y$ another continuous random variable with probability density function with probability density function $g$, such that $g(y) = 0$ only when $f(y) = 0$. Let $$ \widehat{\theta}_n^{\mathrm{IS}} = \frac{1}{n} \sum_{i=1}^n \frac{f(Y_i)}{g(Y_i)}\,\phi(Y_i)  $$ be the importance sampling Monte Carlo estimator of $\theta$. Then:

1.  $\widehat{\theta}_n^{\mathrm{IS}}$ is unbiased, in that $\operatorname{bias}\big(\widehat{\theta}_n^{\mathrm{IS}}\big) = 0$.

2.  The variance of of $\widehat{\theta}_n^{\mathrm{IS}}$ is $$ \operatorname{Var}\big(\widehat{\theta}_n^{\mathrm{IS}}\big) = \frac{1}{n} \operatorname{Var}\left( \frac{f(Y)}{g(Y)}\,\phi(Y) \right). $$

3.  The mean-square error of $\widehat{\theta}_n^{\mathrm{IS}}$ is $$ \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{IS}}\big) = \frac{1}{n} \operatorname{Var}\left( \frac{f(Y)}{g(Y)}\,\phi(Y) \right) . $$

4.  The root-mean-square error of $\widehat{\theta}_n^{\mathrm{IS}}$ is $$ \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{IS}}\big) = \frac{1}{\sqrt{n}} \,\sqrt{\operatorname{Var}\left( \frac{f(Y)}{g(Y)}\,\phi(Y) \right)}. $$
:::

::: {.proof}
Part 1 follows essentially the same argument as our discussion at the beginning of this lecture. We have
$$ \Ex \left( \frac{1}{n} \sum_{i=1}^n \frac{f(Y_i)}{g(Y_i)}\,\phi(Y_i) \right) = \frac{1}{n}\, n\, \Ex \left(\frac{f(Y)}{g(Y)}\,\phi(Y)\right) = \Ex \left(\frac{f(Y)}{g(Y)}\,\phi(Y)\right) . $$
But
$$ \Ex \left(\frac{f(Y)}{g(Y)}\,\phi(Y)\right) = \int_{-\infty}^{+\infty} \frac{f(y)}{g(y)}\,\phi(y)\,g(y)\,\mathrm{d}y = \int_{-\infty}^{+\infty} \phi(y) \, f(y) \, \mathrm{d}y = \Exg \phi(X) . $$
This last step is because $f$ is the PDF of $X$; it doesn't matter whether the dummy variable in the integration is $x$ or $y$. Hence the estimator is unbiased.

Parts 2 to 4 follow in the usual way.
:::

As we are now used to, we can estimate the variance using the sample variance.
-->
::: {#exm-IS2}
{{< include ../examples/L08-2-IS2.qmd >}}
:::

**Next time:** *We continue our study of importance sampling by considering how to pick a good distribution $Y$. This will complete (for now) our study of Monte Carlo estimation.*

::: {.mysummary}
**Summary:**

* Importance sampling estimates $\Exg \phi(X)$ by sampling from a different distribution $Y$.

* The importance sampling estimator is ${\displaystyle \widehat{\theta}_n^{\mathrm{IS}} = \frac{1}{n} \sum_{i=1}^n \frac{f(Y_i)}{g(Y_i)}\,\phi(Y_i)}$.

* The importance sampling estimator is unbiased with mean-square error
$$ \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{IS}}\big) = \frac{1}{n} \operatorname{Var}\left( \frac{f(Y)}{g(Y)}\,\phi(Y) \right) . $$

**Read more:** [Voss, *An Introduction to Statistical Computing*](https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/1fj430b/cdi_askewsholts_vlebooks_9781118728031), Subsection 3.3.1.
:::