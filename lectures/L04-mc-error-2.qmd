# Monte Carlo error II: practice

{{< include ../_header.qmd >}}


## Recap

Let's recap where we've got to. We know that the Monte Carlo estimator for $\theta = \Exg \phi(X)$ is 
$$ \widehat{\theta}_n^{\mathrm{MC}} = \frac{1}{n} \sum_{i=1}^n \phi(X_i) .$$
Last time, we saw that the Monte Carlo estimator is unbiased, and that its mean-square and root-mean-square errors are
$$ \operatorname{MSE}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \frac{1}{n} \operatorname{Var}\big(\phi(X)\big) \qquad \operatorname{RMSE}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \sqrt{\frac{1}{n} \operatorname{Var}\big(\phi(X)\big)} . $$
We saw that these themselves can be estimated as $S^2/n$ and $S/\sqrt{n}$ respectively, where $S^2$ is the sample variance of the $\phi(X_i)$s.

Let's do one more example before moving on.


::: {#exm-MCprob2}
{{< include ../examples/L04-1-MCprob2.qmd >}}
:::


## Confidence intervals

So far, we have described our error tolerance in terms of the MSE or RMSE. But we could have talked about "confidence intervals" or "margins of error" instead. This might be easier to understand for non-mathematicians, for whom "root-mean-square error" doesn't really mean anything.

Here, we will want to appeal to the central limit theorem approximation. A bit more probability revision: Let $Y_1, Y_2, \dots$ be IID again, with expectation $\mu$ and variance $\sigma^2$. Write $\overline Y_n$ for the mean. We've already reminded ourselves that $\mathbb E \overline Y_n = \mu$  and $\Var(\overline{Y}_n) = \sigma^2/n$. But the **central limit theorem** says that the distribution of $\overline Y_n$ is approximately normally distributed with those parameters, so $\overline Y_n \approx \operatorname{N}(\mu, \sigma^2/n)$ when $n$ is large. (This is an informal statement of the central limit theorem: you probably know some more formal ways to more precisely state the it, but this will do for us.)

Recall that, in the normal distribution $\operatorname{N}(\mu, \sigma^2)$, we expect to be within $1.96$ standard deviations of the mean with 95% probability. More generally, the interval $[\mu - q_{1-\alpha/2}\sigma, \mu + q_{1-\alpha/2}\sigma]$, where $q_{1-\alpha/2}$ is the $(1- \frac{\alpha}{2})$-quantile of the normal distribution, contains the true value with probability approximately $1 - \alpha$.

We can form an approximate confidence interval for a Monte Carlo estimate using this idea. We have our Monte Carlo estimator $\widehat{\theta}_n^\mathrm{MC}$ as our estimator of the $\mu$ parameter, and our estimator of the root-mean-square error $S/\sqrt{n}$ as our estimator of the $\sigma$ parameter. So our confidence interval is estimated as 
$$\bigg[ \widehat{\theta}_n^\mathrm{MC} - q_{1-\alpha/2}\,\frac{S}{\sqrt{n}}, \ \widehat{\theta}_n^\mathrm{MC} + q_{1-\alpha/2}\,\frac{S}{\sqrt{n}} \bigg] . $$


::: {#exm-MCprob3}
{{< include ../examples/L04-2-MCprob3.qmd >}}
:::


## How many samples do I need?

In our examples we've picked the number of samples $n$ for our estimator, then approximated the error based on that. But we could do things the other way around -- fix an error tolerance that we're willing to deal with, then work out what sample size we need to achieve it.

We know that the root-mean-square error is
$$ \operatorname{RMSE}\big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \sqrt{\frac{1}{n} \operatorname{Var}\big(\phi(X)\big)} $$
So if we want to get the RMSE down to $\epsilon$, say, then this shows that we need
$$ n = \frac{1}{\epsilon^2} \Var\big(\phi(X)\big) . $$

We still have a problem here, though. We (usually) don't know $\Var(\phi(X))$. But we can't even *estimate* $\Var(\phi(X))$ until we've already taken the samples. But we can use this idea with a three-step process:

1. Run an initial "pilot" Monte Carlo algorithm with a small number of samples $n$. Use the results of the "pilot" to estimate the variance $S^2 \approx \Var(\phi(X))$. We want $n$ small enough that this runs very quickly, but big enough that we get a reasonably OK estimate of the variance.

1.  Pick a desired RMSE accuracy $\epsilon$. We now know that we require roughly $N = S^2 / \epsilon^2$ samples to get our desired accuracy.

1. Run the "real" Monte Carlo algorithm with this big number of samples $N$. We will put up with this being quite slow, because we know we're definitely going to get the error tolerance we need.

(We could potentially use further steps, where we now check the variance with the "real" big-$N$ samples, and, if we learn we had underestimated in Step 1, take even more samples to correct for this.)


::: {#exm-MC22}
{{< include ../examples/L04-3-MC22.qmd >}}
:::


Generally, if we want a more accurate Monte Carlo estimator, we can just take more samples. But the equation
$$ n = \frac{1}{\epsilon^2} \Var\big(\phi(X)\big) $$
is actually quite bad news. To get an RMSE of $\epsilon$ we need order $1/\epsilon^2$ samples. That's not good. Think of it like this: to *double* the accuracy we need to *quadruple* the number of samples. Even worse: to get "one more decimal place of accuracy" means dividing $\epsilon$ by ten; but that means multiplying the number of samples by one hundred!

More samples take more time, and cost more energy and money. Wouldn't it be nice to have some better ways of increasing the accuracy of a Monte Carlo estimate besides just taking more and more samples?


**Next time:** *We begin our study of clever "variance reduction" methods for Monte Carlo estimation.* 

::: {.mysummary}
**Summary:**

* We can approximate confidence intervals for a Monte Carlo estimate by using a normal approximation.

* To get the root-mean-square error below $\epsilon$ we need $n = \Var(\phi(X))/\epsilon^2$ samples.

* We can use a two-step process, where a small "pilot" Monte Carlo estimation allows us to work out how many samples we will need for the big "real" estimation.

**Read more:** [Voss, *An Introduction to Statistical Computing*](https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/1fj430b/cdi_askewsholts_vlebooks_9781118728031), Subsections 3.2.2--3.2.4.
:::