# Monte Carlo error II

::: {.hidden}
$$\newcommand{\Exg}{\operatorname{\mathbb{E}}} 
\newcommand{\Ex}{\mathbb{E}} 
\newcommand{\Ind}{\mathbb{I}}$$
:::

## Examples

[RECAP]

We've done enough theory. Let's see some examples.

::: {#exm-MCexp2}
Let's go back to the very first example in the module, @exm-MCexp, where we were trying to find the expectation of an $\operatorname{Exp}(2)$ random variable. We used this R code:

```{r, cache = TRUE}
n <- 1e6
samples <- rexp(n, 2)
MCest <- mean(samples)
MCest
```

(Because Monte Carlo estimation is random, this won't be the *exact* same estimate we had before, of course.)

So if we want to investigate the error, we can use the sample variance of these samples.

```{r, cache = TRUE}
var_est <- var(samples)
MSEest  <- var_est / n
RMSEest <- sqrt(MSEest)
c(var_est, MSEest, RMSEest)
```

The first number is `var_est` $= `r signif(var_est, 3)`$, the sample variance of our $\phi(x_i)$s:
$$ s^2 = \frac{1}{n-1} \sum_{i=1}^n \big(\phi(x_i) - \widehat{\theta}_n^{\mathrm{MC}}\big)^2 . $$
This should be a good estimate of the true variance $\operatorname{Var}(\phi(X))$.

The second number is `MSEest` $= `r signif(var_est/n, 3)`$, our estimate of the mean-square error. Since $\operatorname{MSE}(\widehat{\theta}_n^{\mathrm{MC}}) = \frac{1}{n} \operatorname{Var}(\phi(X))$, our estimate of the MSE is $\frac{1}{n} s^2$.

The third number is `RMSEest` $= `r signif(sqrt(var_est/n), 3)`$ our estimate of the root-mean square error, which is simply the square-root of our estimate of the mean-square error.
:::

::: {#exm-MCprob2}
In @exm-MCprob, we were estimating $\mathbb P(Z > 2)$, where $Z$ is a standard normal.

Our code was
```{r}
n <- 1e6
samples <- rnorm(n, 0, 1)
MCest <- mean(samples > 2)
MCest
```
So our root-mean-square error can be approximated as
```{r}
RMSEest <- sqrt(var(samples > 2) / n)
RMSEest
```
:::

## How many samples do I need?

In our examples we've picked the number of samples $n$ for our estimator, then approximated the error based on that. But we could do things the other way around -- fix an error tolerance that we're willing to deal with, then choose the sample size based on that.

That is, we have a three-step process:

1. Run an initial "pilot" Monte Carlo algorithm with a small number of samples $n$. We want $n$ small enough that this runs very quickly.

1. Use the results of the "pilot" to approximate the error. Then use this to decide what value $n$ we will need for the "real" Monte Carlo algorithm.

1. Run the "real" Monte Carlo algorithm with this big number of samples $n$. We will put up with this being quite slow, because we know we're definitely going to get the error tolerance we need.

::: {#exm-MC22}
Let's try this with @exm-MC2 from before. We were trying to esimate $\Exg(\sin X)$, where $X \sim \operatorname{N}(1, 2^2)$.

We'll start with just $n = 1000$ samples, for our pilot study

```{r, cache = TRUE}
n_pilot <- 1000
samples <- rnorm(n_pilot, 1, 2)
var_est <- var(sin(samples))
var_est
```
  
This was super-quick! But with only 1000 samples, it won't have been an accurate estimate yet.

Let's suppose we want to get the root-mean-square error down to $\epsilon = 10^{-4}$. We know, using `var_est` to estimate the variance, that we want
$$ \epsilon = \operatorname{RMSE} \big(\widehat{\theta}_n^{\mathrm{MC}}\big) = \frac{1}{\sqrt{n}} \,\operatorname{sd}\big(\phi(X)\big) \approx \frac{1}{\sqrt{n}} \times \sqrt{`r signif(var_est, 4)`} . $$
Rearranging to make $n$ the subject and using $\epsilon = 10^{-4}$, we need
$$ n \approx `r signif(var_est, 4)`\times \frac{1}{\epsilon^2} = `r signif(var_est, 3)`\times 10^{8} =  `r signif(var_est * 1e8, 4)` \approx `r signif(var_est * 1e8, 1)`$$
samples, or about 50 million.

```{r, cache = TRUE}
epsilon <- 1e-4
n_real  <- round(var_est / epsilon^2)
n_real
samples <- rnorm(n_real, 1, 2)
MCest <- mean(sin(samples))
MCest
RMSEest <- sqrt(var(sin(samples)) / n_real)
RMSEest
```

This was very slow! But we see that we have indeed got our Monte Carlo estimate to (near enough) the desired accuracy.
:::

A lot of this is actually quite bad news. We noticed that to get an RMSE of $\epsilon$ we need order $1/\epsilon^2$ samples. That's not good. Think of it like this: to *double* the accuracy we need to *quadruple* the number of samples. Even worse: to get "one more decimal place of accuracy" means dividing $\epsilon$ by ten; but that means multiplying the number of samples by one hundred!

## Margins of error

We could describe our error tolerance in terms of the RMSE. But we could have talked about "confidence intervals" instead, by appealing the central limit theorem approximation. This might be easier to understand for non-mathematicians, for whom "root-mean-square error" doesn't really mean anything.

A bit more probability revision: Let $Y_1, Y_2, \dots$ be IID again, with the mean $\overline Y_n$.

The expectation of $\overline Y_n$ staying at $\mu$ while the variance $\sigma^2/n$ gets smaller and smaller means the probability gets ever more concentrated around $\mu$. This gives the **law of large numbers** which says that $\overline Y_n \to \mu$ in probability as $n \to \infty$. You may remember that this means the Monte Carlo estimate is **consistent**.

While we know the expectation of $\overline Y_n$ is $\mu$ and the variance is $\sigma^2/n$, the **central limit theorem** says that the distribution of $\overline Y_n$ is approximately normally distributed with those parameters. Informally, we can say $\overline Y_n \approx \operatorname{N}(\mu, \sigma^2/n)$ when $n$ is large. (You probably know some more formal ways to more precisely state the central limit theorem, but this will do for us.)

Recall that, in the normal distribution, we expect to be within $1.96$ standard deviations of the mean with 95% probability.
So our previous example could interpret this as a $1.96\epsilon \approx 2\times 10^{-4}$ "margin of error" or a "95% confidence interval" of $`r round(MCest, 5)` \pm (2.0\times 10^{-4})$.



[EXAMPLE?]

Wouldn't it be nice to have some better ways of increasing the accuracy of a Monte Carlo estimate besides just taking more and more samples?

**Next time:** *We begin our study of clever "variance reduction" methods for Monte Carlo estimation.* 